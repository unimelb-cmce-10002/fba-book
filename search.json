[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Foundations of Business Analytics",
    "section": "",
    "text": "Welcome\nIn this course, you’ll develop the essential skills to analyze and interpret data for better business decision-making. Whether you’re new to coding or looking to strengthen your analytical thinking, we’ll guide you through the fundamentals of working with data in R — structuring, transforming, visualizing, analyzing, and interpreting information effectively.\nBusiness analytics spans three critical dimensions: understanding what happened in the past, anticipating what will happen next, and uncovering why things happen through causal analysis. Through hands-on case studies, you’ll apply these approaches to real-world business problems, extracting meaningful insights that drive action. The power of data lies in the stories it tells. From ancient oral traditions to modern visualizations, effectively communicated insights create impact. You’ll learn to transform raw data into compelling narratives that enhance clarity and directly influence business decisions.\nYour approach will mirror that of a scientist: observing carefully, experimenting methodically, and making evidence-based predictions about firm and consumer behavior. This systematic framework provides a practical toolkit for addressing complex business challenges. With this analytical power comes significant responsibility. We’ll explore ethical dimensions of working with sensitive information, recognizing and mitigating bias, balancing analytical insight with individual rights, and ensuring transparent communication of findings.\nFor knowledge to create value, it must be communicated clearly and transparently. You’ll develop workflows that ensure accuracy, consistency, and reliability — critical skills for building trust in your analysis.\nData is everywhere, but enduring insights are rare. This course will help you transform numbers into knowledge that informs business and society—ethically, responsibly, and effectively.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#audience-and-assumed-knowledge",
    "href": "index.html#audience-and-assumed-knowledge",
    "title": "Foundations of Business Analytics",
    "section": "Audience and Assumed Knowledge",
    "text": "Audience and Assumed Knowledge\nThis course is designed for first-year undergraduate students enrolled in business, commerce, or economics degrees.\nWe assume students:\n\nHave no prior programming experience.\nHave basic numeracy skills (high school level mathematics).\nAre curious about how data and technology are changing the way businesses operate.\nAre ready to engage in structured problem-solving and critical thinking.\n\nNo specific technical prerequisites are required. Our goal is to meet students where they are and progressively build their skills, confidence, and analytic thinking.\nWe welcome students from all backgrounds — no prior coding or analytics experience is needed, and we will build every concept step-by-step. Curiosity, patience, and a willingness to engage with new ideas are the most important starting points.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#learning-goals",
    "href": "index.html#learning-goals",
    "title": "Foundations of Business Analytics",
    "section": "Learning Goals",
    "text": "Learning Goals\nBy the end of this course, students will be able to:\n\nDefine and explain key concepts in business analytics.\nIdentify different types of business problems and select appropriate analytic approaches.\nCritically assess data sources and recognize the limitations of data-driven insights.\nCommunicate analytic findings through clear, structured narratives.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#pedagogical-approach",
    "href": "index.html#pedagogical-approach",
    "title": "Foundations of Business Analytics",
    "section": "Pedagogical Approach",
    "text": "Pedagogical Approach\nOur teaching philosophy blends three core elements:\n\nBusiness Challenge First\n\nEach chapter begins with a real-world business problem or question.\nReaders are invited to think creatively and critically about how they would approach the challenge before technical methods are introduced.\n\nSkills of a Business Scientist\n\nWe teach how to combine technical data skills, business judgment, and communication.\nEmphasis is placed on asking good questions, interpreting results thoughtfully, and communicating clearly.\n\nData to Narrative\n\nStudents learn how to move from raw data, through analysis, to actionable insights.\nStorytelling is treated as a fundamental analytic skill: interpreting patterns, understanding context, and communicating findings persuasively to decision-makers.\n\n\nThis text is designed to serve both as a foundation for classroom discussion and as a self-contained guide for independent study. Analytics is about exploration and iteration — it is normal to encounter challenges and learn through the process. We encourage a growth mindset: thoughtful questions, careful reasoning, and creative thinking are celebrated throughout the journey into business analytics.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#about-the-instructors",
    "href": "index.html#about-the-instructors",
    "title": "Foundations of Business Analytics",
    "section": "About the Instructors",
    "text": "About the Instructors\nLachlan Deer is a Lecturer in the Management & Marketing Group at the University of Melbourne. His research interests lie in quantitative marketing, digital markets and public policy. Lachlan has spent the last decade introducing computing and data science skills into business, economics and social science curriculums, adopting a hands-on and learner focussed practical training, enabling learners to become more efficient and productive in their work by bridging the gap between the increasing reliance on computational skills demanded by employers and the limited training many learners receive in these areas in traditional courses in these areas. Prior to joining University of Melbourne, Lachlan has worked at Tilburg University and the University of Chicago Booth School of Business. Lachlan earned his PhD in Economics from the University of Zurich.\nPatrick Ferguson is a Senior Lecturer in the Accounting Group at the University of Melbourne and a Research Affiliate at the Laboratory for Innovation Science at Harvard University. In his research, Patrick uses observational data and field experiments to study how organizations measure, reward, and report on worker performance. He is also interested in contest design and sports economics. He received a PhD in Business Administration from Harvard University.\nJunhao Liu is a Senior Lecturer in the Accounting Group at the University of Melbourne. Junhao’s research interests broadly pertain to archival research in financial accounting, and he is particularly interested in accounting intangibles, financial reporting and disclosure, financial analysts, and valuation. Junhao obtained my Ph.D. degree in accounting from Rotman School of Management at University of Toronto.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "foundations/what_is_business_analytics.html",
    "href": "foundations/what_is_business_analytics.html",
    "title": "1  What is Business Analytics?",
    "section": "",
    "text": "1.1 What is Business Analytics?\nBusinesses have always had to make important decisions: Who should we hire? Where should we invest? How much should we spend on advertising? In the past, these decisions were often made based on experience, intuition, or the authority of senior leadership—a dynamic sometimes known as the “HIPPO” effect: Highest Paid Person’s Opinion.\nThat’s no longer enough. Today’s businesses operate in faster, more complex environments. Fortunately, they also have access to something that previous generations did not: data. From customer transactions to social media posts, microsecond stock price information to supply chain records, nearly every part of a business now generates data. A single online purchase creates dozens of data points: what the customer viewed, how long they hesitated, which payment method they chose, even how they arrived at the site. The challenge isn’t collecting it—it’s knowing what to do with it.\nThis is where business analytics comes in.\nBusiness analytics is the use of data and analytical tools to support better decision-making. It combines statistical thinking, technological capability, and business insight. But it’s not just about building models or generating dashboards. At its core, it’s about helping organizations answer the fundamental question that drives all business strategy: What should we do next?\nThat story is ultimately what business analytics is trying to uncover. Analysts help businesses listen to what the data is saying—about customer needs, regulatory risks, investor sentiment, competitive dynamics, or operational bottlenecks—and translate that into actions.\nBut the essence of decision-making remains what it has always been: reasoning through uncertainty, weighing trade-offs, and telling a compelling story about what to do. The difference today is that we can do it better—with data as our guide, structure as our method, and analytical tools that help us move beyond educated guesswork toward informed action.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is Business Analytics?</span>"
    ]
  },
  {
    "objectID": "foundations/what_is_business_analytics.html#what-is-business-analytics",
    "href": "foundations/what_is_business_analytics.html#what-is-business-analytics",
    "title": "1  What is Business Analytics?",
    "section": "",
    "text": "“Let the data tell its story.” — Gregory Crawford, Chief Economist at Zalando\n\n\n\n\n\n\n\n\n\nNoteBusiness Analytics vs Data Science\n\n\n\nBusiness analytics and data science overlap, but they are not the same. Data science often focuses on building general-purpose models and technical innovations. Business analytics stays grounded in context-specific decisions. A good business analyst is less concerned with predictive accuracy in the abstract and more concerned with whether an insight leads to a better business outcome.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is Business Analytics?</span>"
    ]
  },
  {
    "objectID": "foundations/what_is_business_analytics.html#how-business-decision-making-has-evolved",
    "href": "foundations/what_is_business_analytics.html#how-business-decision-making-has-evolved",
    "title": "1  What is Business Analytics?",
    "section": "1.2 How Business Decision-Making Has Evolved",
    "text": "1.2 How Business Decision-Making Has Evolved\nBusiness decision-making has gone through a quiet revolution. While the core challenge—deciding what to do—hasn’t changed, the tools, habits, and expectations surrounding decision-making have shifted dramatically. Business analytics sits at the center of this transformation.\nWe can think of the evolution in four broad stages:\nStage 1: Gut Instinct (The HIPPO Era)\nFor most of the 20th century, decisions were dominated by experience, instinct, and authority. The most senior person in the room often had the final say—regardless of what the data might suggest. This is the “HIPPO” model we mentioned earlier: Highest Paid Person’s Opinion. While experience remains valuable, this approach often lacks transparency and makes it difficult to learn from both successes and failures.\nStage 2: Simple Data Support\nWith the rise of digital systems, businesses began generating and storing data—and using it to create basic reports and forecasts. Dashboards and spreadsheets became common tools for tracking performance. But in this stage, data was mainly descriptive: it helped explain what had happened, not what to do next. Think monthly sales reports or annual budget reviews.\nStage 3: Advanced Analytics\nAs computing power increased, so did analytical ambition. Businesses began using predictive models and machine learning to forecast future outcomes and identify hidden patterns. This allowed for more proactive decisions—anticipating which customers might leave, deciding when to issue stock to raise capital, or optimizing prices dynamically. Yet even the most sophisticated model is only as useful as the decision it ultimately supports.\nStage 4: Experimentation and Causal Learning\nThe frontier today is not just prediction—it’s learning what actually works. Companies now use A/B testing, field experiments, and causal inference to evaluate strategies and understand the true impact of different choices. This lets firms go beyond correlation and ask the crucial question: What happens if we actually do X instead of Y?\n\n“Nearly every decision we make about our product and business is guided by member behavior observed in test” (i.e. an experiment). — Netflix Tech Blog\n\n\n“Our success at Amazon is a function of how many experiments we do per year, per month, per week, per day.” — Jeff Bezos\n\nThis shift toward experimentation represents more than just a technical advancement—it’s a cultural transformation. It reflects a mindset that values curiosity, iteration, and evidence over hierarchy or habit. Modern business analytics is as much about learning and adapting as it is about measuring and predicting.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is Business Analytics?</span>"
    ]
  },
  {
    "objectID": "foundations/what_is_business_analytics.html#what-is-business-analytics-1",
    "href": "foundations/what_is_business_analytics.html#what-is-business-analytics-1",
    "title": "1  What is Business Analytics?",
    "section": "1.3 What is Business Analytics?",
    "text": "1.3 What is Business Analytics?\nWe’ve spent quite a bit of time talking about business analytics without actually defining it. Time to fix that.\nBusiness Analytics is the practice of using data and models to solve business problems and improve decision-making.\nIt’s not just about collecting numbers—it’s about using them wisely. In today’s data-rich environment, the real challenge is knowing how to extract insight, frame decisions, and take action in a way that creates value.\n\n“Data are widely available; what is scarce is the ability to extract wisdom from them.” — Hal Varian, Chief Economist at Google\n\nThat’s where business analytics comes in. It combines technical skills with business understanding to move from raw data to real-world impact. A business analyst doesn’t just answer what happened—they help organizations understand why, predict what might happen next, and evaluate what to do about it.\nThe process usually involves five key steps:\n\nDefining the Problem — Every good analysis begins with a sharp question. What decision needs to be made? What are the options? What does success look like?\nCollecting and Cleaning Data — Data is rarely ready to use out of the box. Analysts spend significant time gathering relevant data sources and ensuring they’re accurate, complete, and reliable.\nAnalyzing Data Appropriately — With the right tools—from simple summaries to predictive models—analysts identify patterns, test hypotheses, and quantify trade-offs.\nCommunicating Insights — Even the best analysis fails if no one understands it. Analysts must tell persuasive, relatable stories with data—stories that clarify the decision at hand and guide action.\nActing to Create Value — Ultimately, analytics only matters if it leads to better decisions. The goal is not analysis for its own sake, but to generate insights that drive strategy, improve operations, control costs, better serve customers, or deliver stronger returns to shareholders.\n\nBusiness analytics is more than a technical field—it’s a way of thinking. It reflects a broader evolution in business: away from gut instinct alone, and toward evidence-based decision-making. But even as the tools evolve—from dashboards to machine learning to experimentation—the fundamental goal remains the same: to make smarter, more confident choices in an uncertain world.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is Business Analytics?</span>"
    ]
  },
  {
    "objectID": "foundations/what_is_business_analytics.html#the-three-types-of-business-analytics",
    "href": "foundations/what_is_business_analytics.html#the-three-types-of-business-analytics",
    "title": "1  What is Business Analytics?",
    "section": "1.4 The Three Types of Business Analytics",
    "text": "1.4 The Three Types of Business Analytics\nBusiness analytics isn’t one-size-fits-all. Different business problems require different types of insight—and therefore, different analytical tools. A helpful way to understand the field is to distinguish between three broad types of business analytics:\n\n1.4.1 Descriptive Analytics: What happened?\nDescriptive analytics focuses on summarizing and reporting what has already occurred in a business. It answers questions like:\n\nWhat were our total sales last quarter?\nWhich products had the highest return rates?\nWhat were the top 10 ASX-listed companies by shareholder return last year?\n\nTools used here include summary statistics, tables, visualizations, and dashboards. These methods don’t make predictions or test strategies—they organize past data in a way that helps managers understand performance, spot trends, or identify anomalies.\nThink of this as the rearview mirror of analytics. It helps you see where you’ve been.\n\n\n1.4.2 Predictive Analytics: What might happen next?\nPredictive analytics goes a step further: it uses past data to forecast future outcomes. This includes questions like:\n\nHow many customers are likely to churn next month?\nHow are annual depreciation expenses likely to change over the next three years?\nWill the RBA cut interest rates next month, and if so by how many basis points?\n\nMethods here often involve statistical models and machine learning techniques that identify patterns and extrapolate them forward. These models don’t guarantee what will happen, but they give decision-makers a probabilistic view of likely outcomes.\nImportantly, predictive models do not answer causal questions. A forecast that sales will increase next quarter doesn’t tell you why—or what would happen if you changed your pricing or launched a new campaign.\nThink of this as the weather forecast of analytics: useful for planning, but not a substitute for experimentation or causal understanding.\n\n\n1.4.3 Causal (Prescriptive) Analytics: What will happen if we act?\nThis is where things get more strategic—and more difficult.\nCausal analytics (sometimes called prescriptive analytics) tries to answer counterfactual questions: What would happen if we did X instead of Y?\nExamples include:\n\nWhat impact will a 10% price increase have on demand?\nHow will merging with another firm affect our share price?\nWould switching to a new supplier improve delivery times?\n\nThese questions can’t be answered by simply observing correlations or running a predictive model. They require methods designed to isolate cause and effect—such as randomized experiments (A/B testing), instrumental variables, difference-in-differences, or other econometric techniques.\nThis is the decision engine of analytics: it helps businesses not just anticipate the future, but change it deliberately.\n\n\n1.4.4 Key Insight: Different Problems, Different Tools\nEach type of analytics serves a different purpose:\n\n\n\n\n\n\n\n\n\nType\nCore Question\nTypical Tools\nBusiness Use\n\n\n\n\nDescriptive\nWhat happened?\nSummaries, visualizations\nReporting and monitoring\n\n\nPredictive\nWhat might happen?\nForecasting, classification\nAnticipating future trends\n\n\nCausal (Prescriptive)\nWhat will happen if we act?\nExperiments, causal inference\nStrategy and decision-making\n\n\n\nChoosing the wrong tool for the question can lead to misleading conclusions. For example, a model that predicts which companies are likely to identify and correct mistakes in their previously published financial statements is not the same as understanding why those mistakes occurred—or what internal controls or governance changes might have prevented them.\nTo be a great business analyst, it’s not enough to know how to use the tools. You need to know which tool matches which question.\n\n\n10 min\n\n\nMatching Analytics Types to Business Questions\nNow that you understand the three types of business analytics, let’s practice identifying which type of analysis different business questions require.\nFor each business area below, we’ve provided examples of questions that fall into our three categories: Descriptive (What happened?), Predictive (What might happen?), and Causal (What will happen if we act?).\nSome examples are filled in to get you started. Your job is to complete the missing entries by thinking about what questions a business analyst in each area might ask.\n\n\nComplete the Table\n\n\n\n\n\n\n\n\n\nBusiness Area\nDescriptive\nPredictive\nCausal\n\n\n\n\nSales\nHow much did we sell last quarter?\n[Your turn: What might happen in the future?]\n[Your turn: What question would test the impact of changing something?]\n\n\nMarketing\n[Your turn: What happened in the past?]\nWho will buy next?\n[Your turn: What question would test the impact of changing something?]\n\n\nHiring\n[Your turn: What happened in the past?]\n[Your turn: What might happen in the future?]\nWhat if we change recruitment practices?\n\n\nFinance\n[Your turn: What happened in the past?]\nWill we meet next quarter’s revenue goals?\n[Your turn: What question would test the impact of changing something?]\n\n\nSupply Chain\nHow many units were delivered on time?\n[Your turn: What might happen in the future?]\n[Your turn: What question would test the impact of changing something?]\n\n\n\n\n\n\n\n5 min\n\n\nClassify These Questions\nFor each question below, identify whether it requires Descriptive, Predictive, or Causal analytics:\n\n“Which marketing channels generated the most leads last month?”\n\nType: _______________\n\n“How will our share price change if we replace our CEO with our current COO?”\n\nType: _______________\n\n“Which customers are most likely to upgrade to our premium service?”\n\nType: _______________\n\n“Which of our product lines account for the largest share of corporate overhead expenses?”\n\nType: _______________\n\n“Would offering free shipping increase our conversion rate?”\n\nType: _______________\n\n“Will analysts issue a buy/hold/sell recommendation for our stock next week?”\n\nType: _______________\n\n\n\n\n\n\n\n\n\n\nTipThinking Like an Analyst\n\n\n\nRemember: the same business problem can often be approached from multiple analytical angles. The key is being intentional about which type of analysis will best serve the decision you need to make.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is Business Analytics?</span>"
    ]
  },
  {
    "objectID": "foundations/what_is_business_analytics.html#the-power-and-limits-of-analytics",
    "href": "foundations/what_is_business_analytics.html#the-power-and-limits-of-analytics",
    "title": "1  What is Business Analytics?",
    "section": "1.5 The Power and Limits of Analytics",
    "text": "1.5 The Power and Limits of Analytics\nBusiness analytics is a powerful tool—but it is not a crystal ball. At its best, analytics sharpens our thinking, clarifies options, and supports better decisions. But it also has limits that every analyst must understand.\n\n1.5.1 Good decisions depend on good data\nIt’s a simple principle, but an essential one: garbage in, garbage out. Even the most advanced model cannot save an analysis built on poor-quality data, vague problem definitions, or biased assumptions. Clean, relevant, well-structured data is a prerequisite—not a guarantee—for good insights.\n\nPrinciple: Garbage in, garbage out.\n\nAnalytics doesn’t make decisions for us. It supports decision-making. The quality of that support depends entirely on the inputs we provide and the care with which we interpret the results.\nThis is why, later in the course, we devote specific chapters to how data is collected and how it is stored and managed—because those choices shape everything that follows in the analytical process.\n\n\n1.5.2 Analytics doesn’t eliminate uncertainty—or judgment\nAll models are simplifications. They help us focus on what matters, but they also leave things out. A forecast can offer probabilities, but never certainties. A causal estimate can quantify impact, but only under specific assumptions. The world is more complex than any spreadsheet or statistical model can fully capture.\nThis means that even with data:\n\nUncertainty remains. Analysts must often make decisions under incomplete information or ambiguous results.\nTrade-offs persist. No model can resolve tensions between short-term gains and long-term investments, or between efficiency and innovation. These are managerial choices, not statistical ones.\nJudgment is essential. Good analysts don’t just run models—they think critically, question results, and tell coherent stories that connect the data to the broader context.\n\n\n\n\n\n\n\nWarningCase Study: When Forecasting Falls Short\n\n\n\nWoolworths, Australia’s largest supermarket chain, learned this lesson during the ACCC’s 2024-25 supermarket inquiry. Despite having access to massive amounts of checkout data and sophisticated forecasting systems, Woolworths admitted to significant shortcomings in the accuracy of volume forecasts provided to fresh fruit and vegetable suppliers.\nAs CEO Amanda Bardwell acknowledged during ACCC hearings, the company had access to detailed checkout data that should have given them “a firm understanding of likely future demand.” Yet former Managing Director Natalie Davis admitted that an internal review found Woolworths had “failed to provide volume forecasts to suppliers that were as accurate as possible.”\nThe problem wasn’t the technology—it was how they used their data. Davis explained that Woolworths had failed to apply new analytical capabilities as they became available, essentially letting their forecasting methods lag behind their data collection abilities.\nKey lessons:\n\nGarbage in, garbage out: Having lots of data doesn’t automatically lead to good predictions\nTools must match the problem: Fresh produce forecasting requires different approaches than predicting demand for shelf-stable goods\nImplementation matters: Even good analytical capabilities are useless if they’re not properly applied\n\nSource: ACCC Supermarket Inquiry Final Report, March 2025\n\n\n\n\n1.5.3 The analyst’s role\nBeing a good analyst means more than knowing the right techniques. It means understanding the business problem, evaluating the quality of the data, and making thoughtful decisions about how to present and interpret results. It’s about being honest when uncertainty is high, and being clear about what the data can and cannot tell us.\nIn short, analytics gives us leverage—but it doesn’t replace leadership. The best decisions come when rigorous analysis meets sound judgment.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is Business Analytics?</span>"
    ]
  },
  {
    "objectID": "foundations/what_is_business_analytics.html#ethics-in-business-analytics",
    "href": "foundations/what_is_business_analytics.html#ethics-in-business-analytics",
    "title": "1  What is Business Analytics?",
    "section": "1.6 Ethics in Business Analytics",
    "text": "1.6 Ethics in Business Analytics\nBusiness analytics is not just a technical activity—it is also a human one. Every decision about what data to collect, how to analyze it, and how to act on it involves ethical considerations.\n\n1.6.1 Data use raises ethical responsibilities\nAs businesses gain more powerful tools to collect and interpret data, they also take on greater responsibility. Three core ethical concerns should guide any data-driven initiative:\n\nAvoid bias.\nAlgorithms and models can reflect—and even amplify—existing biases in the data. For example, a hiring model trained on past decisions may replicate patterns of discrimination unless actively corrected. Ethical analytics involves detecting, understanding, and mitigating such bias.\nProtect privacy.\nWith more data comes more risk. Businesses must handle customer, employee, and user data with care. That means complying with legal requirements (like the GDPR or Australia’s Privacy Act), but also going beyond compliance to ask: Are we using this data in ways that our stakeholders would consider reasonable and respectful?\nPromote fairness.\nDecisions driven by analytics—such as who gets a loan, a promotion, or a targeted offer—can have real consequences. Ethical analytics means being transparent about how decisions are made and striving to ensure that outcomes are just and inclusive.\n\n\nEthical analytics isn’t just about what you can do with data—it’s about what you should do.\n\nThese issues will return in later chapters—especially when we discuss data collection, machine learning, and A/B testing. In each of those contexts, ethical decision-making is not an optional add-on, but a core part of responsible business practice.\n\n\n\n\n\n\nNoteCase Study: Algorithmic Management and Worker Autonomy\n\n\n\nIn Southeast Asia, ride-hailing platforms like Grab have implemented algorithmic management systems to coordinate labor, match drivers with passengers, and set dynamic pricing. While these systems enhance efficiency, they have also raised concerns about transparency and fairness.\nDrivers often report a lack of clarity on how decisions are made regarding ride assignments and fare calculations, leading to feelings of disempowerment and uncertainty about their earnings. A study by the Carnegie Endowment for International Peace highlighted that such algorithmic management can disrupt traditional labor relations, making work more individualized and reducing opportunities for collective bargaining.\nThe opaque nature of these systems can exacerbate existing inequalities, particularly for workers in marginalized communities who may lack institutional support. This case underscores the importance of embedding ethical considerations into the design and implementation of analytics systems, ensuring that they promote fairness and transparency for all stakeholders.\nSource: Carnegie Endowment for International Peace\n\n\n\n\n1.6.2 Ethics is not optional\nFirms that ignore these responsibilities may face backlash, regulatory penalties, or long-term reputational damage. But beyond risk avoidance, ethical analytics also presents a positive opportunity: to build trust, foster loyalty, and support a more responsible form of business.\nEthical thinking doesn’t happen at the end of an analysis. It must be embedded at every stage—from how we frame questions to how we communicate results. Throughout this course, we will return to this theme as we encounter specific ethical trade-offs in modeling, measurement, and experimentation.\nDone well, business analytics can help firms grow in ways that are not only efficient, but sustainable and fair. It’s up to analysts to ensure that data is used wisely—and responsibly.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is Business Analytics?</span>"
    ]
  },
  {
    "objectID": "foundations/what_is_business_analytics.html#becoming-a-business-analyst-the-interpreter-role",
    "href": "foundations/what_is_business_analytics.html#becoming-a-business-analyst-the-interpreter-role",
    "title": "1  What is Business Analytics?",
    "section": "1.7 Becoming a Business Analyst: The Interpreter Role",
    "text": "1.7 Becoming a Business Analyst: The Interpreter Role\nBy now, we’ve seen that analytics is not just about algorithms and datasets—it’s about decisions. And good decisions need more than numbers. They need interpretation, judgment, and communication. This is where the business analyst comes in.\nBusiness analysts sit at the intersection of data and decision-making. They are not just technicians or model-builders—they are interpreters. Their job is to make data meaningful to the people who need to act on it.\n\n“MBAs who have good business intuition but also speak the language of statisticians are rock stars.” — Susan Athey, Professor of Economics at Stanford\n\nWhile this quote mentions MBAs, it’s not just about the degree. It’s about the blend of skills—business understanding, analytical fluency, and communication—that define a high-impact analyst. And that skillset isn’t reserved for postgrads. A well-trained undergraduate business analyst with this toolkit can add enormous value from day one. That’s exactly the kind of foundation this course is designed to build.\n\n1.7.1 Analysts are modern storytellers\nHumans are natural storytellers. For thousands of years, stories have helped us make sense of complexity—turning scattered events into patterns, and patterns into meaning.\nToday’s business analysts continue this tradition. But instead of myth or anecdote, they use data. The tools may be newer, but the mission is timeless: to tell clear, truthful, and compelling stories that help others see what is happening—and decide what to do next.\nThat story might be about why sales are falling, how investor sentiment is improving, or what might happen if the firm raises prices or changes strategy. But in every case, the analyst’s role is the same: to build a bridge between the data and the decision.\n\n\n1.7.2 Core skills of the business analyst\nTo succeed in this role, analysts need a blend of skills:\n\nBusiness understanding\nYou must understand the context: What is the decision? What matters to stakeholders? What constraints shape the options?\nData analysis\nYou need to clean, explore, and model data using the right tools. This includes both descriptive and predictive methods—and increasingly, causal tools to support strategy.\nCommunication and storytelling\nYou must craft narratives that are not only statistically sound, but also understandable, relevant, and actionable.\n\nThese skills don’t develop overnight. They grow through practice—through working on real problems, engaging with real data, and reflecting on how analysis shapes action. In the next chapter we will devote more time to to thinking about the skill set of a modern business analyst.\n\n\n1.7.3 Collaboration is essential\nBusiness analysts don’t work alone. They collaborate with data engineers, who build the infrastructure and pipelines that make analysis possible; with business stakeholders, who define the problems and act on the results; and with other analysts, who bring complementary expertise and perspectives.\nThe analyst is the glue—the person who keeps the analytical process connected to business goals, ethical standards, and practical decisions.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is Business Analytics?</span>"
    ]
  },
  {
    "objectID": "foundations/what_is_business_analytics.html#how-to-approach-this-course-and-have-fun-with-it",
    "href": "foundations/what_is_business_analytics.html#how-to-approach-this-course-and-have-fun-with-it",
    "title": "1  What is Business Analytics?",
    "section": "1.8 How to Approach This Course (And Have Fun with It!)",
    "text": "1.8 How to Approach This Course (And Have Fun with It!)\nBy now, you’ve seen what business analytics is really about: using data to make better decisions, combining technical tools with human insight, and communicating clearly to create value. You’ve also seen that becoming a business analyst means developing a wide-ranging skillset—one that draws on statistics, strategy, storytelling, and judgment.\nBut knowing what the role requires is just the start. Now comes the fun part: learning how to do it.\nThis course is your first step into the world of business analytics—and it’s meant to be challenging, empowering, and fun.\nWe’ll move from data to narrative. From raw spreadsheets to compelling stories. From abstract problems to concrete insights that help real businesses make smarter decisions.\nYou won’t just learn how to analyze data. You’ll learn how to think like a business analyst: someone who blends technical skills with business understanding and communicates insights clearly and persuasively.\nOur goal is to help you develop the mindset of someone who is curious about the world, rigorous with evidence, and focused on impact. Someone who doesn’t just look at data, but learns from it—and helps others do the same.\n\n\n\n\n\n\nNoteWhat if I don’t know how to do all of that yet?\n\n\n\nThat’s the point. You’re not expected to know everything now.\nAnalytics is a skill you build through doing—not memorizing. Come to class with a growth mindset: it’s okay to be unsure, to try something that doesn’t work, to ask a question that feels basic. That’s how you learn.\nWhat we ask of you:\n\nBe curious\n\nBe open to feedback\n\nBe willing to try, fail, and improve\n\nAsk questions—lots of them\n\nAnd support each other as we learn\n\n\n\nThis course will give you a solid foundation in the skills of a business analyst. But more than that, we hope it gives you a new way of seeing the world: as a place full of data, decisions, and opportunities to think better.\nNext, we’ll explore the key skills that make a great business analyst—and begin to develop the mindset of someone who approaches business problems like a scientist.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is Business Analytics?</span>"
    ]
  },
  {
    "objectID": "foundations/how_to_do_business_analytics.html",
    "href": "foundations/how_to_do_business_analytics.html",
    "title": "2  The Core Skills of a “Business Scientist”",
    "section": "",
    "text": "2.1 Becoming a Business Scientist: A Scientific Mindset Within a Business\nMany students begin a business analytics course expecting to learn tools—how to write some code, build some charts, maybe even fit a model or two. And those are important outcomes. But learning the foundations of business analytics means more than just acquiring technical capabilities. It’s about becoming someone who can use data to tackle messy, ambiguous, and important business problems. That’s a different kind of challenge. It requires not just tools, but judgement. Not just answers, but knowing how to formulate better questions and how to choose the right tools to answer them.\nThis means being able to think conceptually about a problem before diving into data or code. That includes understanding the context, identifying what matters, and knowing how different kinds of questions—descriptive, predictive, or causal—can shape the analysis. It means treating analysis not just as a task, but as a form of problem-solving. This is the mindset of a business scientist: someone who combines data, domain knowledge, and structured reasoning to help businesses make better decisions. This is the identity we’ll use throughout the rest of the book. It’s an opinionated choice, but a deliberate one. We believe “business scientist” captures the blend of analytical skill, conceptual thinking, and real-world impact that defines the kind of analyst we’re shaping you to become. It reflects not just what you do, but how you think—and the value you bring to the organisations you can aspire to work with in the future.\nThis chapter introduces the foundational skills that support that kind of thinking and working. These skills aren’t tied to any one analytics tool, task or business discipline. Instead, they shape how you approach problems, how you engage with data, and how you turn analysis into insight and action. Some of them are technical—like transforming and modelling data. Others are strategic—like understanding the business context or framing a problem in the right way. Still others are communicative—like helping a team of non-technical experts see the story behind the numbers. All of these skills are part of doing good business analytics.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Core Skills of a \"Business Scientist\"</span>"
    ]
  },
  {
    "objectID": "foundations/how_to_do_business_analytics.html#becoming-a-business-scientist-a-scientific-mindset-within-a-business",
    "href": "foundations/how_to_do_business_analytics.html#becoming-a-business-scientist-a-scientific-mindset-within-a-business",
    "title": "2  The Core Skills of a “Business Scientist”",
    "section": "",
    "text": "NoteThinking Like a Business Scientist\n\n\n\nScience is about systematically learning from the world. Scientists don’t just collect data for the sake of it—they begin with questions, develop hypotheses, and design ways to test them. They use evidence to update their beliefs and remain open to the possibility that they might be wrong. Thinking like a scientist means embracing uncertainty, asking sharp questions, and letting the data guide you—rather than simply confirming what you already believe.\nBusiness scientists do the same thing, but in a commercial setting. The tools might look different, and the decisions might need to move faster, but the mindset is the same: stay curious, stay structured, and stay open to what the data is telling you.\nIn business contexts, the questions are often practical—and the stakes are real. You might find yourself investigating:\n\nWhy a product isn’t selling as expected\n\nWhether a loyalty program actually increased retention\n\nHow to segment your customer base more effectively\n\nIn each case, you’re not just reporting numbers. You’re solving a puzzle. You’re forming hypotheses and testing them against available data. You’re navigating messy constraints, incomplete information, and real-world pressure. And you’re building a case that helps someone else—your manager, your client, your team—make a better decision.\nThat’s what it means to think like a business scientist: to combine data and discipline with creativity and judgment, all in service of making sense of the world and creating value from that understanding.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Core Skills of a \"Business Scientist\"</span>"
    ]
  },
  {
    "objectID": "foundations/how_to_do_business_analytics.html#from-mindset-to-method",
    "href": "foundations/how_to_do_business_analytics.html#from-mindset-to-method",
    "title": "2  The Core Skills of a “Business Scientist”",
    "section": "2.2 From Mindset to Method",
    "text": "2.2 From Mindset to Method\n\n\n\nThe Business Scientist — part detective, part strategist, part storyteller\n\n\nSo what does this mindset actually look like in practice? Here’s how a business scientist approaches problems—not with a fixed recipe, but with a way of thinking that guides every step of the analysis.\n\nForming hypotheses You don’t dive into data blindly. You start by thinking: What might be driving this outcome? What would I expect to see if that explanation were true?\nTesting ideas with data You treat data as evidence. Your ideas aren’t valid because you believe them—they’re supported because the evidence suggests they hold up.\nIterating and refining You rarely get everything right on the first attempt. You explore, learn, revise, and improve. That’s not a weakness—it’s how good analytics works.\nCommunicating clearly. You translate complexity into clarity. You don’t just present findings—you tell a story that helps people act on what you’ve discovered.\n\nThink of the business scientist as a motley mix of three personalities — part detective, part strategist, part storyteller. Their role, which will be yours in time, is to uncover patterns, test explanations, and help businesses make smarter, evidence-based decisions. Companies today sit on mountains of data. But data alone doesn’t create value. Thinking well with data—asking good questions, finding the right evidence, drawing clear conclusions—is what transforms information into insight.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Core Skills of a \"Business Scientist\"</span>"
    ]
  },
  {
    "objectID": "foundations/how_to_do_business_analytics.html#the-8-skills-of-a-business-scientist",
    "href": "foundations/how_to_do_business_analytics.html#the-8-skills-of-a-business-scientist",
    "title": "2  The Core Skills of a “Business Scientist”",
    "section": "2.3 The 8 Skills of a Business Scientist",
    "text": "2.3 The 8 Skills of a Business Scientist\nAs we’ve discussed so far, doing business analytics isn’t just about knowing which buttons to press. It’s about how you think—and how you apply that thinking to solve real problems. This emphasis on thinking clearly with data is what sets great business scientists apart.\nIn many workplaces, the focus of those working in business analytics roles often drifts toward tools—what software someone can use, how many models they’ve memorised. But tools are only part of the picture. The best business scientists succeed because of their skillset: the habits of mind, ways of working, and communicative strengths that help them adapt to new problems, collaborate with others, and turn analysis into action. The tools everyone wants to talk about tend to come and go — often too fast for you to reach true proficiency before newer ones take over. Skills, by contrast, stick with you. In what follows, we’ll outline what we believe are the eight essential skills that form the foundation of thinking and working like a business scientist.\nHere are 8 essential skills that make up the foundation of great business analytics:\n\n1. Institutional Detail: Understanding the Business World\nBusinesses don’t operate in a vacuum. Each one exists within a broader ecosystem—shaped by its customers, competitors, suppliers, regulations, the wider industry it operates within and traditions. Great business scientists take the time to understand how their organization works from the inside out. Just like a biologist wouldn’t study an animal without first understanding its habitat, a business scientist doesn’t analyze data without grasping the environment the business operates in. Context isn’t background noise—it’s the terrain in which every decision plays out.\nThis means learning about pricing structures, seasonality, logistics, marketing strategies, regulatory constraints, and competitive dynamics. It means understanding the industry’s rhythms: when do customers typically buy? What external factors drive demand? How does the sales process actually work? Without this context, it’s easy to misinterpret what the data is telling you.\nWe think of learning institutional details as becoming fluent in your company’s language. Every business has its own vocabulary, its own calendar of important events, and its own unwritten rules about how things get done. The more you understand these local dynamics, the better you’ll be at spotting meaningful patterns in the data—and avoiding misleading ones.\nWhile this step can feel dry or peripheral at first, it’s anything but. The deeper you go, the more you’ll find that institutional knowledge doesn’t slow analysis down—it sharpens it. It’s what lets you ask smarter questions, rule out bad explanations, and bring a level of insight that purely technical analysts often miss.\n\n\n\n\n\n\nTipExample: Thinking with Context at Starbucks\n\n\n\nBefore diving into Starbucks sales data, a business scientist would first step back and learn how the business actually works. That means understanding:\n\nSeasonal patterns, like when the Pumpkin Spice Latte appears—and how limited-time drinks create predictable spikes in demand\n\nLoyalty mechanics, such as how stars are earned and redeemed, and how that influences purchasing behaviour\n\nStore types, since an airport kiosk has very different dynamics than a suburban drive-thru or university campus location\n\nOperational rhythms, like the intensity of the morning rush, how new drinks are rolled out, or how promotions vary across regions\n\nAll of these shape what success looks like—and what patterns in the data might actually mean.\nWhat could go wrong?\nSuppose you notice a sharp drop in cold brew sales during one week of the quarter. Without context, it’s tempting to infer a shift in customer preferences or the failure of a promotion. But a closer look might reveal something entirely different: a temporary supply chain issue that led to the coffee beans used to make cold brew being unavailable in many stores, or a change in the loyalty program that nudged customers toward redeeming rewards for other products. The data didn’t lie—but interpreting it without understanding the business can lead to the wrong story.\n\n\n\n\n2. Data Discovery: Finding the Right Data\nData discovery is the art of figuring out what information exists—and what’s actually useful. Real-world data is messy and scattered. It lives in spreadsheets, dashboards, PDFs, databases, websites, cloud tools, and even emails. Analysts must be part detective, knowing where to look and who to ask. This involves both technical knowledge, understanding different data formats and storage systems, and social intelligence, knowing which colleagues or third-party data providers have access to what information. But finding the data is only the beginning.\nOnce you’ve tracked it down, you need to map the data landscape: What gets measured? What doesn’t—and why? Who owns which datasets? How frequently is information updated? Just as importantly, you need to assess quality and completeness. Is this data reliable? Are there systematic gaps? Does it actually capture what we care about?\nSometimes, the data you want doesn’t exist at all. In those cases, the your job is to get creative: to find proxies, build new measures, find ways to actively collect the data you need, or rethink the question in a way that makes progress possible.\n\n\n\n\n\n\nTipExample: Estimating Profit Margins When You Don’t Have Cost Data\n\n\n\nImagine you’re working with sales data from a multinational retailer. The dataset includes product-level revenue, discounting, and returns—but there’s no reliable cost data. You’ve been asked to estimate profit margins across product categories.\nAt first, it seems impossible. But a business scientist doesn’t stop there.\nYou start by mapping what is available: supplier codes, shipping methods, inventory turnover, and some high-level financial statements. Then you talk to colleagues in finance and procurement. Eventually, you piece together a working solution:\n\nYou estimate average unit costs using inventory and COGS figures from quarterly reports.\n\nYou use product weight and supplier codes to model shipping costs.\n\nYou apply assumptions based on margin benchmarks in similar categories.\n\nIs it perfect? No. But it’s documented, grounded, and far better than guessing—or giving up. That’s what good data discovery looks like in practice: not just finding data, but building a defensible understanding of what the data can and cannot tell you.\n\n\n\n\n3. Business Theory: Understand People and Firms\nBehind every data point is a decision. Whether it’s made by a customer, an employee, or an executive, that decision reflects goals, trade-offs, habits, and constraints. If we want to interpret data meaningfully, we need to understand what motivates the people and organisations behind it.\nThat’s where business theory comes in. Theory gives you mental models—ways of thinking—that help you make sense of patterns. You’re not just memorizing textbook definitions of “demand” or “utility.” You’re using concepts like pricing strategy, consumer psychology, supply and demand, and organizational incentives to interpret what you’re seeing in the data.\nWhy might sales spike on certain days? What drives customer loyalty? How do competitors respond to a new product launch? These aren’t just descriptive questions we answer with data —they’re behavioural ones. Theory helps you move from what happened, to why it happened, to what might happen next. In that sense, theory helps you uncover the mechanism—or more informally, the “why”—behind the patterns the data reveal.\nGood business scientists borrow the tools of economists and psychologists among others. They think in incentives, trade-offs, and feedback loops. They understand that behaviour is often predictable—and that understanding those patterns is key to turning data into insight.\n\n\n\n\n\n\nTipExample: Why Are the Employees Quitting?\n\n\n\nImagine you’re analyzing employee turnover at a large firm. One team has unusually high resignation rates, and leadership wants answers. You start by examining basic patterns—tenure, role, compensation, manager—but nothing obvious jumps out.\nA surface-level analysis might stop there. But a great business scientist brings theory to the table.\nYou recall models of motivation and incentives: Is the bonus structure rewarding the right behaviours? Are promotion paths clear? You consider organizational dynamics: Has there been a change in leadership or reporting lines? Are people leaving for competitors with a more flexible culture?\nYou dig deeper and discover that while pay is competitive, the team’s recent restructuring removed autonomy from senior staff—shifting decision-making to HQ. That’s not in the spreadsheet, but it explains the exits.\nTheory didn’t just help you describe what was happening—it helped you understand why it was happening, and what might happen next if the structure doesn’t change.\n\n\n\n\n4. Analytical Modelling: Choose the Right Tool\nDifferent business problems require different methods. Sometimes a simple average is all you need. Other times, the problem calls for predictive algorithms, statistical models, or machine learning techniques. Great business scientists know how to match the complexity of the tool to the complexity of the task.\nThat means more than just knowing how to run a model—it means understanding the assumptions behind different approaches. When is a correlation meaningful? When do you need a more rigorous method to support a causal claim? How do you trade off accuracy against interpretability? The goal isn’t to reach for the fanciest method — it’s to choose the one that fits the question, the data, and the decision context.\nAnd just as importantly, good analysts know when not to model. Sometimes the clearest and most compelling answer comes from a simple table or visualisation. Sometimes the question isn’t answerable—yet—because the right data doesn’t exist. Exercising judgment about when to model, how to model, and when to hold back is what separates technically competent business scientists from strategic, decision-focused ones.\n\n\n\n\n\n\nTipExample: Planning Fresh Produce Orders at Coles\n\n\n\nImagine you’re helping Coles figure out how much fruit and vegetables to order each week. The goal is to keep the shelves stocked—but not waste food.\nYou start with something simple: looking at the average sales from past weeks. It works okay—until a public holiday or heatwave throws things off.\nNext, you try a more advanced method that also considers the season, day of the week, and weather. It works better, but someone suggests using a really complex model that nobody on the store team understands. You push back—because if the store managers don’t trust the numbers, they won’t use them.\nAnd for rare items with very little data, you decide not to build a model at all. You just show the trends in a simple chart.\nThe skill: A good business scientist knows when a simple method is enough, when a more advanced one helps, and when to just keep it clear and practical. It’s not about the flashiest tool—it’s about using the right tool for the problem.\n\n\n\n\n5. Analytical Theory: Know What Can Go Wrong\nWorking with data and analytical models is powerful—but it can also be risky. A misleading analysis can lead to the wrong decision, and in a business setting, that can be costly. Great business scientists are curious, but also careful. They think critically about whether the data and the method are really giving them the full picture.\nThis skill is closely connected to what you might hear in your statistics or quantitative methods classes: statistical thinking. It’s the habit of reasoning about data in a structured, careful way. It means questioning your results, spotting weak points in your reasoning, and understanding what your analysis can (and can’t) actually say.\nThat includes asking tough but important questions:\n\nAre we sure we’re measuring what we think we’re measuring?\nCould something else be causing this pattern?\nWould our conclusions change if we looked at different data—or looked at it a different way?\nWhat assumptions are we making—and are they reasonable for this situation?\n\nYou don’t need to know all the technical terms yet. What matters is developing the habit of looking under the hood. Sometimes your data leaves out important groups of people. Sometimes two things move together just by coincidence. Sometimes your analysis works well on past data but falls apart in the real world. The key idea is to always ask: What might I be missing, and how could that change the story the data appears to tell?\nThe best business scientists aren’t afraid to poke holes in their own work. They look for alternative explanations, check their assumptions, and clearly explain what their results do—and don’t—mean. That’s not being negative. That’s being honest about what the evidence actually supports. It’s what makes your work trustworthy—and it’s one of the most valuable habits you can develop. Some might say business scientists get a little too good at this—always finding what could go wrong. But don’t worry, it’s what makes your insights dependable, not just impressive.\n\n\n\n\n\n\nTipExample: Did the Loyalty Program Really Work?\n\n\n\nSuppose a retailer launches a new loyalty program, and soon after, analysts notice that members are spending more than non-members. The initial report celebrates the program’s success.\nBut one analyst pauses: were customers spending more because of the program, or were the high spenders just more likely to sign up?\nThere was no clear before-and-after comparison. No comparison group who weren’t offered the option to sign-up. No test of whether the program caused the change. The team had found a pattern—but assumed it meant impact.\nAnalytical thinking in action: Good analysts don’t just spot patterns—they ask what explains them. Correlation alone isn’t enough. Without the right comparisons and reasoning, your analysis might be telling the wrong story.\n\n\n\n\n6. Computational Skills: Work with Code and Technology\nToday’s business scientists work with code. Languages like R allow you to clean data, test models, automate your workflow, and produce results that others can reproduce and build on. Knowing how to code doesn’t just make you faster—it makes your work clearer, more flexible, and more independent. You’re not waiting around for someone else to pull the data or run the analysis—you can do it yourself.\nBut this isn’t about becoming a software engineer. It’s about being comfortable enough with technology to solve data problems efficiently. That means knowing how to load data, reshape it, create useful visualisations, implement an analytical model, and document your process so that others can follow what you did.\nIt also means working across tools and platforms. Modern analytics happens in spreadsheets, databases, dashboards, cloud services, and version-controlled files. The specific tools will change over time—but the underlying mindset stays the same. Business scientists learn to break problems into steps, fix things when they break, and automate repetitive tasks so they can focus on the thinking.\n\n\n\n\n\n\nTipTip: Learning to Code Is Like Learning to Cook\n\n\n\nThink of coding like learning to cook. At first, it can feel daunting—just like the first time you turn on the gas stove and see real flames and worry you might burn down the house. But once you get used to it, the heat becomes something you learn to control and use.\nEarly on, you follow recipes—step-by-step instructions written by others. You might not fully understand why something works, but you can still get a decent result.\nOver time, you start to improvise. You recognise patterns, adapt techniques, and build your own workflows. You learn to debug when something “burns,” and eventually, you create your own reusable recipes for solving data problems.\nYou don’t need to become a professional chef—or a software engineer. But having basic coding skills gives you independence, creativity, and confidence in the analytics kitchen.\n\n\n\n\n7. Presentation & Communication: Communicating what you find to non-specialists\nGreat insights don’t speak for themselves. You need to help them speak to others. Whether it’s through a graph, a dashboard, slide deck or a short written brief, business scientists shape their findings into clear, purposeful stories that help others understand what the data shows—and what to do about it.\nGood communication starts with your audience. What decisions are they trying to make? What do they need to know to move forward? Different audiences need different approaches. Executives often want clarity: What’s happening, and what should we do about it? Your fellow analysts might care more about the method, the data sources, or the assumptions you made. A great business scientist can move between both—zooming out to frame the big picture, or zooming in when detail matters.\nThis kind of communication isn’t about flashy visuals or technical jargon. It’s about structured thinking, clean presentation, and sharp writing. That means:\n\nDesigning graphs, tables and slides that show exactly what matters—no more, no less\nTelling a story with a beginning, middle, and end\nAnticipating questions before they’re asked\nHighlighting the recommendation, not just the result\n\nThe goal isn’t to impress people with how much you know. It’s to make it easier for them to think clearly, decide confidently, and take the next step.\n\n\n\n\n\n\nTipExample: The Executive Slide That Looked Like a Stats Exam\n\n\n\nA team prepares a presentation for senior leadership to explain the results of an A/B test. They include p-values, confidence intervals, model coefficients, and three dense charts about statistical assumptions.\nWhen they present, the executives skim the slide and ask, “So… should we roll out the new version or not?”\nThe analysis was solid—but the message didn’t land.\nThe lesson: Even the best results can fall flat if the audience can’t see the relevance. For decision-makers, lead with the insight—not the math.\n\n\n\n\n\n\n\n\nTip📊 Example: The Analyst Who Got the ‘So What?’ Question\n\n\n\nAn analyst builds a clean, interactive dashboard showing churn rates across products and regions. They walk through the filters and visual options, confident in the design.\nBut when they present to a room of non-technical managers, someone interrupts: “This looks great—but what are we supposed to do with it?”\nThere’s no interpretation. No recommendation. Just data.\nThe lesson: A beautiful dashboard might not be enough for analytics focused audiences. If your audience can’t see what matters—or what to do—it won’t drive action.\n\n\n\n\n8. Creativity & Judgment: Asking the Right Questions\nBusiness analytics starts with curiosity. Before you can run any analysis, you need a question worth answering. Great business scientists don’t just respond to requests—they help shape them. They turn vague goals like “improve customer experience” or “reduce churn” into specific, testable questions that data can help resolve. Framing the problem is the first step toward solving it.\nThis takes both creativity and judgment. Creativity helps you see problems from new angles, reframe challenges, and imagine what might be possible. Judgment helps you filter: Which questions are actually worth answering? Which ones tie to real business decisions? The best questions are those that are both interesting and useful—they push thinking forward and point toward action.\nThis skill also involves timing. You won’t always be given a clean, well-defined problem to solve. Sometimes, the brief is too broad. Other times, the data reveals something surprising, and you have to decide whether it’s a meaningful lead or just noise. Knowing when to dig deeper and when to move on is part of the craft.\nThe best business scientists act like investigators. They notice when something doesn’t add up. They wonder about cause-and-effect relationships. They play out “what if” scenarios in their head before opening the dataset. This kind of curiosity—anchored by a strong sense of business relevance—is what helps them identify the questions that will lead to insights, and the insights that will lead to better decisions.\n\n\n\n\n\n\nTipTip: A Good Question Is Half the Work\n\n\n\nWhen you’re stuck in an analysis, take a step back and ask yourself:\nWhat exactly am I trying to learn—and why does it matter?\nGood business scientists don’t just dive into data—they pause to clarify the question. They ask things like:\n\nIs this question specific enough to answer?\nWhat would a useful answer look like?\nWho needs to make a decision based on this?\n\nFraming a question well often makes the rest of the analysis easier. It tells you what data to look for, what methods to use, and when you’re done.\n\n\n\n\n10 min\n\n\n📝 Check-In: Which Skills Do You Already Have?\nTake a moment to reflect. Which of the 8 skills below do you feel most confident in? Which ones do you want to develop this semester?\n\n\n\nSkill\nConfidence Level (1 = low, 5 = high)\n\n\n\n\nUnderstanding the business world\n☐ 1 ☐ 2 ☐ 3 ☐ 4 ☐ 5\n\n\nFinding the right data\n☐ 1 ☐ 2 ☐ 3 ☐ 4 ☐ 5\n\n\nUnderstanding how people and firms work\n☐ 1 ☐ 2 ☐ 3 ☐ 4 ☐ 5\n\n\nChoosing the right model or tool\n☐ 1 ☐ 2 ☐ 3 ☐ 4 ☐ 5\n\n\nKnowing what can go wrong\n☐ 1 ☐ 2 ☐ 3 ☐ 4 ☐ 5\n\n\nCoding and working with data tools\n☐ 1 ☐ 2 ☐ 3 ☐ 4 ☐ 5\n\n\nCommunicating insights\n☐ 1 ☐ 2 ☐ 3 ☐ 4 ☐ 5\n\n\nAsking good questions\n☐ 1 ☐ 2 ☐ 3 ☐ 4 ☐ 5\n\n\n\nReflection questions:\n\nWhich skill are you most excited to build?\nWhich skill do you think businesses value most? Why?\n\nWe’ll revisit these questions throughout the course. For now, share your thoughts with a classmate—or drop your top skill in the class poll.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Core Skills of a \"Business Scientist\"</span>"
    ]
  },
  {
    "objectID": "foundations/how_to_do_business_analytics.html#applying-and-combining-the-skills",
    "href": "foundations/how_to_do_business_analytics.html#applying-and-combining-the-skills",
    "title": "2  The Core Skills of a “Business Scientist”",
    "section": "2.4 Applying and Combining the Skills",
    "text": "2.4 Applying and Combining the Skills\nNot every business scientist uses every skill in equal measure. Some of the skills you’ve just seen—like communicating clearly, thinking critically with data, or understanding the business context—will show up in nearly every project you work on. Others—like predictive modelling or building dashboards—depend more on your specific role, your team, or the kinds of problems you’re tackling.\nWhat we’ve offered so far is a starting point: a map of what it means to think and work like a business scientist. You won’t need to master all these skills overnight. But recognising which ones are foundational versus role- or project-specific can help you focus your development—and understand what makes different paths within analytics both challenging and rewarding.\nSome of these differences also help explain why people are drawn to different types of analytics work. If you enjoy testing ideas and running experiments, you might be interested in projects that compare what happens when a business changes something—like a website layout or a pricing offer. If you like explaining things and helping others make sense of data, you might prefer working on reports, dashboards, or presentations. And if you enjoy solving puzzles behind the scenes, you might gravitate toward tasks like cleaning data, building models, or improving how information flows through a system.\nIn what follows, we’ll explore how these skills show up across different kinds of analytics roles and how they come together in practice.\n\nUniversal Skills: Always Needed\nSome skills form the foundation of good analytics practice. They willshow up in nearly every project, across all industries, and regardless of what tools or techniques you use. Think of these as the core muscles of a business scientist—skills that support everything else.\nUnderstanding the Business World is perhaps the most universal skill of all. No analysis happens in a vacuum. Whether you’re writing a simple sales summary or building a complex model, you need to understand how the business works in order to interpret patterns and give your results meaning. Without this context, even technically correct analysis can lead to irrelevant—or misleading—conclusions.\nFinding the Right Data is just as essential. Good decisions require good inputs. Whether you’re scraping a website, querying a database, conducting a survey, or combining messy spreadsheets, the challenge is the same: figure out what information matters, and track it down. It’s not just about technical skills—it’s about knowing what to look for and why it matters.\nCommunicating What You Find is the bridge between analysis and action. If you can’t explain your findings in a clear and compelling way, your work won’t get used—no matter how insightful it is. Whether through graphs, slides, dashboards, or short write-ups, you need to help others understand what the data shows and what it means for them.\nAsking the Right Questions is what sets great analysts apart, and arguably the most useful of the universal skills. Every project begins with curiosity: Why is this happening? What should we try next? What don’t we know yet? Knowing how to frame a focused, answerable, and relevant question is a skill in itself—and one that applies to every kind of analytics work.\nTogether, these four skills show up again and again. They’re not tied to any one job title or toolset—they’re what make you effective, no matter where you apply them.\n\n\nRole- or Project-Specific Skills: Context-Dependent\nThe remaining skills become more or less important depending on your role, your industry, or the kind of problems you’re solving. This is where specialization begins—where business scientists develop deeper strengths based on what they do and where they work.\nUnderstanding People and Firms becomes especially important in roles focused on strategy, consulting, or customer insights—anywhere human behavior and incentives drive outcomes. If you’re investigating why customers leave, how employees react to policy changes, or how a competitor might respond to a price cut, this skill goes from helpful to essential.\nChoosing the Right Tool is crucial for roles that involve forecasting, optimization, or experimentation—like pricing analysts, risk managers, or product data scientists. In these roles, you’re not just analyzing past patterns—you’re building models that inform real decisions. That means knowing which method fits the question, the data, and the stakes.\nKnowing What Can Go Wrong becomes especially important when the decisions being made based on your analysis carry significant risk—whether that’s financial, reputational, or strategic. If your findings influence pricing changes, hiring decisions, product launches, or policy shifts, mistakes can be costly. In these situations, being able to identify assumptions, check for errors, and communicate uncertainty clearly isn’t just a bonus—it’s part of being a responsible business scientist.\nWorking with Code and Technology varies the most across roles. If you’re building data pipelines, automating reports, or working with big datasets, this skill is essential. But in roles focused more on communication or insight interpretation, you might rely more on pre-built dashboards or visual tools. Either way, having some comfort with code gives you flexibility and independence.\n\n\n2.4.1 Mini Case: Different Roles, Same Goal\nStreamly, a fictitious fast-scaling global music streaming service, has recently expanded into over a dozen countries across Europe, Asia, and Latin America. But something’s wrong—churn rates in some regions are much higher than expected. The executive team wants to know why users are leaving—and what can be done to retain them.\nThree different business scientists are brought in to investigate. Each brings a different background, focus, and skill mix. Their conclusions differ—but each adds a crucial piece of the puzzle:\n🤖 Product Data Scientist\nThe Data Scientist works with millions of user-level records. Using survival models and machine learning techniques, they identify early warning signs of churn. The models reveal that users who don’t engage with playlists or explore new genres in their first 10 days are much more likely to cancel. Their recommendation? Introduce smarter onboarding journeys that promote exploration and improve the relevance of recommendations.\nKey skills: Choose the Right Tool + Work with Code and Technology + Know What Can Go Wrong\n🌍 Consumer Insights Manager\nThe Consumer Insights Manager conducts interviews and analyzes survey data across regions. They uncover that musical tastes and content expectations differ widely. In some markets, users feel disconnected due to a lack of local artists or region-specific features like live event tie-ins. Their recommendation? Build stronger regional editorial teams and tailor content strategies to local cultures.\nKey skills: Understand People and Firms + Find the Right Data + Understand the Business World\n📊 Strategy and Operations Analyst\nThe Strategy and Operations Analyst benchmarks Streamly’s pricing, bundling, and distribution models against local competitors. Their research shows that high-churn markets often feature competitors with more flexible plans—like student discounts or weekly passes. Their insight? Product-market fit includes price. Their recommendation? Introduce locally responsive pricing structures.\nKey skills: Understand the Business World + Ask the Right Questions + Understand People and Firms\n\n\n10 min\n\n\n📝 Check-In: Questions for Reflection\n\nWhat kind of analyst would you most want to be in this situation? Why?\nIf you were leading the project, how would you decide which recommendation to prioritize first?\nAre there any skills missing from this team that could make the solution even stronger?\nHow might cultural differences, platform design, or competitive strategy interact in ways that none of the analysts saw on their own?\n\n\n\n\nKey Takeaway\nDifferent skill combinations reveal different dimensions of the same problem. True impact often comes from integrating insights across roles. Business scientists don’t just answer questions—they collaborate to shape better ones.\nWhile every business scientist needs a broad foundation across all eight skills, your role and interests will naturally shape which ones you develop most deeply. One size doesn’t fit all—but every project needs strong fundamentals.\nAs you progress in your career, you’ll likely find yourself gravitating toward certain types of problems that align with your strongest skills. That’s not just natural—it’s strategic. The best analysts understand their own capabilities and seek out projects where they can have the greatest impact.\nThe goal isn’t to be equally strong in everything. It’s to be strong enough in the fundamentals to be effective anywhere, while developing specialized expertise in the areas that matter most for the work you want to do.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Core Skills of a \"Business Scientist\"</span>"
    ]
  },
  {
    "objectID": "foundations/how_to_do_business_analytics.html#building-a-versatile-skill-set",
    "href": "foundations/how_to_do_business_analytics.html#building-a-versatile-skill-set",
    "title": "2  The Core Skills of a “Business Scientist”",
    "section": "2.5 Building a Versatile Skill Set",
    "text": "2.5 Building a Versatile Skill Set\nDeveloping as a business scientist is a marathon, not a sprint. The eight skills you’ve just explored offer a roadmap—but no one masters all of them at once. Some will feel intuitive from the start. Others may take time, practice, and the right context to develop. The key is to build systematically, not randomly: start with a clear sense of where you are, where you want to go, and how your skills align with the kinds of problems you’re most excited to solve.\n\nHow to Identify Which Skills to Develop\nStart with an self-assessment. Reflect on projects you’ve worked on—in class, during internships, or in previous roles. What parts came naturally? Where did you struggle? This isn’t about self-criticism—it’s about gaining insight into your current strengths and identifying areas for growth.\nFor each of the eight skills, think of one time you used it well, and one time you wished you were stronger. Maybe you’re great at spotting patterns in messy datasets, but find it difficult to communicate your findings to non-technical audiences. Or perhaps you have strong intuition about business problems, but aren’t confident choosing the right analytical method.\nOnce you’ve taken stock of your skills, look ahead. What kinds of roles interest you? Not just job titles—but the actual day-to-day work. A “data analyst” at a government agency might focus on policy dashboards and stakeholder communication. That same title in a tech startup might involve coding, experimentation, and product analytics. Research job postings, talk to people in roles that intrigue you, and pay attention to which skills show up again and again. Look beyond technical requirements—many roles quietly demand strengths in business understanding, communication, and good judgment.\nNow that you’ve reflected, you have the start of the game plan. You know which skills you want to develop. Now its about developing them.\n\n\n\n\n\n\nTipTip: Revisit Your Skills Over Time\n\n\n\nYour interests will evolve. So will your strengths.\nCome back to this self-assessment exercise at key points in your journey— such as in the middle or at the end of this or other courses, after an internship, before you start your first job or when you are looking for a new job. You’ll often discover new strengths you didn’t know you had, or areas you’ve grown in more than expected.\nAs you try different kinds of work and collaborate with people who bring out different sides of you, you’ll get a better sense of what energizes you—and what kind of business scientist you’re becoming.\nThink of this as your personal skill map. Update it often.\n\n\n\n\nDeveloping Skills Over Time\nThe temptation is often to chase the flashiest skills first. But the strongest business scientists build from the ground up. Focus on the fundamentals before diving deep.\nStart with the four universal skills:\n\nUnderstanding the business world\nFinding the right data\nCommunicating clearly\nAsking the right questions\n\nThese aren’t optional extras—they’re the foundation that makes every other skill more effective. You can’t choose the right model without understanding the problem. You can’t drive action if you can’t explain your findings well. Throughout this book you will be using these skills in every chapter and in every tutorial exercise.\nAs these foundations grow are solid, choose one or two specialization areas to develop more deeply. If you’re drawn to strategy, focus on business theory and behavioral insight. If you love solving technical puzzles, work on analytical modeling and computational skills. We will help you develop each of these gradually over the following chapters, by introducing you to business problems that need theoretical underpinnings and developing your analytical skills to answer these business problems with data and code. While this book blends these two components equally to build uo your foundational knowlegde, as you continue your journey towards becoming a business scientist you can choose how to specialize further.\nHere’s how to approach skill development practically:\nThrough Projects: The best learning happens when you’re trying to solve real problems. Seek out opportunities to work on projects that stretch your capabilities. Volunteer for analyses that require skills you want to develop. Take on side projects that let you experiment with new tools or techniques.\nThrough Courses: Formal learning provides structure and depth. But choose courses strategically. If you’re weak on fundamentals, prioritize business courses, statistics, and communication training over advanced machine learning. If you’re strong on basics, dive deeper into specialized areas.\nThrough Collaboration: Work with people whose skills complement yours. Partner with someone who’s strong where you’re weak. Watch how experienced analysts approach problems. Ask questions about their process, not just their conclusions.\nThrough Reflection: Periodically spend time thinking about what went well and what didn’t. Which skills made the biggest difference? Where did you get stuck? What would you do differently next time? This reflection turns experience into expertise.\nRemember: skill development never stops. Technology changes, industries evolve, and new analytical methods emerge. The goal isn’t to master everything once—it’s to build a foundation that lets you adapt and grow throughout your career. The best business scientists are curious, self-aware, and strategic about their development. They know their strengths, understand their gaps, and actively work to expand their capabilities. Most importantly, they never stop learning.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Core Skills of a \"Business Scientist\"</span>"
    ]
  },
  {
    "objectID": "foundations/how_to_do_business_analytics.html#the-business-analytics-workflow",
    "href": "foundations/how_to_do_business_analytics.html#the-business-analytics-workflow",
    "title": "2  The Core Skills of a “Business Scientist”",
    "section": "2.6 The Business Analytics Workflow",
    "text": "2.6 The Business Analytics Workflow\nNow that we’ve seen how different skill combinations shape the way analysts think and work, it’s almost time to put those skills into action. The workflow we’re about to explore offers a structured—but adaptable—way to move from a vague business problem to a clear, evidence-based recommendation. It helps ensure you approach problems with the same mindset as a business scientist: curious, critical, and creative.\nThink of this workflow as your analytical GPS. Just like navigation software, it provides a reliable route while allowing for detours when you discover something unexpected. The workflow doesn’t follow a strict linear order—you might go back and forth between steps as your understanding deepens. That’s not a flaw, it’s part of what makes this process powerful.\nMore importantly, each step in the workflow draws on the skills you’ve been learning about. You’ll see how “Ask the Right Questions” drives the first step, how “Find the Right Data” and “Work with Code and Technology” power the middle steps, and how “Communicate What You Find” brings everything together at the end.\nBelow, we walk through each stage:\n\n1. Define the Business Question and Plan Outputs\nEvery good analysis starts with a clear question. What are we trying to understand, predict, or change? Defining the business question forces clarity about what matters. Just as important is imagining what the final output might look like—a chart, a dashboard, a decision. By sketching the destination, we can plan a smarter route to get there.\nWe also ask: what does success look like? Who is this analysis for? What would make it useful? This step is where your ability to Ask the Right Questions and Understand the Business World becomes crucial.\n\n\n2. Acquire and Prepare the Data\nOnce we know what we’re looking for, we gather the data that might help us find it. This step includes locating sources, cleaning messy files, checking for missing values, and creating new variables that capture meaningful patterns. Preparing data well isn’t glamorous—but it’s where most of the hard work happens. And doing it right sets everything else up for success.\nHere’s where Find the Right Data and Work with Code and Technology take center stage.\n\n\n3. Explore and Visualize Patterns\nBefore jumping into advanced methods, we take time to understand the data through visualization and summary statistics. What do distributions look like? Are there surprising trends or outliers? Do any relationships jump out? This stage is about seeing what the data wants to tell us—and starting to form hypotheses we might want to test more formally.\nThis exploratory phase draws on multiple skills: Ask the Right Questions (what patterns might exist?), Understand People and Firms (do these patterns make business sense?), and Know What Can Go Wrong (are we seeing real patterns or just noise?).\n\n\n4. Analyze and Interpret Findings\nWith a clearer view of the patterns, we bring in formal analytical tools. This might mean running regressions, building prediction models, or estimating causal effects. But analysis doesn’t end with numbers. We need to interpret those numbers in business terms. What do they mean for the question we started with? Do they hold up under scrutiny? Have we validated the results and checked our assumptions?\nThis is where Choose the Right Tool and Know What Can Go Wrong become essential, supported by your understanding of the business context.\n\n\n5. Communicate Insights and Recommendations\nFinally, we present our findings. Not just the numbers—but the story they tell. This includes choosing clear visuals, tailoring the message to our audience, and offering concrete recommendations. A good presentation doesn’t just report—it persuades. It helps others act.\nThe culmination of everything: Communicate What You Find backed by all the analytical rigor that came before.\nTogether, these five steps form a flexible but disciplined approach to solving problems with data. You’ll return to this workflow throughout the course—and throughout your career. Each time you use it, you’ll get better at knowing which skills to emphasize at which stages, and how to adapt the process to different types of business challenges.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Core Skills of a \"Business Scientist\"</span>"
    ]
  },
  {
    "objectID": "foundations/how_to_do_business_analytics.html#analytics-in-the-age-of-generative-ai",
    "href": "foundations/how_to_do_business_analytics.html#analytics-in-the-age-of-generative-ai",
    "title": "2  The Core Skills of a “Business Scientist”",
    "section": "2.7 Analytics in the Age of Generative AI",
    "text": "2.7 Analytics in the Age of Generative AI\nWe’re living through a remarkable moment in business analytics. Artificial intelligence can now write code, generate reports, create visualizations, and even propose business recommendations. For many students, this raises an uncomfortable question: if AI can do analytics, why do we need human analysts?\nThe answer lies in what AI can and cannot do—and why the analytics mindset becomes even more important, not less, in an AI-powered world.\n\nAI and the Analytics Mindset: Why Human Judgment Still Matters\nAI has transformed many parts of analytics. It can summarize findings, generate visualizations, and surface patterns in huge datasets. But it doesn’t yet understand business context. It doesn’t yet know what matters most or how decisions will actually get made. These missing pieces aren’t minor details; they’re central to the business scientist’s job. To see the difference, imagine you take a photo of an important artwork at a museum and ask an AI to explain it to your grandmother. The AI might describe the colors, objects, and even name the painter. But it won’t know why that artwork moves you, or what personal story it tells in your family.\nIn the same way, AI might correctly identify a dip in satisfaction scores—but it won’t know that your company just restructured, or that the drop is actually better than where things were six months ago. This is where human analysts shine. We can interpret AI-generated findings with context, judgment, and awareness of organizational realities. We know when something “doesn’t smell right,” when results are promising but politically difficult, or when a finding isn’t actionable even if it’s true.\nWithout that human touch, AI outputs too often becoming sophisticated noise.\n\n\nAI as a Tool, Not a Replacement\nIt’s tempting to see AI as a magic fix for analytics. But giving someone AI without critical thinking is like handing them a sports car without teaching them to drive. The car is powerful. But without a skilled driver, it’s more dangerous than useful.\nAI is a powerful assistant, not a decision-maker. Here’s how the roles split:\nAI excels at: crunching, or giving you advice on how to crunch large datasets, finding patterns, generating charts, drafting code, and automating tasks.\nHumans excel at: understanding business context, framing questions, judging quality, interpreting ambiguity, and communicating insights with nuance.\nStrong analysts use AI to speed up routine work so they can focus on the harder thinking. They validate outputs, test alternatives, and guide the AI toward useful paths. They remain in the driver’s seat.\n\n\nWhat This Means for Your Development\nRather than making human analysts obsolete, AI raises the bar. Some core skills that we discussed in this chapter become more valuable:\n\nAsk the Right Questions: AI can answer anything—but it can’t tell you which questions matter.\nUnderstand the Business World: AI doesn’t grasp context or strategy. You do.\nKnow What Can Go Wrong: AI is confident, even when wrong. Someone needs to spot the flaws.\nCommunicate What You Find: AI can’t read the room, persuade a team, or tailor a story to your boss.\n\n\n\nLooking Forward: The AI-Augmented Analyst\nThe business scientists who thrive won’t compete with AI—they’ll collaborate with it. They’ll use AI to explore ideas, check assumptions, and speed up their workflow. But they’ll bring what AI can’t: human insight.\nAI is raising the baseline. The value you bring lies in your ability to think clearly, communicate well, and make decisions that account for real-world complexity. That’s the business scientist mindset—and it’s more important now than ever.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Core Skills of a \"Business Scientist\"</span>"
    ]
  },
  {
    "objectID": "foundations/how_to_do_business_analytics.html#whats-next",
    "href": "foundations/how_to_do_business_analytics.html#whats-next",
    "title": "2  The Core Skills of a “Business Scientist”",
    "section": "2.8 What’s Next?",
    "text": "2.8 What’s Next?\nYou now have the foundation every business scientist needs: a clear understanding of the eight core skills and a structured workflow for approaching problems. More importantly, you understand that business analytics isn’t just about technical tools—it’s about thinking clearly and asking smart questions in service of better decisions.\nBut understanding the framework is just the beginning. The real learning happens when you start getting your hands dirty with actual data.\n\nFrom Theory to Practice\nIn the chapters ahead, you’ll stop reading about analytics and start doing it. You’ll work with real datasets from companies across different industries. Each project will give you the chance to practice different aspects of the workflow:\n\nYou’ll wrestle with messy, incomplete data and learn how to clean it systematically\nYou’ll explore datasets to uncover unexpected patterns and form hypotheses about what’s driving them\n\nYou’ll choose analytical methods that match your questions and learn when simple approaches work better than complex ones\nYou’ll create visualizations that actually communicate insights rather than just displaying numbers\nYou’ll present findings to different audiences and discover how the same analysis can be framed in multiple ways\n\n\n\nBuilding Your Analytical Intuition\nReading about the analytics mindset is one thing; developing it is another. As you work through upcoming projects, you’ll start to build the intuition that experienced analysts rely on—that sense of when something doesn’t look right, when to dig deeper, and when you’ve found something worth acting on.\nYou will make mistakes. Your first attempts at data cleaning will probably miss important issues or generate many error messages. Your initial hypotheses will sometimes be completely wrong. You’ll create charts that confuse rather than clarify. That’s not just normal—it is a necessary part of the learning process. Each mistake teaches you something that no textbook can.\nYou’ll also experience those breakthrough moments when everything clicks—when a pattern emerges from seemingly random data, when you finally understand what’s driving a business problem, or when you present an insight that genuinely changes how someone thinks about their business. Those moments are what make all the technical learning worthwhile.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Core Skills of a \"Business Scientist\"</span>"
    ]
  },
  {
    "objectID": "foundations/reproducible.html",
    "href": "foundations/reproducible.html",
    "title": "3  Reproducible Workflows with R & Posit Cloud",
    "section": "",
    "text": "3.1 Getting Started with Posit Cloud\nIf science is about systematically building and organizing knowledge in the form of testable explanations and predictions, then business science applies this mindset to the world of data-driven business decision-making. A business scientist doesn’t just crunch numbers—they seek to generate insights that are reliable, testable, and useful to others.\nThis means we don’t just ask questions and analyze data. We build knowledge that others can follow, test, and build upon. It’s not enough to reach the right conclusion once, in a way that only you understand. True business science requires reproducible workflows—ways of working with data that are clear, transparent, and repeatable.\nThis chapter introduces the tools we’ll use to begin doing just that: R, a language designed for data analysis, and Posit Cloud, a cloud-based environment for running R code. Together, they’ll help us learn how to think and work like business scientists from day one.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Workflows with R & Posit Cloud</span>"
    ]
  },
  {
    "objectID": "foundations/reproducible.html#getting-started-with-posit-cloud",
    "href": "foundations/reproducible.html#getting-started-with-posit-cloud",
    "title": "3  Reproducible Workflows with R & Posit Cloud",
    "section": "",
    "text": "Why We Use Posit Cloud\nTo begin working with data, you’ll need a place to write code to analyse the data and then run the code. Normally, this would mean downloading and installing software—something that can be messy, especially if things go wrong or if you’re using a shared computer.\nInstead, we’ll use a tool called Posit Cloud.\nPosit Cloud is a website that gives you access to a full data science workspace right in your web browser. It’s like opening a notebook for coding—but online. There’s nothing to install, and your work is saved automatically. You can log in from any computer, at home or on campus, and pick up right where you left off.\nHere’s why we’ve chosen it for this course:\n\nNo setup stress You don’t need to download or install anything. You just log in and get started.\nEveryone sees the same thing Because we’re all working in the same environment, it’s easier to follow along in class—and much easier to get help when you’re stuck.\nYour work is always saved Posit Cloud automatically saves your work, so you won’t lose progress if your internet drops or your battery dies.\nIt helps us build good habits Later in the course, we’ll talk about how professional data scientists work in teams, share code, and keep track of their analyses. Posit Cloud helps us start working that way from day one.\n\nThink of Posit Cloud as your online lab bench for doing business analytics.\n\n\nLogging Into Posit Cloud\nTo start using R, you’ll first need to log in to Posit Cloud—our online workspace for coding and analysis.\nHere’s the steps you need to take to log into Posit Cloud\n\nClick on the Link in Email Invite. You will have received an email inviting you to join Posit Cloud. Click the link. Posit Cloud works best in Chrome or Firefox, but any modern browser should be fine.\n\n\n\n\nCourse Invite\n\n\n\nCreate a free account. A page will open in your browser asking you to sign up to Posit Cloud. Create a free account using your university email address.\n\n\n\n\nSign In\n\n\n\nVerify your Account & Access the Main Page . You may be asked verify your account via your email. Do this verification step if needed, and then you will be taken to the main page of Posit Cloud.\n\n\n\n\nMain Page\n\n\n\nAccess the Course Workspace. From the left panel, click on the subject’s shared workspace, ‘CMCE10002 – 2025 Semester 2.’\n\n\n\n\nFoundations of Business Analytics Workspace\n\n\n\n\nMaking Your Own Copy of a Project\nWhen you first enter the workspace, you’ll see shared projects. In the figure above you see two shared projects:\n\nLecture 2\nTutorial 2\n\nBefore you can work on a project, you need to make your own copy. That way, you can save your progress and safely make changes.\nHere’s the steps to follow to to make your own copy:\n\nFind the Project you want to work on. In this case, let’s work on “Lecture 2”\nClick the + icon on the right hand side to “Make a Copy”.\nClick “OK” on the pop out, and the project will be copied to your account.\n\nAfter you project has successfully been copied, it will automatically launch the project. When a project is launched an RStudio instance will be ready for you to start work. The Figure below shows you what this looks like:\n\n\n\nLaunching an RStudio Project\n\n\nWhen the project first launches you may see a “Temporary Copy” message flashing at the top of your window. Click the “Save Permanent Copy” button next to this message to make a permanent copy of the project.\n\n\n\n\n\n\nNoteMaking Your Own Copy Creates Your Personal Version\n\n\n\n\n\nWhen you copy a project, you’re creating your own personal version of the files.\nThis means:\n\nYou can write code, make mistakes, and experiment without affecting the original.\nYour work is saved automatically—even if you close your browser or your internet drops.\nYou can come back to it later, pick up where you left off, and keep building from there.\n\nThink of it like making a photocopy of the class worksheet—you get your own copy to write on, while the original stays clean for everyone else.\n\n\n\n\n\n\n\n\n\nNoteWhere to Find Your Work\n\n\n\n\n\nOnce you’ve made a copy of a project, it’s saved just for you inside the course workspace.\nTo come back to it later:\n\nGo to posit.cloud\nClick on the “Workspaces” tab in the top menu\nChoose our course workspace\nLook for the project with your name on it (e.g., Test Student in the figure below)\n\n\n\n\nYour Own Project Copy is Stored in Our Environment\n\n\nClick on the project’s title to re-launch the project, and you’ll be right back where you left off.\n\n\n\n\n\n\n\n\n\nTipFinding Projects by Week\n\n\n\n\n\nAs the course goes on, your workspace will fill up with more projects. To keep things tidy, use the navigation bar at the top of the course workspace.\nClick on tabs like “Week 2”, “Week 3”, etc., to see only the projects for that specific week. This helps you:\n\nFocus on the material you need right now\nAvoid getting overwhelmed by everything at once\nQuickly jump to the right week or task\n\nAssignments will also appear in their own tabs (like Assignment 1) so you can easily find and return to your work without digging through the full list.\nUse the tabs to stay organized—it’s how we’ve designed the workspace to grow with you.\n\n\n\n\n\n\n\n\n\nWarningMade a Mess? No Problem.\n\n\n\n\n\nIf your project gets completely tangled—broken code, missing files, or just too messy to fix—don’t stress.\nYou can always:\n\nDelete your copy, and then\nMake a fresh copy from the original project in the course workspace.\n\nIt’s a clean slate. No harm done.\nEveryone experiments. Everyone breaks things. That’s part of learning.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Workflows with R & Posit Cloud</span>"
    ]
  },
  {
    "objectID": "foundations/reproducible.html#what-is-rstudio",
    "href": "foundations/reproducible.html#what-is-rstudio",
    "title": "3  Reproducible Workflows with R & Posit Cloud",
    "section": "3.2 What is RStudio?",
    "text": "3.2 What is RStudio?\nWhen you open your project in Posit Cloud, you’ll land in a tool called RStudio.\nRStudio is an IDE—short for Integrated Development Environment. That just means it’s a workspace designed to help you write, run, and manage code, all in one place.\nWhen you first open RStudio, you’ll see three panels:\n\n\n\nRStudio\n\n\n\nConsole and Terminal (left side). This is where R runs your code and shows the output—results, messages, and errors. You might also see a Terminal tab—feel free to ignore that for now.\nEnvironment / History (upper right). This panel shows what R currently “knows.”\n\nEnvironment: a list of all the objects (like data sets or variables) in your current session\nHistory: a record of the commands you’ve run\nYou won’t need to manage these tabs actively, but they’re useful for checking what’s going on.\n\nFiles / Plots / Packages / Help / Viewer (lower right). This panel contains several tabs:\n\nFiles shows the files in your project\nPlots displays any charts you generate\nPackages helps you manage extra tools for R\nHelp is your built-in reference guide\nViewer lets you preview interactive content later on\n\n\nThat’s a lot of tabs and panes. We’ll introduce each tab as we need it — no need to memorize them all now.\n\nUnderstanding Paths (Even If You Never Type One)\nAs you progress this course, you’ll work with data files, save your results, and generate visualisations. All of that happens inside a project—a self-contained folder that holds everything you need.\nWhen you open a project in Posit Cloud, something helpful happens behind the scenes: Posit Cloud automatically sets your working directory to the root of your project. That’s a fancy way of saying: “When R looks for a file (like a dataset), it starts looking inside this project and its sub-folders.” A project, in this case, just means a folder that contains everything related to a piece of work—your data, code, results, and notes. Think of it like a digital binder that keeps all the pieces of an assignment in one place. When you open a project, R knows to look for files inside that folder—and nowhere else.\n\n\n\n\n\n\nCautionThe Headache You Never Knew You Avoided\n\n\n\n\n\nIf you’ve never written code before, you might not realise this—but finding and loading files is one of the biggest early frustrations in data analysis.\nIn most setups, you have to:\n\nFigure out where your files are on your computer,\nType out long, exact folder paths,\nAnd debug mysterious “file not found” errors when something goes wrong.\n\nPosit Cloud takes care of all of that for you. By opening a project, you’ve already told R where to look—and everything just works.\nYou won’t need to think about file paths much in this course. But you will be quietly doing things the right way.\n\n\n\n\n\n\n\n\n\nTipWorking with Files: Keep It Simple\n\n\n\n\n\n💡 As a rule: Keep all your data files inside your project folder. That way, R always knows where to find them.\nYou can refer to files by name (like \"sales_data.csv\"), or by their path within the project if they’re in a subfolder (e.g., \"data/sales_data.csv\").\nNo need to write long file paths like C:/Users/YourName/Desktop/... — those won’t work for others and break the idea of reproducibility.\nStick to filenames and project-relative paths, and everything will stay clean, portable, and easy to run on any computer.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Workflows with R & Posit Cloud</span>"
    ]
  },
  {
    "objectID": "foundations/reproducible.html#our-first-steps-with-r",
    "href": "foundations/reproducible.html#our-first-steps-with-r",
    "title": "3  Reproducible Workflows with R & Posit Cloud",
    "section": "3.3 Our First Steps with R",
    "text": "3.3 Our First Steps with R\nWe’ve learned where your projects live, how Posit Cloud helps manage your files, and how RStudio is set up. Now it’s time to try writing some actual R code.\nSo what is R?\nR is a programming language built for working with data. You can use it to calculate values, summarise information, create visuals, or build statistical models. And just like a calculator, it can respond immediately when you ask it to do something.\nLet’s start by using R like a calculator. In the Console, you can type code and press Enter to run it. You’ll see the result appear immediately below.\nTry typing this in the Console:\n\n2 + 2\n\nAnd after pressing Enter, you should see the following returned to screen:\n\n\n[1] 4\n\n\nThe [1] just means “this is the first item in the result.” You can ignore it for now.\nSo far, we’ve just typed code and seen the answer appear in the Console. That’s useful for quick calculation—but what if you want to use that result again later?\nIn R, you can assign a value to a variable using the arrow operator &lt;-. For example:\n\nresult &lt;- 2 + 2\n\nThis stores the result of 2 + 2 in a variable called result.\nNow if you type:\n\nresult\n\nyou’ll see:\n\n\n[1] 4\n\n\nWhy is this useful?\n\nYou don’t have to repeat calculations\nYou can build more complex code step by step\nIt makes your work easier to read and modify\n\n\n\n\n\n\n\nTipVariables as Labelled containers\n\n\n\nThink of a variable as a labelled container: you give something a name so you can refer to it later.\n\n\n\n\n\n\n\n\nTipShortcut Key: The Assignment Operator\n\n\n\nYou can type the assignment arrow &lt;- manually, but there’s also a handy shortcut:\n\nWindows: Alt + -\nMac: Option + -\nLinux: Alt + -.\n\nTry using it as you type to save time!\n\n\n\nAnother Example: The AFL Ladder\nLet’s imagine you’re looking at a simplified version of the AFL ladder in 2025. Suppose these four teams have the following points:\n\nCollingwood Magpies: 60\nAdelaide Crows: 60\nBrisbane Lions: 58\nWestern Bulldogs: 48\n\nYou can store these numbers in R as a vector—a collection of values:\n\nladder_points &lt;- c(60, 46, 58, 48)\n\nNow type:\n\nladder_points\n\nYou should see:\n\n\n[1] 60 46 58 48\n\n\n\n\n\n\n\n\nTipc() combines elements\n\n\n\nc() stands for combine. It creates a vector by combining values into one object.\n\n\n\n\n5 min\n\n\nCreate a new vector team_names that contains the four team names in points order. To type a string of text in R, wrap the text in parentheses like \"this\".\nPrint the result in the console.\n\n\n\nSolution\n\n\n\nteam_names &lt;- c(\"Collingwood Magpies\", \"Adelaide Crows\", \n                    \"Brisbane Lions\", \"Western Bulldogs\")\n\nteam_names\n\n[1] \"Collingwood Magpies\" \"Adelaide Crows\"      \"Brisbane Lions\"     \n[4] \"Western Bulldogs\"   \n\n\n\n\n\nFrom Vectors to Datasets\nSo far, we’ve worked with a single vector—just the ladder points. But most real-world data comes in tables, where each row represents an observation (like a team), and each column represents a variable (like their points).\nLet’s turn our two vectors—team names and ladder points—into a mini dataset.\nTo turn this into a proper table, we’ll use a tool from the tidyverse—a collection of R packages that make data analysis more consistent and readable. We’ll need to first load the tidyverse package and then use a function provided by the tidyverse called tibble which combines R objects into datasets.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nladder &lt;- \n    tibble(team = team_names, \n           points = ladder_points\n           )\n\nladder\n\n# A tibble: 4 × 2\n  team                points\n  &lt;chr&gt;                &lt;dbl&gt;\n1 Collingwood Magpies     60\n2 Adelaide Crows          46\n3 Brisbane Lions          58\n4 Western Bulldogs        48\n\n\nWhat is a tibble? That seems like a fair question to as. In short, A tibble is a modern, tidy version of a data frame. It looks like a spreadsheet but works in a more predictable way. You will use tibbles throughout this course.\n\n\n\n\n\n\nNoteWhat’s a Package?\n\n\n\n\n\nIn R, a package is a bundle of code, functions, and data designed to make your life easier. Instead of writing everything from scratch, you can load a package and get access to pre-built tools.\nTo use a package, you first install it (which we’ve done for you), and then load it with library(package_name). After that, you can start using the functions it provides.\n\n\n\n\n\n\n\n\n\nNoteWhat is the tidyverse?\n\n\n\n\n\nThe tidyverse is a collection of R packages designed to work together seamlessly for data heavy tasks. These packages share a common philosophy:\n\nUse clear, readable code\nHandle data in a consistent and predictable way\nEncourage best practices for reproducibility\n\nYou don’t need to learn all of the functions now — we’ll introduce them bit by bit as the course progresses.\n\n\n\n\n\nFrom Console to Script: Why Save Your Code?\nSo far, you’ve been writing code directly into the Console. That’s a great way to experiment and test things quickly. But there’s one big problem: When you close your session, everything you typed into the Console disappears.\nThat means:\n\nYour code isn’t saved\nYou can’t easily reuse or edit it later\nIt’s harder to share or get help\n\nThat’s why the next step is to start using R scripts — files where you can write and save your code. Scripts help you:\n\nKeep a clean, editable history of your work\nBreak your work into reusable steps\nMake your work reproducible for others (and your future self!)\n\nNow that we know why it’s better to save your code, let’s learn how to do it.\nHow to Open a New R Script\n\nIn RStudio, go to the File menu\nChoose New File &gt; R Script\nA blank window will open in the top-left panel — this is your script editor\n\nYou can now type your code here instead of the Console.\nLet’s copy some of the code we used earlier into the script:\n\nlibrary(tidyverse)\n\nteam_names &lt;- c(\"Collingwood Magpies\", \"Adelaide Crows\", \n                    \"Brisbane Lions\", \"Western Bulldogs\")\nladder_points &lt;- c(60, 46, 58, 48)\n\nladder &lt;- \n    tibble(team = team_names, \n           points = ladder_points\n           )\n\nladder\n\n# A tibble: 4 × 2\n  team                points\n  &lt;chr&gt;                &lt;dbl&gt;\n1 Collingwood Magpies     60\n2 Adelaide Crows          46\n3 Brisbane Lions          58\n4 Western Bulldogs        48\n\n\nTo run a line (or selection of lines), you can:\n\nHighlight it and click Run in the top right of the script panel, or\nUse the shortcut: Ctrl + Enter (Windows/Linux) or Cmd + Enter (Mac)\n\nThis sends the code to the Console and runs it as before — but now your code is saved.\nDon’t forget to save your script, with a name like ladder_analysis.R using File &gt; Save As. You can reopen it anytime to continue working.\n\n\nCommenting Your Code\nAs your code grows longer, it’s helpful to leave notes for yourself (or others) explaining what each part does.\nIn R, you can add a comment using the # symbol. Anything after the # on a line will be ignored by R when the code runs.\nFor example\n\nlibrary(tidyverse)\n# This creates a vector of team names\nteam_names &lt;- c(\"Collingwood Magpies\", \"Adelaide Crows\", \n                    \"Brisbane Lions\", \"Western Bulldogs\")\n\nladder_points &lt;- c(60, 46, 58, 48)\n\nladder &lt;- \n    tibble(team = team_names, \n           points = ladder_points\n           )\n\nladder\n\n# A tibble: 4 × 2\n  team                points\n  &lt;chr&gt;                &lt;dbl&gt;\n1 Collingwood Magpies     60\n2 Adelaide Crows          46\n3 Brisbane Lions          58\n4 Western Bulldogs        48\n\n\nComments are a simple but powerful habit—they help keep your work readable and easier to debug.\n\n\nGetting Help in R\nR has built-in documentation for almost every function. If you want to learn how a function works (or what arguments it takes), you can use the ? symbol followed by the function name. For example, suppose we want to find the minimum number of points in the dataset we have created. If we know the function to do this is min() but we dont know how to use it we can type the following into the Console:\n\n?min\n\nThis opens the help page for the min() function, which finds the minimum value in a vector. We can use it as follows:\n\nmin(ladder_points)\n\n[1] 46\n\n\non the vector ladder_points. If we needed to use it on the dataset ladder, then we need to access the column of points in the dataset. You can do that as follows:\n\nmin(ladder$points)\n\n[1] 46\n\n\nwhere the $ operator lets you extract a specific column from a dataset.\n\n\n\n\n\n\nNoteWhat if I don’t know the function name?\n\n\n\n\n\nSometimes you know what you want to do, but you don’t know the name of the function.\nHere are a few ways to figure it out:\n\nUse the Help panel (lower right in RStudio) and type keywords into the search bar.\nTry the ?? operator to search function names and help pages in the Console. For example:\n\n\n??minimum\n\nThis shows help files related to the word “minimum.”\n\nUse Google or search “R how to [do the thing]”\n\nOver time, you’ll learn common function names. But it’s totally normal to look them up — everyone does, even the experts.\n\n\n\n\n\n10 min\n\n\nAdd comments to the code in the script to explain what each line of code is doing\nFind the maximum number of points in the ladder_points vector. Repeat for the ladder dataset.\nFind the mean number of points in the ladder_points vector. Repeat for the ladder dataset.\nFind the minimum of the team column inside the ladder_points data.\nCan you explain the result from Q4?",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Workflows with R & Posit Cloud</span>"
    ]
  },
  {
    "objectID": "foundations/reproducible.html#from-script-to-story-writing-with-quarto",
    "href": "foundations/reproducible.html#from-script-to-story-writing-with-quarto",
    "title": "3  Reproducible Workflows with R & Posit Cloud",
    "section": "3.4 From Script to Story: Writing with Quarto",
    "text": "3.4 From Script to Story: Writing with Quarto\nYou’ve seen how scripts help you save and structure your code. But what if you want to share your analysis—complete with notes, explanations, and results—all in one place?\nThat’s where Quarto comes in.\nQuarto lets you combine code, output, and written explanation in a single document. It’s like a notebook that tells the full story—not just what you did, but why it matters.\nA Quarto document is a special kind of file (with a .qmd extension) that:\n\nLets you write regular text using Markdown\nLets you include code chunks that run in R\nShows both the code and the results/output\n\nWe’ll use Quarto documents to write up your work, show your results, and explain your reasoning.\nWhy would we want to use Quarto. There’s at least 3 good reasons why we might prefer Quarto to scripts:\n\nDocumentation + Analysis: Keep your code and your thinking in one place.\nReproducibility: Anyone can re-run your analysis and get the same results.\nCommunication: Great for assignments, reports, and portfolios.\n\n\nCreating Your First Quarto Document\nCreating a Quarto document in RStudio is simple. Here’s how to get started:\n\nIn the RStudio menu, go to File &gt; New File &gt; Quarto Document…\n\nA dialog box will appear. You can:\n\nGive your file a Title (e.g. “AFL Ladder Summary”)\nLeave the Author field blank or enter your name\nLeave Format as HTML for now\nDeselect the “Use visual markdown editor”\nClick Create\n\nYou’ll see a new file appear in the top-left pane with some example text included.\nThis is your Quarto source file, and it will have a .qmd extension (short for Quarto Markdown).\nThis is where you’ll write your analysis: both the R code and the explanation that goes with it.\nWe’ll now walk through the key elements of a Quarto document.\n\n\nTop Matter\nAt the very top of the Quarto document, we can see a small block that looks like this:\n\n---\ntitle: \"AFL Ladder Summary\"\nformat: html\n---\n\nThis section is called the YAML front matter (or just “top matter”). It tells Quarto how to render your document — like the title, output format, and other settings.\nIt always starts and ends with three dashes (---), and everything inside follows a key: value format.\nWe can add some key-value pairs to the top matter. Let’s add the date as follows:\n\n---\ntitle: \"AFL Ladder Summary\"\ndate: today\nformat: html\n---\n\nNow that we have some content inside our Quarto document, we want to view the output. Click the Render button at the top of the script panel. Quarto will run the code, insert the results into your document, and generate a clean HTML page. In Posit Cloud, the rendered HTML will open in a new browser tab. You can scroll through it to check that everything looks as expected—text, code, and results all in one place. If something doesn’t look right, just go back to your .qmd file, make changes, and click Render again. It’s like refreshing your work after an update.\n\n\n5 min\n\nSuppose we wanted to change the date format to read “August, 2025”. We can change this using the date-format key and using the value MMMM, YYYY.\n\nAdd the date-format key value pair to the top matter and render the edited document.\nTry a different date format by choosing one from here and inserting it into the document.\n\n\n\n\nEssential Syntax for Quarto\nWhen writing in Quarto, you’ll use a format known as Markdown to format your text. Here are the essentials you’ll need throughout this course:\n\nEmphasis in text\nUse * or _ for emphasis:\n\nBold:\n**This is bold** → This is bold\nItalics:\n*This is italic* → This is italic\n\nYou can even combine them:\n***Bold and italic*** → Bold and italic\n\n\nHeaders\nHeaders help structure your document. Use # symbols:\n# This is a level 1 heading\n## This is a level 2 heading\n### This is a level 3 heading\nNote that a level 1 heading is a title in Quarto. So you should only use level 2 headings or below.\n\n\nOrdered and Unordered Lists\n\nUnordered List (with - or *)\n- Apples\n- Bananas\n- Oranges\nProduces:\n\nApples\n\nBananas\n\nOranges\n\n\n\nOrdered List (with numbers)\n1. First item  \n2. Second item  \n3. Third item\nLeads to:\n\nFirst item\n\nSecond item\n\nThird item\n\n\n\nNested (Sublists)\n- Fruits\n  - Apples\n  - Oranges\n- Vegetables\n  - Carrots\n  - Spinach\nYields:\n\nFruits\n\nApples\n\nOranges\n\n\nVegetables\n\nCarrots\n\nSpinach\n\n\nUse two or four spaces to indent sub-items.\n\n\n\nLinks (URLs)\nTo include a link, use square brackets for the text and parentheses for the URL:\n[Visit the AFL website](https://www.afl.com.au)\nProduces:\nVisit the AFL website\n\n\nParagraphs\nJust leave a blank line between paragraphs:\nThis is the first paragraph.\n\nThis is the second paragraph.\nQuarto will automatically format this with proper spacing in your final document:\nThis is the first paragraph.\nThis is the second paragraph.\n\n\n\nWorking with R Code Chunks\nIn a Quarto document, code lives inside chunks. These are special blocks that run R code and display the results right alongside your writing.\nHere’s what a basic chunk looks like:\n```{r}\nmean(c(1, 2, 3, 4, 5))\n```\nThis chunk will:\n\nRun the code\nShow the code\nShow the result (which is 3)\n\ni.e. we get the following:\n\nmean(c(1, 2, 3, 4, 5))\n\n[1] 3\n\n\nBy default, both the code and the result will appear in the final document.\n\nCustomising Code Chunks\nSometimes, you don’t want everything to be shown. Maybe you want to hide the code, or skip running it entirely. That’s where chunk options come in.\nChunk options start with #| and go inside the chunk, like this:\n```{r}\n#| eval: false\n\nmean(c(1, 2, 3, 4, 5))\n```\nWhen we include that chunk in our document:\n\nmean(c(1, 2, 3, 4, 5))\n\nNotice how now the code was shown, but there was no output printed afterwards? That’s because the #| eval: false instructs Quarto not to run the code inside this chunk.\n\n\necho: Show or hide the code\nWe can do the opposite to displaying code and no output. Using:\n#| echo: false\nleads to:\n\nThe code will run\n\nThe result will appear\n\nBut the code won’t be shown\n\nThis is useful when you want the results to appear, but don’t want to show the underlying code.\n\n\nwarning: Suppress warning messages\nWhen we add:\n#| warning: false\n\nThe code will run normally\n\nAny warnings will be hidden in the output\n\nThis keeps your output clean when R gives non-critical warnings that you want to suppress. We haven’t come across any of these yet, but when we do we will come back to this option.\nWe might stumble across other chunk options as well as we progress through the class:\n\nmessage: false — hides startup or package-loading messages\n\ninclude: false — runs the code, but hides both the code and the output (great for setup chunks)\n\nYou don’t need to memorize all of these now — we’ll remind you when they come up.\n\n\n\nWriting Equations in Quarto\nIn Quarto, you can write mathematical equations using the same syntax used in LaTeX. There are two main ways to do this.\n\nInline Math\nUse single dollar signs $...$ to include math inside a sentence:\nThe average is given by $\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i$.\nThis renders as:\nThe average is given by \\(\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\\).\n\n\nDisplay Math\nUse double dollar signs $$...$$ to write standalone equations on their own line:\n$$\n\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n$$\nThis will render as:\n\\[\n\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n\\]\nMath is kept to a minimum in this book. As a result you won’t need to learn the too much of these details. Think of this as a reference in case at some point in your study you want to write equation in a Quarto document.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Workflows with R & Posit Cloud</span>"
    ]
  },
  {
    "objectID": "foundations/reproducible.html#using-r-in-practice",
    "href": "foundations/reproducible.html#using-r-in-practice",
    "title": "3  Reproducible Workflows with R & Posit Cloud",
    "section": "3.5 Using R in Practice",
    "text": "3.5 Using R in Practice\n\nWhat to Do When Your Code Doesn’t Work\nLet’s be real: at some point, your code will break.\nIt might throw an error. It might do something weird. Or it might not run at all.\nThis is completely normal—it happens to everyone, even professionals. The authors of this book spend many hours per day trying to fix their own broken code. Learning to fix things when they go wrong is a big part of learning to code.\nHere are some strategies that can help when you hit a roadblock:\n\n1. Read the error message\nWe know they’re not always friendly. But sometimes, error messages give useful clues. Try to read them carefully and spot:\n\nWhat function is causing the problem?\nIs it missing something? (like a dataset or a column name)\nDoes it mention a type of object you didn’t expect?\n\n\n\n2. Google the error (add “in R” or “tidyverse”)\nYou’re not the first person to hit this problem.\nTry copying part of the error message into Google. Add words like “in R” or “tidyverse” to get better results.\nLook for answers on Stack Overflow, RStudio Community, or helpful blog posts.\n\n\n3. Use the help system\nIf you’re not sure how a function works, try this:\n?some_function\nAs we discussed above that opens the help file for the function. It shows:\n\nWhat the function does\nWhat arguments it needs\nExamples of how to use it\n\nCheck that you’ve spelled the function and its arguments correctly. A common mistake is using quotes when you shouldn’t (or vice versa).\n\n\n4. Comment out chunks of code to isolate the issue\nIf your document won’t render, try removing or commenting out code until it works again. Then slowly add pieces back one at a time.\nThis helps you figure out where the problem is happening.\n\n\n5. Check the type of your data\nSometimes things break because R is treating your data differently than you expect.\nTry this:\ntype(data_set$column)\nThis tells you if it’s a number, character, factor, etc. Some functions need one type and won’t work with another.\n\n\n6. Restart your R session\nSometimes R just gets confused. You can give it a clean start by going to:\nSession &gt; Restart R and Clear Output\nThis reloads your project and starts fresh.\n\n\n7. Restart your computer\nYes, seriously. It sometimes works.\n\n\n8. Search for what you’re trying to do—not just the error\nInstead of Googling the exact error, try searching your goal, like:\n\n’save PDF of ggplot in R”\n’how to rename a column with dplyr”\n\nThis often leads to clearer answers or helpful examples.\n\n\n\nMentality\nYou don’t need to be a ’tech person” or have all the answers to be a programmer. If you write code—even one line—you’re already doing it. There’s no special badge you need.\nOver time, great coders tend to share a few habits—not because they’re geniuses, but because they’ve learned what works. And so can you.\n\n1. Be Focused\nSaying ’I want to learn R” is a nice thought, but it’s kind of like saying ’I want to learn everything.” There’s no finish line. It’s easy to get lost or feel stuck.\nA better approach? Pick something small and specific, like:\n\n’I want to make a histogram of AFL ladder points using ggplot2.”\n\nNow you have something to aim for. It’s focused. It’s achievable. You can ask for help if you need it. And when you’re done—you’ve learned something real.\n\n\n2. Be Curious\nIf you’re not sure what a function does, or how something works — try it and see.\nWhat happens if you give ggplot() a vector instead of a data frame? Try it.\nWhat does summary() do on a tibble? Try it.\nWill this code break your computer? (Nope. Not unless you’re really trying.)\nCoding is about experimenting. You’ll learn more by doing than by reading alone.\n\n\n3. Be Pragmatic\nCuriosity is great—but don’t change everything at once. Small steps are key.\nLet’s say you want to use a new package like data.table instead of dplyr for a regression. Rather than rewriting your whole pipeline, try swapping out just one function. Get that working. Then move to the next step.\nIt’s not about being fancy—it’s about making progress.\n\n\n4. Be Tenacious (But Know When to Pause)\nCode will break. Unexpected things will happen. This is normal.\nKeep going. Try a different angle. Ask someone. Take a walk. Come back later.\nBut also: know when to let go. If you’ve spent hours chasing one bug, and it’s stopping your progress, it’s okay to set it aside and try a new approach. Asking for help doesn’t mean giving up—it means you’re working smart.\n\n\n5. Be Planned\nBefore you dive into code, pause and sketch out what you want to do.\nWant to make a graph? Think:\n\nWhere is the data coming from?\nWhat packages will I use?\nWhat will I do if I can’t find the data I need?\n\nEven five minutes of planning can save you hours of frustration.\n\n\n6. Done is Better Than Perfect\nYour first code doesn’t need to be elegant. Or tidy. Or clever.\nIt just needs to work.\nYou can always come back and clean it up later. But the most important thing is to finish something, no matter how small. Ugly, working code is better than perfect code that never gets used.\nYou’re already a coder. Keep going.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Workflows with R & Posit Cloud</span>"
    ]
  },
  {
    "objectID": "foundations/reproducible.html#concluding-remarks",
    "href": "foundations/reproducible.html#concluding-remarks",
    "title": "3  Reproducible Workflows with R & Posit Cloud",
    "section": "3.6 Concluding Remarks",
    "text": "3.6 Concluding Remarks\nIn this chapter we have considered much and it is normal to be overwhelmed. Come back to the Quarto section as needed. Let’s now get our hands dirty with some data!",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Workflows with R & Posit Cloud</span>"
    ]
  },
  {
    "objectID": "insights/data_visualisation.html",
    "href": "insights/data_visualisation.html",
    "title": "4  Data Visualisation for Business Intelligence",
    "section": "",
    "text": "4.1 Why We Begin with Data Visualisation\nData analysis starts long before you write a model, calculate a statistic or manipulate data. It starts with a question and a dataset — and the need to make sense of what you’re looking at. That’s why the first thing we cover is data visualisation.\nVisualisation is how we as analysts get our bearings. Before we manipulate, summarise, or test anything, we need to see the shape of the data — how it varies, where it might surprises us, and what patterns be waiting underneath the surface. A simple graph can often tell us more in seconds than a table of numbers could in minutes.\nVisualisation also helps us engage with data in ways that raw numbers cannot. Our brains process visual information far faster than text—some estimates say up to 60,000× faster—which is why well-designed graphs make it easier to spot patterns, outliers, and anomalies that might otherwise go unnoticed. Visualisations can reveal hidden relationships that shape customer behavior, pricing outcomes, or operational performance.\nAnd just as importantly, visualisation is often one of the main ways in which we communicate our work. It bridges the gap between technical analysis and business decision-making. Graphs are essential to data storytelling—they help us explain both the broad patterns and the important details. A good graph doesn’t just display data; it encodes meaning in a way that others can decode. The success of a visual depends not only on what it shows, but on how well it helps an audience understand the underlying story while simultaneously preserving its context.\nThroughout this course, we’ll often begin with a business challenge, explore the data to see what patterns emerge, and then work toward communicating our findings in a way that matters—whether through a plot, a written summary, or a recommendation to decision-makers. This chapter sets the tone for that structure. We start with a real business problem. We explore it through a series of visualisations. And we practice turning what we see into insights someone else could use.",
    "crumbs": [
      "Getting Our First Insights from Data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualisation for Business Intelligence</span>"
    ]
  },
  {
    "objectID": "insights/data_visualisation.html#why-we-begin-with-data-visualisation",
    "href": "insights/data_visualisation.html#why-we-begin-with-data-visualisation",
    "title": "4  Data Visualisation for Business Intelligence",
    "section": "",
    "text": "“The greatest value of a picture is when it forces us to notice what we never expected to see.” — John Tukey",
    "crumbs": [
      "Getting Our First Insights from Data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualisation for Business Intelligence</span>"
    ]
  },
  {
    "objectID": "insights/data_visualisation.html#the-business-challenge",
    "href": "insights/data_visualisation.html#the-business-challenge",
    "title": "4  Data Visualisation for Business Intelligence",
    "section": "4.2 The Business Challenge",
    "text": "4.2 The Business Challenge\n\nThe Topic: Understanding Price Sensitivity in Beer Sales\nWe begin our analytics journey with a fundamental business question: How do price changes affect sales? Pricing is one of the most powerful levers in retail. Some consulting firms have made huge claims about price sensitivity in retail contexts, suggesting that a 1% increase in price can lift profits by up to 10–11%, due to the direct impact of price on profit margins. However, more recent academic research suggests that the relationship is often less dramatic and highly context-dependent, especially once you account for consumer responses and competitive dynamics. In reality, the profit impact of a price change depends on how sensitive demand is. Getting a first handle on that price sensitivity, and stylized facts around prices and quantities sold is exactly the issue we want to explore.\nTo explore this, we will work with data from the beer category, a $16.8 billion market in Australia. Understanding how price affects demand is critical for retailers, analysts, and category managers making everyday pricing and promotion decisions.\nIn this chapter, we ask:\n\nHow do price changes affect sales across different beer brands?\n\nAnd more specifically:\n\nDo imported and domestic beers respond differently to price changes?\n\n\n\nThe Data: Retail Scanner Data from Dominick’s Finer Foods\nTo answer this, we’ll work with real-world retail scanner data. While the data that we will use comes from a U.S. supermarket chain, it closely mirrors the kind of sales and pricing information that analysts at Woolworths, Coles, or other retailers use daily. The goal is not just to practice making plots—but to begin learning how to use data to guide commercial decisions.\nTo explore our pricing question, we’ll use retail scanner data from Dominick’s Finer Foods, a now-defunct U.S. supermarket chain that made its data available to researchers. The dataset captures weekly sales transactions across multiple stores and includes detailed information for each observation:\n\nThe beer brand,\nPrice per unit,\nNumber of units sold,\nWhether the beer is imported or domestic,\nThe store identifier,\nThe week of sale, and\nWhether the product was under promotion that week\n\n\n\nWhere we’re headed\nOur goal is to show how just a few lines of R code can transform raw numbers into business intelligence. We’ll start with a simple data frame—just rows and columns of weekly beer sales—and build a series of visualisations that reveal patterns you could actually use to inform pricing and category strategy.\nFrom this:\n\n\n# A tibble: 10 × 6\n   store  week brand       qty price imported\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   \n 1    86    91 Budweiser    23  3.49 domestic\n 2    86    91 Corona       13  5.79 imported\n 3    86    91 Lowenbrau    13  3.99 imported\n 4    86    91 Miller       15  3.69 domestic\n 5    86    92 Budweiser    46  3.49 domestic\n 6    86    92 Corona       24  5.79 imported\n 7    86    92 Lowenbrau    21  3.99 imported\n 8    86    92 Miller      117  2.99 domestic\n 9    86    93 Budweiser    47  3.49 domestic\n10    86    93 Corona       23  4.39 imported\n\n\nTo this: (REMARK Still need to complete the pretty-ing of plots)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nEach plot we build will add a layer of understanding: differences in price across product types, variation in sales volume, and finally a visual model of price sensitivity. By the end, we’ll have a dashboard that tells a real story about consumer behavior—and gets us closer to answering our business question.",
    "crumbs": [
      "Getting Our First Insights from Data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualisation for Business Intelligence</span>"
    ]
  },
  {
    "objectID": "insights/data_visualisation.html#loading-and-understanding-the-data",
    "href": "insights/data_visualisation.html#loading-and-understanding-the-data",
    "title": "4  Data Visualisation for Business Intelligence",
    "section": "4.3 Loading and Understanding the Data",
    "text": "4.3 Loading and Understanding the Data\nNow that we’ve seen what’s possible with visualisation, let’s load the data and start exploring it for ourselves. We’ll work with a real-world dataset of weekly beer sales and use a handful of R packages to help us plot, organise, and format our results. To get started, we’ll load the following R packages:\n\nlibrary(tidyverse)  # for plotting, includes ggplot\nlibrary(patchwork)  # for combining multiple plots into subfigures\nlibrary(scales)     # for formatting axis scales\nlibrary(ggokabeito) # color blind friendly color palette -- this course's default\n\n\n\n\n\n\n\nCautionReminder: What are R packages?\n\n\n\nIn the last chapter, we introduced R packages as collections of functions, data, and documentation that extend R’s capabilities. Think of them as plug-ins that add new tools to your RStudio toolbox.\nToday, we’re using a few common packages that help us visualise data, combine plots, and format labels. You only need to load them once per session using library().\n\n\nAfter we load our packages, the first thing we do is read in our dataset. In R, this is done using a function. Think of a function is just a named action — something R knows how to do. Most functions take arguments — extra pieces of information that tell the function how to behave.\nWe’ll use the read_csv() function to load our beer sales data from a CSV file:\n\nbeer &lt;- read_csv(\"data/beer.csv\")\n\nThis loads a dataset called beer from a file that contains weekly price and sales data for four brands across multiple stores. You will see that this beer object now appears in the Environment tab on the upper-right in RStudio. This means we now have a dataset stored in R that we can explore, and turn into visual insights.\n\n\n\n\n\n\nTip💡 First look: Reading in data\n\n\n\nThis line has three key parts:\n\nread_csv( ) is the function that tells R to read a CSV file\nThe file path (in quotes) is the argument, telling it which file to read\nbeer &lt;- is the assignment, which stores the result into a new object called beer\n\nWhen we run this line, R fetches the file and stores it as a data frame we can work with in our R environment.\n\n\n\nThe Beer Sales Dataframe\nLet’s take a first look at our dataset. In R, a dataset like this is stored in a structure called a data frame — a rectangular table where each row is an observation (in this case, a product-week-store combination), and each column is a variable (like brand, price, or quantity sold).\nWe’ve saved this data frame as an object called beer. To inspect it, we can simply type the name:\n\nbeer\n\n# A tibble: 4,033 × 17\n   store  week brand      upc   qty price sales_indicator city  price_tier  zone\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt;           &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1    86    91 Budwei… 1.82e9    23  3.49 FALSE           Chic… medium         2\n 2    86    91 Corona  8.07e9    13  5.79 FALSE           Chic… medium         2\n 3    86    91 Lowenb… 3.41e9    13  3.99 FALSE           Chic… medium         2\n 4    86    91 Miller  3.41e9    15  3.69 FALSE           Chic… medium         2\n 5    86    92 Budwei… 1.82e9    46  3.49 FALSE           Chic… medium         2\n 6    86    92 Corona  8.07e9    24  5.79 FALSE           Chic… medium         2\n 7    86    92 Lowenb… 3.41e9    21  3.99 FALSE           Chic… medium         2\n 8    86    92 Miller  3.41e9   117  2.99 FALSE           Chic… medium         2\n 9    86    93 Budwei… 1.82e9    47  3.49 FALSE           Chic… medium         2\n10    86    93 Corona  8.07e9    23  4.39 FALSE           Chic… medium         2\n# ℹ 4,023 more rows\n# ℹ 7 more variables: zip &lt;dbl&gt;, address &lt;chr&gt;, latitude &lt;dbl&gt;,\n#   longtitude &lt;dbl&gt;, start_of_week &lt;date&gt;, is_holiday_week &lt;lgl&gt;,\n#   imported &lt;chr&gt;\n\n\nBut that can be overwhelming if there are lots of rows and columns. Instead, we’ll use the glimpse() function. It gives us a quick overview of the structure: variable names, types, and a few sample values from each.\n\nglimpse(beer)\n\nRows: 4,033\nColumns: 17\n$ store           &lt;dbl&gt; 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86…\n$ week            &lt;dbl&gt; 91, 91, 91, 91, 92, 92, 92, 92, 93, 93, 93, 93, 94, 94…\n$ brand           &lt;chr&gt; \"Budweiser\", \"Corona\", \"Lowenbrau\", \"Miller\", \"Budweis…\n$ upc             &lt;dbl&gt; 1820000016, 8066095605, 3410021505, 3410000554, 182000…\n$ qty             &lt;dbl&gt; 23, 13, 13, 15, 46, 24, 21, 117, 47, 23, 34, 118, 46, …\n$ price           &lt;dbl&gt; 3.49, 5.79, 3.99, 3.69, 3.49, 5.79, 3.99, 2.99, 3.49, …\n$ sales_indicator &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE…\n$ city            &lt;chr&gt; \"Chicago\", \"Chicago\", \"Chicago\", \"Chicago\", \"Chicago\",…\n$ price_tier      &lt;chr&gt; \"medium\", \"medium\", \"medium\", \"medium\", \"medium\", \"med…\n$ zone            &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ zip             &lt;dbl&gt; 60618, 60618, 60618, 60618, 60618, 60618, 60618, 60618…\n$ address         &lt;chr&gt; \"3350 Western Ave\", \"3350 Western Ave\", \"3350 Western …\n$ latitude        &lt;dbl&gt; 41.94235, 41.94235, 41.94235, 41.94235, 41.94235, 41.9…\n$ longtitude      &lt;dbl&gt; -87.68999, -87.68999, -87.68999, -87.68999, -87.68999,…\n$ start_of_week   &lt;date&gt; 1991-06-06, 1991-06-06, 1991-06-06, 1991-06-06, 1991-…\n$ is_holiday_week &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE…\n$ imported        &lt;chr&gt; \"domestic\", \"imported\", \"imported\", \"domestic\", \"domes…\n\n\nLet’s break this down.\nThis dataset has 4,033 rows and 17 columns. Each row represents the sales of one beer brand at one store during one week. The columns tell us what was sold, where and when it was sold, and under what conditions.\nHere are a few important variables to notice:\n\nbrand: the name of the beer (e.g., Budweiser, Corona)\nqty: how many units were sold that week\nprice: the price per unit\nimported: whether the beer is domestic or imported\nweek and start_of_week: indicators of when the sale occurred\nstore, city, zip, and zone: store location identifiers\nsales_indicator and is_holiday_week: flags for whether the product was on promotion or sold during a holiday week\n\nWe’ll mostly focus on variables like price, qty, and imported in this chapter, but it’s good to know that the data contains much more we could explore later.\n\n\n5 min\n\n\nIn your own words, describe what a single row in the beer dataset represents.\nWhat are the key variables we’ll likely focus on when exploring how price affects sales? Why those?\nIdentify two other columns in the dataset that might be useful for understanding context (e.g. when or where the sale happened).\nAs you look at the glimpse() output, notice the data types in each column (e.g. &lt;chr&gt;, &lt;dbl&gt;, &lt;lgl&gt;, &lt;date&gt;). Why might it matter whether a column is stored as a number, category, or date?\n\n\n\nSolution\n\n\nWhat does a row represent?\n\nEach row shows the sales of a specific beer brand at a specific store during a specific week. It tells us how many units were sold, at what price, and includes information about the product and store.\n\nKey variables for pricing analysis:\n\nprice: how much the product was sold for that week\nqty: how many units were sold\nimported: whether the beer is domestic or imported\nbrand: which brand the sales belong to\n\nThese variables let us explore patterns in price, demand, and brand differences.\nContextual columns:\n\nweek or start_of_week: tells us when the sale occurred\nstore, zip, or city: tells us where the sale happened\n\nThese can help us account for seasonal effects or location-based differences in consumer behavior.\nWhy data types matter:\n\nNumeric variables like price and qty can be plotted, summarised, or used in calculations\nCategorical variables like brand or imported can be used to group or color plots\nLogical (TRUE/FALSE) variables like sales_indicator are useful for filtering\nDate variables help with time-based analysis\n\nUnderstanding data types helps us choose the right visualisation and transformation later.",
    "crumbs": [
      "Getting Our First Insights from Data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualisation for Business Intelligence</span>"
    ]
  },
  {
    "objectID": "insights/data_visualisation.html#visualizing-the-data",
    "href": "insights/data_visualisation.html#visualizing-the-data",
    "title": "4  Data Visualisation for Business Intelligence",
    "section": "4.4 Visualizing the Data",
    "text": "4.4 Visualizing the Data\nIn this book, we’ll build most of our plots using the ggplot2 package. It follows what is know as the grammar of graphics approach, which means you build a plot in layers:\n\nStart with the data and aesthetic mappings (what goes on the x- and y-axis, or what defines colour)\nAdd geoms (the shapes in the plot — bars, points, lines, etc.)\nThen layer in labels, scales, and themes to clarify and style the plot\n\nThink of it like building a chart step by step, rather than picking a template.\nWe’ll also need a few key data visualisation principles to guide how we build our plots:\n\nChoose the right visual for the question you’re asking\nDon’t overload the viewer—clarity beats complexity\nColour and scale should help highlight structure, not distract from it\n\nLet’s dive into creating the plots!\n\nInvestigating Price Distributions\nWe begin by exploring how beer prices vary across products. Specifically, we’ll compare the prices of imported and domestic beers to see if there are noticeable differences — and try and infer, where possible, what those differences might tell us about the market.\nWe start by telling R what data we’re using. This line alone won’t produce a plot, but it sets the stage—like opening a blank canvas and choosing your dataset.\n\nggplot(beer)\n\n\n\n\n\n\n\n\nNext, we tell ggplot what kind of plot to draw. We’ll use a geom_boxplot(), which creates a box plot—a compact way to show how a variable is distributed.\nBox plots are useful when you want to:\n\nCompare medians and variability across groups\nSpot outliers or unusually high/low values\nSee how spread out a variable is\n\nIn our case, we want to understand the distribution of beer prices, and eventually compare prices between imported and domestic products. A box plot helps us do both quickly and clearly.\n\nggplot(beer) +\n    geom_boxplot()\n\n\n\n\n\n\n\nTipWhat is a box plot?\n\n\n\nA box plot shows the spread of a numeric variable:\n\nThe box captures the middle 50% of values (from the 1st to 3rd quartile)\nThe line inside the box shows the median\nThe “whiskers” extend to typical lower and upper values\nAny points outside the whiskers are considered outliers\n\nThis lets you compare distributions without showing every single data point.\n\n\nNow we need to map a variable to an aesthetic—a visual feature like position, colour, or size. Here, we’re interested in price, so we map price to the y-axis because we want to see the distribution of prices displayed vertically.\n\nggplot(beer) +\n    geom_boxplot(aes(y = price))\n\n\n\n\n\n\n\n\nThis gives us a boxplot showing the overall distribution of beer prices, but it doesn’t yet separate imported from domestic products. To do this, we need to add an additional aesthetic. Because we have prices displayed vertically, we will use the horizontal dimension to show different box plots for imported and domestic beers, giving us a side-by-side comparison. Thus, we add imported as the x aesthetic:\n\nggplot(beer) +\n    geom_boxplot(aes(y = price, x = imported))\n\n\n\n\n\n\n\n\nThis is our first visual insight. On average, imported beers tend to have higher prices than domestic ones. The imported beers also have more variation in pricing, which we can see from the relatively longer ‘box’ part of the plot. You might notice that:\n\nFor imported beers, the median (the horizontal line inside the box) is closer to the bottom of the box\nFor domestic beers, the median is roughly centered within the box\n\nThis suggests that imported beer prices are more right-skewed—that is, most imported beers are priced closer to the lower end of their group, but there are a few higher-priced rows of data pulling the distribution upward. This could indicate a wider range of premium pricing among imported brands. In contrast, domestic beer prices are more symmetrically distributed, with prices more evenly spread above and below the median. This might reflect tighter price clustering and less variation in pricing strategy among domestic brands.\nThese insights makes intuitive sense: imported products often face higher shipping and distribution costs, and may be positioned as premium options in the market. Still, this is just the start—we’re observing, not yet explaining.\nWe’ll return to this idea later when we explore the price-quantity relationship.\n\n\n5 min\n\n\nCreate a box plot comparing the prices across different beer brands.\nSummarize the main insights from the graph in 3 sentences.\n\n\n\nSolution\n\n\n\nggplot(beer) +\n    geom_boxplot(aes(y = price, x = brand))\n\n\n\n\n\n\n\n\n\n\nStoring Our Plot for Later\nSo far, we’ve been building plots and printing them immediately either into our Quarto document or the Plot pane in RStudio. But just like with data frames or numbers, you can also store a plot to an object in R. This is helpful when you want to:\n\nReuse the plot later in your script\nAdd more layers or styling in a separate step\nExport the plot to a file (e.g. PNG or PDF)\n\nHere, we assign the plot to an object called price_box:\n\nprice_box &lt;- \n    ggplot(beer) +\n    geom_boxplot(aes(y = price, x = imported))\n\nThis doesn’t display the plot yet—it just stores it. To view it, you can type the object name and press Enter:\n\nprice_box\n\n\n\n\n\n\n\n\nIf we’d like to save the plot as an image file you can use outside of R (e.g. in a report or slide), use ggsave():\n\nggsave(\"price_box.png\", \n       plot = price_box, \n       width = 5, height = 4\n       )\n\n\n\n\nExploring Quantity Sold\nNext, let’s look at sales volume — how many units of beer were sold each week. This can help us understand the distribution of demand across our observations and spot patterns like common purchase sizes or unusually large orders.\nA histogram shows how often different values occur by grouping them into bins. It’s great for seeing the shape of a distribution — for example, whether most sales are small, or if there are frequent large sales.\nHere’s a basic histogram of the quantity sold in a store-brand-week (qty):\n\nggplot(beer) +\n    geom_histogram(aes(x = qty))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nBy default, ggplot chooses a bin size for you, but adjusting the bin width can make the story clearer. A smaller bin width shows more detail, while a larger bin width smooths over small variations. Let’s experiment with the binwidth argument to see how changing it alters the level of detail in our histogram. In the code, we set binwidth = 5. This means each bar in the histogram represents a range of 5 units sold. For example, one bar might count all weeks where 1–5 units were sold, the next bar shows 6–10 units, and so on. Choosing binwidth = 5 groups the data into wider ranges, smoothing out small fluctuations and making overall patterns easier to see.\n\nggplot(beer) +\n    geom_histogram(aes(x = qty), binwidth = 5)\n\n\n\n\n\n\n\n\nAlternatively, you can set the number of bins instead of the width. Both methods control how finely the data is grouped. Let’s experiment with the bins argument to see how changing it alters the level of detail in our histogram. In the code, we set bins = 40. This means the range of values for quantity sold is divided into 40 equally spaced intervals (bins), and each bar shows how many observations fall into each interval. Choosing a larger number of bins gives a more detailed view of the distribution, while a smaller number of bins smooths out fine-grained variation and shows broader trends.\n\nggplot(beer) +\n    geom_histogram(aes(x = qty), bins = 40)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip💡 Recognising Right-Skewed Data\n\n\n\nMany real-world business datasets — including retail sales — are right-skewed.\nThis means most observations are clustered at the lower end of the scale, with a long tail of higher values.\nIn our beer sales data, most weekly quantities sold are relatively small, but there are occasional large values when sales spike.\nRight-skewed data can make it harder to see detail in the bulk of the distribution. Later, we’ll look at how log transformations can make these patterns easier to compare.\n\n\nWe can learn more by splitting the histogram into two groups: imported and domestic beers. This lets us see whether the sales volume distribution looks different for each group. We can do this by mapping the imported variable to the fill aesthetic, which tells ggplot to colour each bar in the histogram according to that variable’s value. In our case, this means domestic and imported beers will each get their own colour, so we can compare their sales distributions.\n\nggplot(beer) +\n    geom_histogram(aes(x = qty, \n                       fill = imported\n                       ),\n                   binwidth = 5)\n\n\n\n\n\n\n\n\nBy splitting the histogram by import status, we can compare the sales patterns of domestic and imported beers directly. Most beers, domestic or imported, sell in small quantities most weeks. Domestic brands tend to have slightly more mid-range sales, while imports have proportionally more very-low-volume weeks. This might reflect differences in popularity, price, or store stocking patterns.\n\n\n5 min\n\n\nCreate histograms to visualize the distribution of quantity sold across different beer brands\nCreate histograms to visualize the distribution of prices across different beer brands\n\n\n\nSolution\n\n\n\n\n\n\nggplot(beer) +\n    geom_histogram(aes(x = qty, fill = brand), alpha = 0.35)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(beer) +\n    geom_histogram(aes(x = price, fill = brand), alpha = 0.35)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\nDensity Plot\nHistograms are great for showing how often different quantities occur, but sometimes the bars make it hard to compare two groups side-by-side. A density plot is like a smoothed histogram — it turns the bars into a curve that’s easier to compare. Instead of showing how many weeks fall into each bin, the y-axis now shows density, which stretches or shrinks the bars so that the overall shape reflects the distribution rather than the total number of observations. This means we can compare domestic and imported beers on the same scale, even if one sells far more than the other.\n\nggplot(beer) +\n    geom_density(aes(x = qty))\n\n\n\n\n\n\n\n\nThe density plot shows the overall shape of a distribution by smoothing across values, rather than counting how many observations fall into fixed-width bins. This smoothing can make patterns easier to see—especially when comparing groups—because it removes the distraction of bin boundaries. A useful way to think of the difference between a histogram and a density is as follows: a histogram is a photo with sharp edges; a density plot is a blurred version that keeps the main shapes but smooths out the noise.\n\n\n\n\n\n\nTipTip: Reading a density plot\n\n\n\n\nY-axis change: Density is not a count—it’s scaled so the area under each curve = 1.\nPeaks: High peaks = more common values.\nSpread: Wider curves = more variation in the data.\nOverlap: The more the curves overlap, the more similar the distributions.\n\n\n\nWe can now look at how the densities differ between domestic and imported beer:\n\nggplot(beer) +\n    geom_density(aes(x = qty, fill = imported))\n\n\n\n\n\n\n\n\nWe see the same right-skewed shape as in the histograms—most sales are small in quantity. Domestic beers have a slightly broader spread into mid-range sales, while imports are more concentrated at very low quantities\n\n\n\nDetour: Adding Style to our plots\nLet’s pause and learn how to make the ones we already have easier to read and interpret. We’ve already used fill to compare groups (e.g., imported vs domestic beers), so you’ve seen how visual styling can make a difference. Styling in ggplot is about guiding the reader’s eye to the important parts of the story—it’s not just decoration. Thoughtful styling helps the reader spot patterns faster, compare values more easily, and remember the message. Every visual element—colour, background, text, scales—either supports or distracts from the story you’re telling, so the goal is to make each one work in service of your main message. In practice, we can think of styling as a set of “knobs” we can adjust to make our plot clearer, cleaner, and more effective.\n\nColor / Fill\nUse colour to encode categories or to highlight important differences you want the reader to notice quickly. In our beer example, using one colour for “imported” and another for “domestic” makes the contrast immediately clear. In ggplot, you have two main options:\n\nfill changes the inside colour of shapes (e.g., bars in a bar chart, the interior of a box in a boxplot, or the area under a density curve).\ncolor (spelled without the “u” in code) changes the outline or border colour of shapes, or the colour of points and lines when there is no interior to fill.\n\nFor example, a histogram or boxplot typically uses fill, while a line chart or scatterplot usually uses color. When deciding what colors to use, consistency is key: if “imported” is blue in one plot, it should be blue in every plot in the report. This avoids confusion and helps the audience build a visual memory for the categories. Also, avoid using too many colours—stick to the minimum needed to tell your story clearly.\nBecause colour carries meaning, it’s important to choose palettes that are readable and accessible to everyone, including people with colour vision deficiencies. This course uses the Okabe-Ito colour palette. The Okabe–Ito palette is a scientifically designed set of eight colours chosen to remain distinct for people with the most common forms of colour blindness. It’s widely recommended in scientific publishing because it makes plots more inclusive without sacrificing clarity for those with typical colour vision. By using this palette, you ensure that your plots remain interpretable to the widest possible audience without sacrificing clarity or visual appeal. To use this palette, we add a new line to our ggplot code. Let’s update our price box plot with the new color palette:\n\nggplot(beer) +\n    geom_boxplot(aes(y = price, \n                     x = imported, \n                     fill = imported)) +\n    scale_fill_okabe_ito()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteAbout the Okabe–Ito palette\n\n\n\n\n\nThe Okabe–Ito palette (Okabe & Ito, 2002) is a scientifically designed set of eight colours selected to remain distinguishable for viewers with common forms of colour‑vision deficiency while still looking balanced for typical colour vision. The result is eight colours chosen for maximum distinguishability, even when converted to simulated colour-blind vision. Unlike many “rainbow” palettes that blend into confusion for some viewers, the Okabe–Ito set keeps categories clearly separated, reducing the risk of misinterpretation. It’s widely recommended in scientific figures because it improves inclusivity without sacrificing clarity.\nHere are the colors the Okabe-Ito pallete uses:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThemes\nThemes control the non-data elements of your plot: background colour, grid lines, axis text, titles, legend placement, and more. They don’t change the data itself, but they can make the plot easier—or harder—to read.\nJust like colour choices, themes should help your audience focus on the key message. For example, a busy grid might distract from the pattern you want to highlight, while a clean background and minimal grid lines can make comparisons easier. Think of themes as the “dress code” for your plot—formal, casual, or somewhere in between.\nIn ggplot2, you can tweak themes manually with functions like theme(legend.position = \"none\"), or apply complete presets such as theme_minimal(), theme_classic(), or theme_bw(). Presets give you a quick starting point, and you can then layer on custom changes to suit your needs.\nIn this course, we’ll mostly use theme_minimal() as our default. It removes unnecessary chart junk, keeps the focus on the data, and prints cleanly for reports. It also works well when we layer additional style elements like colour palettes or custom labels.\nLet’s use theme_minimal() along with the code that removes the legend in our price box plot. Note that we more often than not, the legend is useful, and turning it off is more so about showing you what we can do than that we should:\n\nggplot(beer) +\n    geom_boxplot(aes(y = price, \n                     x = imported, \n                     fill = imported)) +\n    scale_fill_okabe_ito() + \n    theme_minimal() +\n    theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipComparing Common ggplot2 Themes\n\n\n\n\n\nHere’s the same plot shown with four different ggplot2 theme presets.\nNotice how each changes the background, grid lines, and overall look, without altering the data.\n\np &lt;- ggplot(beer) +\n    geom_boxplot(aes(y = price, x = imported, fill = imported)) +\n    scale_fill_okabe_ito()\n(\n    (p + theme_grey()   + labs(title = \"theme_grey()\"))   |\n    (p + theme_bw()     + labs(title = \"theme_bw()\"))  \n    )   /\n( \n    (p + theme_classic() + labs(title = \"theme_classic()\")) |\n    (p + theme_minimal() + labs(title = \"theme_minimal()\"))\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nTitles\nA clear, concise title tells the reader what they’re looking at before they even process the axes or legend. Think of it as the “headline” for your plot—something that gives context at a glance. Avoid vague titles like Sales Data; instead, be specific: Weekly Beer Prices by Import Status.\nGood titles are:\n\nInformative – they tell the viewer what is being shown and how it’s grouped.\nConcise – avoid long sentences or jargon.\nConsistent – if your plots are part of a series, use a consistent style and order of information.\n\nIn ggplot2, titles are usually added with the labs() function using the title argument. Let’s add a title to our box plot:\n\nggplot(beer) +\n    geom_boxplot(aes(y = price, \n                     x = imported, \n                     fill = imported)) +\n    scale_fill_okabe_ito() + \n    labs(\n        title = \"Weekly Beer Prices by Import Status\"\n    ) +\n    theme_minimal() +\n    theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nYou can also add a subtitle for more context and a caption to show the data source:\n\nggplot(beer) +\n    geom_boxplot(aes(y = price, \n                     x = imported, \n                     fill = imported)) +\n    scale_fill_okabe_ito() + \n    labs(\n        title = \"Weekly Beer Prices by Import Status\",\n        subtitle = \"Dominicks Finer Foods Stores\",\n        caption = \"Source: Univeristy of Chicago Booth School of Business Kilt's Center for Marketing Research\"\n    ) +\n    theme_minimal() +\n    theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n::: {callout-tip title=“Function Order in ggplot2”}\nIn ggplot2, the order of the functions in your plot code is generally irrelevant, as long as each layer or setting has the information it needs.\nFor example, theme_minimal() can be placed before or after labs(), and your plot will look the same.\nThe exception is when a later layer overrides an earlier one—for example, adding a new theme() call after another theme() will replace some settings. :::\n\n\nAxis Labels\nClear, informative axis labels make your plots easier to understand without extra explanation. A good axis label should answer: “What is being measured?” and “In what units?”\nYou can set axis labels using the labs() function, by specifying x = and y = arguments:\n\nggplot(beer) +\n    geom_boxplot(aes(y = price, \n                     x = imported, \n                     fill = imported)) +\n    scale_fill_okabe_ito() + \n    labs(\n        title = \"Weekly Beer Prices by Import Status\",\n        x = \"Production Location\",\n        y = \"Price (USD)\"\n    ) +\n    theme_minimal() +\n    theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nKeeping axis labels descriptive is especially important when your plot might be seen outside the context of your analysis—such as in a report or presentation slide. Instead of y = \"Price\", specify y = \"Price (USD)\". Instead of x = \"Type\", write x = \"Beer Type\". Note that the best axis label depends on the audience. For a technical audience, precise measurement units are important. For a business executive audience, shorter, cleaner labels may be better—while still avoiding jargon.\nWhen category names on the x-axis are long (or there are many of them), the labels can overlap and become unreadable. One fix is to rotate the text so each label has its own space:\n\nggplot(beer) +\n    geom_boxplot(aes(y = price, \n                     x = imported, \n                     fill = imported)) +\n    scale_fill_okabe_ito() + \n    labs(\n        title = \"Weekly Beer Prices by Import Status\",\n        x = \"Production Location\",\n        y = \"Price (USD)\"\n    ) +\n    theme_minimal() +\n    theme(legend.position = \"none\",\n           axis.text.x = element_text(angle = 45, hjust = 1)\n          )\n\n\n\n\n\n\n\n\nRotating to 45 degrees is often a good balance between readability and space. The hjust = 1 shifts the label’s anchor point so it lines up neatly with the tick mark.\nNotice that we adjusted the rotation of the x-axis labels inside theme() rather than labs().\nThat’s because:\n\nlabs() is for content — it changes the text that appears (the wording of titles, subtitles, captions, and axis labels).\ntheme() is for appearance — it controls how that text (and other plot elements) is drawn: font size, colour, position, rotation, margins, and so on.\n\nRotating the axis text is purely a formatting change; it doesn’t alter what the label says, only how it’s displayed. That makes it part of the plot’s theme, not its labels.\n::: {callout-tip collapse=true title=“Centering the Plot Title”} In ggplot2, you center the title by setting its horizontal justification in theme(), similarly to how we modified the axis labels.\nThe following code centers the axis title:\n\nggplot(beer) +\n    geom_boxplot(aes(y = price, \n                     x = imported, \n                     fill = imported)) +\n    scale_fill_okabe_ito() + \n    labs(\n        title = \"Weekly Beer Prices by Import Status\",\n        x = \"Production Location\",\n        y = \"Price (USD)\"\n    ) +\n    theme_minimal() +\n    theme(legend.position = \"none\",\n          axis.text.x = element_text(angle = 45, hjust = 1),\n          plot.title = element_text(hjust = 0.5) \n          )\n\n\n\n\n\n\n\n\nSee again that we use the hjust argument in the element_text() function. hjust controls the horizontal justification of text: How it works:\n\n0 = left aligned\n0.5 = centered\n1 = right aligned\n\nYou likely won’t remember these. That’s OK, we always search the help to to remember how to center things too! :::\n\n\nAxis Scales\nSo far, we’ve left ggplot to decide how to break up the axes and how to display the numbers. The scale_*_*() family of functions lets you take control. You can:\n\nChange the break points on an axis (where the tick marks appear)\nAdjust labels (e.g., show currency, percentages, dates in a different format)\nSwitch between continuous and categorical scales\n\nFor example, to show beer prices as dollars rather than plain numbers, we want to change the y axis to dollars and keep the scale as continuous:\n\nggplot(beer) +\n    geom_boxplot(aes(y = price, x = imported, fill = imported)) +\n    scale_y_continuous(\n        labels = scales::dollar\n      ) +\n    scale_fill_okabe_ito() +\n    theme(legend.position = \"none\") +\n    labs(y = \"Price\", x = \"Production Location\") + \n    theme_minimal()\n\n\n\n\n\n\n\n\nAnd if we want to have the axis ticks appear every 50 cents we add a breaks argument:\n\nggplot(beer) +\n    geom_boxplot(aes(y = price, x = imported, fill = imported)) +\n    scale_y_continuous(\n        breaks = c(3, 3.5, 4, 4.5, 5, 5.5, 6),\n        labels = scales::dollar\n      ) +\n    scale_fill_okabe_ito() +\n    theme(legend.position = \"none\") +\n    labs(y = \"Price\", x = \"Production Location\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipUnderstanding scale_*_*() naming\n\n\n\n\n\nThe first * refers to the axis:\n\nx for the x-axis\ny for the y-axis\n\nThe second * refers to the type of data on that axis:\n\ncontinuous for numeric/quantitative data\ndiscrete for categories (factors or character variables)\n\nExample:\n\nscale_x_continuous() → Adjusts breaks/labels for a numeric x-axis\nscale_y_discrete() → Adjusts breaks/labels for a categorical y-axis\n\n\n\n\n\n\n\n\n\n\nTipCommon labels helpers from the scales package\n\n\n\n\n\n\n\n\n\n\n\n\n\nHelper\nExample Output\nUse Case\n\n\n\n\nscales::dollar\n$3.50\nPrices, money\n\n\nscales::percent\n25%\nProportions, rates\n\n\nscales::comma\n12,345\nLarge whole numbers\n\n\nscales::number\n12.3\nGeneral numbers\n\n\nscales::date_format(\"%b %Y\")\nJan 2024\nDates with custom formatting\n\n\n\n\n\n\n\n\nAdjusting Transparency with alpha\nWhen two groups overlap in a plot—such as imported and domestic beer histograms—it can be hard to see both clearly if the bars are fully opaque. The alpha argument in the aes() function controls transparency on a scale from 0 (completely transparent) to 1 (fully opaque).\nA lower alpha lets overlapping areas show through, making it easier to compare groups. For example:\n\nggplot(beer) +\n    geom_histogram(\n        aes(x = qty, fill = imported),\n        position = \"identity\",\n        alpha = 0.6,\n        binwidth = 5\n    )\n\n\n\n\n\n\n\n\nHere, alpha = 0.6 makes each bar slightly transparent, so you can see where domestic and imported beer distributions overlap. Use alpha sparingly—too much transparency can make colours look washed out and hard to interpret. Choosing the right value of alpha for an individual plot, is is typically trial and error.\n\n\n5 min\n\n\nCustomize the quantity sold histogram to use the Okabe-Ito color palette and add axis labels.\nChange the plot’s theme to use theme_minimal().\nMake any other changes to the quantity sold histogram that you think improve it. Store the result as sales_plot\nMake any final changes to the price box plot you that think improve it. Store the results as ``\n\n\n\nSolution\n\n\n\nsales_plot &lt;-\n    ggplot(beer) +\n    geom_density(aes(x = qty, fill = imported), \n                 alpha = 0.35\n                 ) +\n    scale_fill_okabe_ito() +\n    theme_minimal()\n\n\nsales_plot\n\n\n\n\n\n\n\n\n\nprice_plot &lt;- ggplot(beer) +\n    geom_boxplot(aes(y = price, x = imported, fill = imported)) +\n    scale_y_continuous(\n        breaks = c(3, 3.5, 4, 4.5, 5, 5.5, 6),\n        labels = scales::dollar\n      ) +\n    scale_fill_okabe_ito() +\n    theme(legend.position = \"none\") +\n    labs(y = \"Price\", x = \"Production Location\") +\n    theme_minimal()\n\n\n\n\n\nPrice–Quantity Relationship\nUp to this point, we’ve explored individual variables—examining price distributions, sales quantities, and ways to style plots for clarity. The next step is to combine two variables to see how they relate. In economics, a “demand curve” usually means a theoretical relationship showing how quantity demanded changes if all other factors stay constant. What we’ll create here looks similar, but it’s simply the relationship between price and sales in our beer data. It’s not a controlled experiment, so other factors—like brand popularity, promotions, or season—may also influence what we see. Even so, plotting price against quantity sold can highlight patterns worth investigating further.\nTo visualise how two continuous variables—like price and quantity—relate, we can use a scatter plot. In ggplot, scatter plots are made with geom_point(), where each point represents one observation in the data. By mapping qty to the x-axis and price to the y-axis, we put sales volume and price on the same plot, allowing us to see whether higher prices are associated with lower sales (or vice versa):\n\nggplot(beer) +\n    geom_point(aes(y = price, \n                   x = qty)\n               )\n\n\n\n\n\n\n\n\nLots of points land on top of each other. We can use what we know about adding transparency (alpha) to to make overlapping points easier to distinguish, revealing where observations are densely or sparsely clustered. We’ll also separate out imported and domestic beer brands using the color option:\n\nggplot(beer) +\n    geom_point(aes(y = price, \n                   x = qty,\n                   color = imported),\n               alpha = 0.25\n               ) +\n    scale_color_okabe_ito()\n\n\n\n\n\n\n\n\nNot every raw data plot will look neat and tidy—and that’s okay. At this stage, our goal was to see the relationship, not to produce a perfect chart. But sometimes, the way data are spread out can make patterns hard to spot.\nOne common trick when working with sales and prices is to use a log–log plot—that is, to take the logarithm of both variables before plotting. Why bother? Because in most sales datasets, quantities can range from just a handful of units to hundreds or even thousands, and prices can also differ a lot between products. When we plot these values on their original scales, most of the data ends up squashed into a small section of the graph, with the rest of the space dominated by a few extreme points.\nTaking the logarithm of both variables helps solve this problem in two ways. First, it spreads out smaller values, so we can see differences between low-volume products more clearly. Second, it compresses very large values, so they don’t overwhelm the plot. The result is a clearer view of the overall relationship between price and quantity.\nThere’s also a neat interpretation benefit: in a log–log plot, straight-line patterns often suggest a constant percentage relationship between the two variables. In business terms, this means we can start thinking about price elasticity—how a percentage change in price might be associated with a percentage change in quantity sold. That makes log–log plots a handy first step toward connecting our visual analysis with economic reasoning.\nWhen we apply the logarithm directly inside the plot code — log(price) for the y-axis and log(qty) for the x-axis — we’re transforming the data on the fly. This means we don’t have to create new columns in our dataset just to see the log–log relationship. T he scatter plot now shows each product’s weekly sales and price on a scale where equal distances represent equal percentage changes:\n\nggplot(beer) +\n    geom_point(aes(y = log(price), \n                   x = log(qty),\n                   color = imported),\n               alpha = 0.25\n               ) +\n    scale_color_okabe_ito()\n\n\n\n\n\n\n\n\nNotice how the points are now more evenly spread across the plot area. Low-quantity, low-price products no longer bunch up in one corner, and the influence of extreme values is reduced. This makes it much easier to spot patterns in the data—like whether higher prices tend to be associated with lower quantities sold—and to start thinking about how strong that relationship might be.\nAnother way to create a log–log plot is to leave the original variables in the data and let ggplot handle the transformation through the scale functions. Instead of wrapping log() around price and qty inside aes(), we can use scale_x_continuous(trans = \"log\") and scale_y_continuous(trans = \"log\"). This means we are applying a logarithmic transformation to the axes.\n\nggplot(beer) +\n    geom_point(aes(y = price, \n                   x = qty,\n                   color = imported),\n               alpha = 0.25\n               ) +\n    scale_color_okabe_ito() +\n # Natural log scale for x-axis with meaningful breaks\n  scale_x_continuous(\n    trans = \"log\",\n    breaks = c(1, 2, 5, 10, 20, 50, 100, 200)\n  ) + \n      # Natural log scale for y-axis with meaningful breaks\n  scale_y_continuous(\n    trans = \"log\",\n    breaks = c(3, 4, 5, 6),\n    labels = scales::dollar\n  ) +\n    labs(x = \"Quantity\", y = \"Price\")\n\n\n\n\n\n\n\n\nThis approach has two big advantages:\n\nCleaner code – you keep the plotting aesthetics simple while still getting the transformation you want.\nMore control over axis appearance – you can set breaks and labels that are easy to interpret, even when using a logarithmic scale.\n\nIn the example below, we specify meaningful breaks for both axes—like quantities of 1, 2, 5, 10, and so on—so the reader doesn’t have to mentally decode what each tick mark means. We also keep the price axis in dollar format, even though it’s on a log scale, which helps connect the plot back to the real-world numbers.\n\nCreating Subplots with facet_wrap()\nSometimes, we want to compare the same relationship across different categories — without cramming everything into a single plot. In our beer data, suppose we want to see whether the price–quantity pattern differs between domestic and imported beer. One way is to make a separate plot for each type… but that’s tedious. Instead, ggplot2 lets us split one plot into small multiples — called facets — so each category gets its own panel but shares the same scales. This can make comparisons much easier. Here’s the code that does this:\n\nggplot(beer) +\n    geom_point(aes(y = price, x = qty,\n                   color = imported), \n               alpha = 0.25\n               ) +\n     facet_wrap(~ imported) +\n # Natural log scale for x-axis with meaningful breaks\n  scale_x_continuous(\n    trans = \"log\",\n    breaks = c(1, 2, 5, 10, 20, 50, 100, 200)\n  ) + \n      # Natural log scale for y-axis with meaningful breaks\n  scale_y_continuous(\n    trans = \"log\",\n    breaks = c(3, 4, 5, 6),\n    labels = scales::dollar\n  ) +\n    labs(x = \"Quantity\", y = \"Price\") + \n    scale_color_okabe_ito() +\n    theme_minimal() +\n    theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nThe facet_wrap(~ type) line tells ggplot:\n\n~ type → make a panel for each value of the type variable\nShare the same x- and y-axes so differences in shape are easy to see\n\nWhen we separate imported and domestic beers, differences in how quantity responds to price become clearer. If the imported beer points drop away more steeply as price rises, it suggests imported beers are more price-sensitive—customers cut back faster when prices go up. Faceting this way makes these patterns easier to see for each group, helping us spot where pricing changes might have a bigger impact on sales.\n\n\nAdding a Statistical Transformation\nWhen you build plots with ggplot, you’re really stacking layers: a data layer, one (or more) geometry layers (points, boxes, lines), plus optional styling layers (scales, themes, labels). Up to now we’ve used one geom in each plot. Now we’ll add another layer: a smooth trend line that helps the eye see the overall relationship between price and quantity.\nAdding a smoothing layer can make patterns in your data much easier to see. A fitted line condenses the overall direction—whether it’s upward, downward, or flat—into a single visual cue, even when the raw data is noisy. When combined with faceting or colour grouping (such as imported vs domestic beers), separate lines make it easier to compare slopes and spot differences between categories. On a log–log plot, a straight fitted line suggests a roughly constant percentage relationship between the two variables, which is a useful way to think about price sensitivity.\nHere’s what we’ll add: we’ll keep the scatter points so the raw data remains visible, but layer on a smoothed line to highlight the main pattern in the data. This line will be drawn using a statistical transformation—such as a simple linear model—so it captures the overall relationship. Together, they tell both the detail and the trend stories on one figure. We choose a linear model because it is a simple, disciplined way of drawing the straight line that best describes the overall relationship between the two variables. We can include a shaded confidence band to show the uncertainty around the fit, or turn it off for a cleaner look. We’ll also be sure that the line’s colour will be distinct but subtle, so it complements the points without overwhelming them.\n\nggplot(beer) +\n    geom_point(aes(y = price, x = qty,\n                   color = imported), \n               alpha = 0.25\n               ) +\n    # This is the addition of the linear model\n    geom_smooth(aes(y = price, x = qty), \n                  method = \"lm\", \n                # add a complementary okabe_ito color\n                  color = palette_okabe_ito()[7],\n                  se = TRUE) +\n     facet_wrap(~ imported) +\n # Natural log scale for x-axis with meaningful breaks\n  scale_x_continuous(\n    trans = \"log\",\n    breaks = c(1, 2, 5, 10, 20, 50, 100, 200)\n  ) + \n      # Natural log scale for y-axis with meaningful breaks\n  scale_y_continuous(\n    trans = \"log\",\n    breaks = c(3, 4, 5, 6),\n    labels = scales::dollar\n  ) +\n    labs(x = \"Quantity\", y = \"Price\") + \n    scale_color_okabe_ito() +\n    theme_minimal() +\n    theme(legend.position = \"none\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\ngeom_smooth() lets us add a fitted line on top of our scatter plot. By setting method = \"lm\", we’re asking ggplot to draw a straight line based on a simple linear model. The aes(y = price, x = qty) tells ggplot which variables to use—here, the same ones as our points. We’ve picked a contrasting colour from the Okabe–Ito palette (palette_okabe_ito()[7]) so the line is easy to spot without overpowering the data. With se = TRUE, ggplot shades in a confidence band around the line, giving us a quick visual sense of how certain (or uncertain) that fitted relationship is.\n\n\n\n\n\n\nCautionCaution on interpretation\n\n\n\nThis fitted line shows association, not causation. In our retail data, promotions, seasons, and brand effects can also influence the pattern. Treat the line as a visual guide, not proof of a causal “demand curve.”\n\n\n\n\n\n\n\n\nTipAvoid repeating yourself\n\n\n\n\n\nNotice we wrote aes() twice—once for the points and again for the smoothing line.\nIf you plan to use the same aesthetics across multiple layers, you can put them inside the initial ggplot() call.\nThat way, every geom_ layer inherits them automatically, keeping your code cleaner and easier to read.\nHere’s how we reduce the duplication in the use of aes():\n\nggplot(beer, \n       aes(y = price, x = qty)\n       ) +\n    geom_point(aes(color = imported), \n               alpha = 0.25\n               ) +\n    # This is the addition of the linear model\n    geom_smooth(method = \"lm\", \n                # add a complementary okabe_ito color\n                  color = palette_okabe_ito()[7],\n                  se = TRUE) +\n     facet_wrap(~ imported) +\n # Natural log scale for x-axis with meaningful breaks\n  scale_x_continuous(\n    trans = \"log\",\n    breaks = c(1, 2, 5, 10, 20, 50, 100, 200)\n  ) + \n      # Natural log scale for y-axis with meaningful breaks\n  scale_y_continuous(\n    trans = \"log\",\n    breaks = c(3, 4, 5, 6),\n    labels = scales::dollar\n  ) +\n    labs(x = \"Quantity\", y = \"Price\") + \n    scale_color_okabe_ito() +\n    theme_minimal() +\n    theme(legend.position = \"none\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5 min\n\n\nModify the plot above to create separate subplots for brands rather than import status.\nModify the plot so that there are separate colors and linear models per brand, but the facets are by import statis.\n\n\n\nSolution\n\n\n\nggplot(beer, \n       aes(y = price, x = qty)\n       ) +\n    geom_point(aes(color = brand), \n               alpha = 0.25\n               ) +\n    # This is the addition of the linear model\n    geom_smooth(method = \"lm\", \n                # add a complementary okabe_ito color\n                  color = palette_okabe_ito()[7],\n                  se = TRUE) +\n     facet_wrap(~ brand) +\n # Natural log scale for x-axis with meaningful breaks\n  scale_x_continuous(\n    trans = \"log\",\n    breaks = c(1, 2, 5, 10, 20, 50, 100, 200)\n  ) + \n      # Natural log scale for y-axis with meaningful breaks\n  scale_y_continuous(\n    trans = \"log\",\n    breaks = c(3, 4, 5, 6),\n    labels = scales::dollar\n  ) +\n    labs(x = \"Quantity\", y = \"Price\") + \n    scale_color_okabe_ito() +\n    theme_minimal() +\n    theme(legend.position = \"none\")\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Getting Our First Insights from Data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualisation for Business Intelligence</span>"
    ]
  },
  {
    "objectID": "insights/data_visualisation.html#bringing-it-all-together-sharing-our-results",
    "href": "insights/data_visualisation.html#bringing-it-all-together-sharing-our-results",
    "title": "4  Data Visualisation for Business Intelligence",
    "section": "4.5 Bringing it all together: Sharing Our Results",
    "text": "4.5 Bringing it all together: Sharing Our Results\n\n\n\n\n\n\nTipOur Three Plots\n\n\n\n\n\nHere is the code for the three plots we have produced so far. We store each of these as their own R objects.\n\nPrice Box Plot\n\n\nprice_plot &lt;- \n    ggplot(beer) +\n    geom_boxplot(aes(y = price, x = imported, fill = imported)) +\n    scale_y_continuous(\n        breaks = c(3, 3.5, 4, 4.5, 5, 5.5, 6),\n        labels = scales::dollar\n      ) +\n    scale_fill_okabe_ito() +\n    theme(legend.position = \"none\") +\n    labs(y = \"Price\", x = \"Production Location\") +\n    theme_minimal()+\n    theme(legend.position = \"none\")\n\n\nThe Quantity Sold Density\n\n\nsales_plot &lt;-\n    ggplot(beer) +\n    geom_density(aes(x = qty, fill = imported), \n                 alpha = 0.35\n                 ) +\n      # Natural log scale for x-axis with meaningful breaks\n      scale_x_continuous(\n        trans = \"log\",\n        breaks = c(1, 2, 5, 10, 20, 50, 100, 200)\n      ) +\n    labs(y = \"Density\", x = \"Quantity Sold\") +\n    scale_fill_okabe_ito() +\n    theme_minimal() \n\n\nThe Price-Quantity Relationship\n\n\nprice_qty &lt;- \n    ggplot(beer, \n       aes(y = price, x = qty)\n       ) +\n    geom_point(aes(color = imported), \n               alpha = 0.25\n               ) +\n    # This is the addition of the linear model\n    geom_smooth(method = \"lm\", \n                # add a complementary okabe_ito color\n                  color = palette_okabe_ito()[7],\n                  se = TRUE) +\n     facet_wrap(~ imported) +\n # Natural log scale for x-axis with meaningful breaks\n  scale_x_continuous(\n    trans = \"log\",\n    breaks = c(1, 2, 5, 10, 20, 50, 100, 200)\n  ) + \n      # Natural log scale for y-axis with meaningful breaks\n  scale_y_continuous(\n    trans = \"log\",\n    breaks = c(3, 4, 5, 6),\n    labels = scales::dollar\n  ) +\n    labs(x = \"Quantity\", y = \"Price\") + \n    scale_color_okabe_ito() +\n    theme_minimal() +\n    theme(legend.position = \"none\")\n\n\n\n\nWe’ve now built several plots—each showing a different angle on the same sales and pricing story. But in a real analysis (or a business meeting), you often want to put those views side by side so the audience can compare them at a glance. Instead of juggling separate images, we can combine them into one display. The patchwork package makes this easy, letting us “add” plots together with simple operators so related visuals live in the same frame. This way, we can present a complete picture without scrolling, flipping slides, or asking the reader to mentally stitch things together.\nLet’s start with the simplest layout: putting all our plots in a single row. With patchwork, this is as easy as using the | operator between plot objects. Think of | as saying “put this next to that.”\n\nprice_qty | price_plot | sales_plot\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThis tells R to take price_qty, place it on the left, then price_plot next to it, then sales_plot on the right. The result is one combined graphic where each plot keeps its own style, scales, and labels—but they’re visually aligned. However, the plots are cramped together to fit along one row. We might instead want to arrange our plots into multiple rows to make them easier to read. For example, we can put the price_qty plot on the top row, and then the price_plot and sales_plot on the second:\n\nprice_qty /\n    (price_plot | sales_plot)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nIn this layout, we’re mixing rows and columns. The / operator tells patchwork to stack things vertically—so here, price_qty (our first plot) goes on the top row. The parentheses group price_plot | sales_plot together, placing them side-by-side on the second row. Think of it like building with LEGO: / makes a new row, | fills that row with plots, and parentheses let you bundle pieces together before stacking. This way, we can create more complex, story-driven layouts while still keeping the code easy to read.\n\n\n5 min\n\nWrite 3-4 sentence summary of the main takeaways from the visualization that you could send to a category manager\n\n\n\n10 min\n\n\nConstruct a similar plot to the one we’ve constructed above, but separate the plots by brand rather than import status.\nWrite 3-4 sentence summary of the main takeaways from the visualization that you could send to a category manager",
    "crumbs": [
      "Getting Our First Insights from Data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualisation for Business Intelligence</span>"
    ]
  },
  {
    "objectID": "insights/data_visualisation.html#always-visualize-the-data",
    "href": "insights/data_visualisation.html#always-visualize-the-data",
    "title": "4  Data Visualisation for Business Intelligence",
    "section": "4.6 Always visualize the data!",
    "text": "4.6 Always visualize the data!\nBefore we wrap up our work with ggplot, it’s worth pausing to remember why we spend so much time visualising data in the first place. Summary statistics like the mean and standard deviation are useful, but they can hide important patterns, relationships, and even strange quirks in the data. Two datasets can have the exact same averages and spreads yet tell completely different stories when you actually plot them. A famous—and fun—example of this is known as the Datasaurus Dozen, which shows how relying on numbers alone can be misleading. Let’s first load the datasets which are stored in an R package:\n\nlibrary(datasauRus)\n\ndino_doz &lt;- datasaurus_dozen\n\nNow we looks at the summary statistics for each data set. In particular we compare the mean and standard deviations:\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Dataset\n                x mean\n                x sd\n                y mean\n                y sd\n              \n        \n        \n        \n                \n                  dino\n                  54.3\n                  16.8\n                  47.8\n                  26.9\n                \n                \n                  away\n                  54.3\n                  16.8\n                  47.8\n                  26.9\n                \n                \n                  h_lines\n                  54.3\n                  16.8\n                  47.8\n                  26.9\n                \n                \n                  v_lines\n                  54.3\n                  16.8\n                  47.8\n                  26.9\n                \n                \n                  x_shape\n                  54.3\n                  16.8\n                  47.8\n                  26.9\n                \n                \n                  star\n                  54.3\n                  16.8\n                  47.8\n                  26.9\n                \n                \n                  high_lines\n                  54.3\n                  16.8\n                  47.8\n                  26.9\n                \n                \n                  dots\n                  54.3\n                  16.8\n                  47.8\n                  26.9\n                \n                \n                  circle\n                  54.3\n                  16.8\n                  47.8\n                  26.9\n                \n                \n                  bullseye\n                  54.3\n                  16.8\n                  47.8\n                  26.9\n                \n                \n                  slant_up\n                  54.3\n                  16.8\n                  47.8\n                  26.9\n                \n                \n                  slant_down\n                  54.3\n                  16.8\n                  47.8\n                  26.9\n                \n                \n                  wide_lines\n                  54.3\n                  16.8\n                  47.8\n                  26.9\n                \n        \n      \n    \n\n\n\nSummary statistics are identical for all datasets. But when we plot them, we see there are very different patterns in each dataset:\n\nggplot(dino_doz,\n       aes(x = x,\n           y = y)) +\n    geom_point() +\n    facet_wrap(~dataset, ncol = 3)\n\n\n\n\n\n\n\n\nWithout visualising the data, we likely would have missed these very different patterns!",
    "crumbs": [
      "Getting Our First Insights from Data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualisation for Business Intelligence</span>"
    ]
  },
  {
    "objectID": "insights/data_visualisation.html#conclusion",
    "href": "insights/data_visualisation.html#conclusion",
    "title": "4  Data Visualisation for Business Intelligence",
    "section": "4.7 Conclusion",
    "text": "4.7 Conclusion\nWe’ve covered a lot of ground in this chapter, but it all comes back to two main ideas. First, the process of building a ggplot is consistent no matter what you’re visualising:\n\nInitialize the plot with ggplot() – set your dataset and define the aesthetic mappings (aes()).\nAdd geometric layers (geom_*) – choose the right geometry for the data and question at hand.\nCustomize the appearance – adjust labels, themes, colours, and axes so the plot tells its story clearly.\n(Optionally) Add statistical transformations to make the main insight easier to spot.\n\nSecond, we paired each plot with one principle of effective data visualisation:\n\nPrice boxplot: Choose visualisations that match your question.\nHistograms & Density plots: Colour enhances categorical comparisons.\nPrice-Quantity Relationship: Transformations (like log scales) can reveal patterns.\n\nWhen you combine a clear plotting process with purposeful design choices, you get more than just nice-looking charts—you get business intelligence. This is one simple yet powerful example of how we turn raw numbers into stories that reveal patterns, answer questions, and guide decisions.\n\n\n\n\n\n\nNoteThe Analytics Workflow (Revisited)\n\n\n\n\n\nRecall our Analytics workflow from Chapter 2. Here’s how we used that in our data visualization journey.\nDefine the Business Question\n→ How does price affect sales? Are imported beers different?\nAcquire and Prepare Data\n→ We used real retail scanner data with brand, price, and sales volume.\nExplore and Visualise Patterns\n→ Through box plots, density plots, and log–log scatterplots.\nAnalyze and Interpret Findings\n→ Noting price dispersion, skewed demand, and elasticity differences.\nCommunicate Insights and Recommendations\n→ Using clear visuals that a category manager could act on.\n\n\n\n\n\n\n\n\n\nWarningBeyond the Geoms We Covered\n\n\n\nWe’ve only scratched the surface of what ggplot2 can do.\nThere are dozens of other geoms each suited to different types of data and questions.\nBefore you create a plot, think about what visual form best matches your data and your audience’s needs.\nYou can browse the full list (with examples) in the official ggplot2 reference.",
    "crumbs": [
      "Getting Our First Insights from Data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualisation for Business Intelligence</span>"
    ]
  },
  {
    "objectID": "insights/data_manipulation.html",
    "href": "insights/data_manipulation.html",
    "title": "5  Data Wrangling for Business Analytics",
    "section": "",
    "text": "5.1 The Business Challenge",
    "crumbs": [
      "Getting Our First Insights from Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Wrangling for Business Analytics</span>"
    ]
  },
  {
    "objectID": "insights/data_manipulation.html#the-business-challenge",
    "href": "insights/data_manipulation.html#the-business-challenge",
    "title": "5  Data Wrangling for Business Analytics",
    "section": "",
    "text": "The Topic: Understanding the Level and Composition of Firm Profits in the Australian Economy\nOn any given trading day, about 1.7 million transactions took place on the Australian Securities Exchange (ASX), moving approximately AUD 6.5 billion in value. Behind these trades are investors buying and selling claims on firms’ profits, and firms deploying that capital to acquire and manage assets in the pursuit of profit.\nIn this environment, the stakes are high: firms aim to maximise profit, while investors aim to maximise return on capital. Understanding which firms and industries excel in generating profits — and delivering returns — is not just an academic exercise; it’s a question that shapes investment strategies, market confidence, and economic growth.\nKey questions we will explore are: Which public firms produced the most profit in 2024? How did profit levels vary by industry? Which industries delivered the best returns for investors?\nBy analysing firm-level financial data, we can uncover patterns in profitability and capital efficiency that help explain the performance of Australia’s largest publicly listed companies.",
    "crumbs": [
      "Getting Our First Insights from Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Wrangling for Business Analytics</span>"
    ]
  },
  {
    "objectID": "insights/data_manipulation.html#the-data-financial-statements-from-yahoo-finance",
    "href": "insights/data_manipulation.html#the-data-financial-statements-from-yahoo-finance",
    "title": "5  Data Wrangling for Business Analytics",
    "section": "The Data: Financial Statements from Yahoo Finance",
    "text": "The Data: Financial Statements from Yahoo Finance\nTo answer these questions, we draw on audited 2024 financial statements for the 200 largest ASX-listed firms (measured by assets).\nFinancial statements provide a standardised snapshot of a company’s performance and financial health. In our data set, the key variables are:\n\nCompany name (conml) – the name of the firm as listed on the ASX\n\nIndustry (industry) – the industry classification, allowing comparison across sectors such as mining, banking, or healthcare\n\nProfit (ebit) – earnings before interest and taxes, representing core business profitability after expenses (from the income statement)\n\nAssets (at) – the total resources the company owns that have economic value (from the balance sheet)\n\nInvested Capital (invested_capital) – the total capital invested in the company by both shareholders and debt holders to fund operations and growth. It generally includes:\n\nEquity (shareequity) – the funds contributed by shareholders plus retained earnings\n\nInterest-bearing debt (dlc, dltt, dvp) – loans and borrowings the company uses for financing\n\n\nThis is real-world data that fundamental investors—those who evaluate companies based on underlying business performance—use to identify firms whose market price is lower than their “intrinsic value”. In other words, they look for undervalued firms with strong growth potential.\nBy working with this data, we can replicate the kind of assessments that professional analysts and fund managers use to guide billion-dollar investment decisions.\n\nLet’s dig into the numbers and see which firms — and which industries — were the real winners in 2024.\n\nThe Method: Using dplyr Transformations to Wrangle Data\n\n“The hardest part of data science isn’t building the model, it’s getting the data ready for the model.” — D.J. Patel (former US Chief Data Scientist)\n\nData cleaning and manipulation with the dplyr package involves transforming raw or messy datasets into clean, well-structured forms using a consistent set of grammar-like verbs in R.\nKey dplyr verbs include:\n- filter() to subset rows based on conditions,\n- mutate() to create new variables or modify existing ones,\n- select() to choose relevant columns,\n- arrange() to reorder rows,\n- summarise() to aggregate data, and\n- group_by() to split data into groups for grouped operations.\nUsing dplyr pipelines with the native pipe operator (|&gt;) or the magrittr pipe (%&gt;%) allows chaining multiple operations into a clear, readable sequence. This approach improves code clarity, reusability, and ease of debugging.\nClean, well-structured data is essential for reliable analysis and visualization. dplyr helps reduce manual errors, speeds up workflows, and ensures transparency and reproducibility — vital aspects in any business analytics project.\nMost data analysis workflows begin with a dplyr pipeline that prepares raw data into a tidy format, ready for modeling, visualization with ggplot2, or reporting and interactive dashboards using tools such as quarto and shiny.\n\n\nWhere we’re headed\nJust a few lines of R code can get data in the right form you need to generate interesting, important and actionable insights.\nFrom this:\n\n\n# A tibble: 10 × 30\n   gvkey  curcd fyear   fyr datadate     at  capx shareequity   dlc   dltt   dvp\n   &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1 013312 USD    2024     6 6/30/20… 102362 9273       49120  2084  18634     NA\n 2 210216 AUD    2024     6 6/30/20…  45550 2288       17352  4228  12740     NA\n 3 223003 USD    2024     6 6/30/20…  38022  849       19401   944  11239     NA\n 4 212650 AUD    2024     6 6/30/20…  36694  104       11678  1590  18596     NA\n 5 100894 AUD    2024     6 6/30/20…  33936 2548        5570  2311  14411     NA\n 6 212427 USD    2024     6 6/30/20…  30060 2834       19531   192   5208     NA\n 7 101601 AUD    2024     6 6/30/20…  27309  923        8585  1165  10113     NA\n 8 226744 AUD    2024     6 6/30/20…  20894  754.       5275.  606. 10332.    NA\n 9 220244 AUD    2024     6 6/30/20…  20564 2761         294   600   5991     NA\n10 017525 AUD    2024     6 6/30/20…  20454  608        9489    68   3310     NA\n# ℹ 19 more variables: ebit &lt;dbl&gt;, netprofit &lt;dbl&gt;, pstk &lt;dbl&gt;, sale &lt;dbl&gt;,\n#   epsexcon &lt;dbl&gt;, nicon &lt;dbl&gt;, conm &lt;chr&gt;, fic &lt;chr&gt;, conml &lt;chr&gt;,\n#   ggroup &lt;dbl&gt;, gind &lt;dbl&gt;, gsector &lt;dbl&gt;, gsubind &lt;dbl&gt;, sector &lt;chr&gt;,\n#   indgroup &lt;chr&gt;, industry &lt;chr&gt;, subind &lt;chr&gt;, debt &lt;dbl&gt;,\n#   invested_capital &lt;dbl&gt;\n\n\nTo this:",
    "crumbs": [
      "Getting Our First Insights from Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Wrangling for Business Analytics</span>"
    ]
  },
  {
    "objectID": "insights/data_manipulation.html#the-game-plan-what-were-aiming-for",
    "href": "insights/data_manipulation.html#the-game-plan-what-were-aiming-for",
    "title": "5  Data Wrangling for Business Analytics",
    "section": "5.2 The Game Plan: What we’re aiming for",
    "text": "5.2 The Game Plan: What we’re aiming for\nRecall our workflow:\n\nDefine the Business Question(s) & Plan Outputs\nAcquire and Prepare Data\nExplore and Visualize Patterns\nAnalyze and Interpret Findings\nCommunicate Insights and Recommendations\n\nSketch of the plan:\n\nDefine the Business Questions\nLoad financial statements data\nWrangle data by operating on rows to isolate and order a subset of observations\nUse bar plot to identify the firms that produce the largest profits and the largest losses\nWrangle data by operating on columns to widen or narrow data frame\nUse histogram to plot the distribution of returns to investors\nWrangle data by operating on groups to summarise and compare subsets of observations\nUse box plot to describe return distributions across industries\nAnalyze, Interpret & Communicate\n\nLet’s get started!",
    "crumbs": [
      "Getting Our First Insights from Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Wrangling for Business Analytics</span>"
    ]
  },
  {
    "objectID": "insights/data_manipulation.html#loading-and-understanding-the-data",
    "href": "insights/data_manipulation.html#loading-and-understanding-the-data",
    "title": "5  Data Wrangling for Business Analytics",
    "section": "5.3 Loading and understanding the data",
    "text": "5.3 Loading and understanding the data\n\nR packages for today\nMuch of this week’s material will involve our using the dplyr package, which is one of the core packages in the tidyverse (one nice feature of dplyr is that it behaves nicely with ggplot, tidyr, stringr, and other data wrangling and visualization packages). The dpylr package is designed specifically for data wrangling. We will use dplyr’s core manipulation functions-filter(), select(), arrange(), mutate(), summarise(), and group_by()-to clean our data frames and make them more readable.\nAnd so to begin, we need to load the packages that we will use for this chapter.\n\n# Load the tidyverse for data manipulation and visualisation\nlibrary(tidyverse)\n\n# Load scales for axis formatting and transformations\nlibrary(scales)\n\n# Load ggokabeito for a colour-blind-friendly palette (available if you choose to use it)\nlibrary(ggokabeito)\n\n# Load ggthemes for additional polished ggplot themes\nlibrary(ggthemes)\n\n# Load patchwork to combine multiple ggplots into one figure\nlibrary(patchwork)\n\n# Load stringr for consistent string manipulation helpers\nlibrary(stringr)\n\n# Load RColorBrewer for qualitative and sequential colour palettes\nlibrary(RColorBrewer)\n\n\n\n\n\n\n\nTipWhat is stringr?\n\n\n\n\n\nYou may notice that we load many of the packages that you used for data visualization in last week’s lecture and tutorial. While the focus of this week’s material is data wrangling, we will do so with a clear goal toward producing insightful plots. You may also notice an unfamiliar package, stringr. This package is a useful tool for working with strings (i.e., text data). It provides a set of functions that simplify common string manipulation tasks, some of which we will use in this week’s lecture and tutorial to create labels and titles for our plots from the text data in our dataframes.\n\n\n\n\n\nLoading the ASX financial statements data\nWe now import the financial statements data set, which covers the 200-largest ASX-listed firms in 2024. The data is from Yahoo Finance, a public website that collects and disseminates information that it gathers from firms’ audited financial statements (i.e., annual reports) and capital markets (i.e., stock prices and volumes).\n\n# Read in the 2024 ASX 200 data set\nasx_200_2024 &lt;- read_csv(\"data/asx_200_2024.csv\")\n\nSo, how is this data frame structured and what information does it contain?\n\nasx_200_2024 \n\n# A tibble: 200 × 30\n   gvkey  curcd fyear   fyr datadate     at  capx shareequity   dlc   dltt   dvp\n   &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1 013312 USD    2024     6 6/30/20… 102362 9273       49120  2084  18634     NA\n 2 210216 AUD    2024     6 6/30/20…  45550 2288       17352  4228  12740     NA\n 3 223003 USD    2024     6 6/30/20…  38022  849       19401   944  11239     NA\n 4 212650 AUD    2024     6 6/30/20…  36694  104       11678  1590  18596     NA\n 5 100894 AUD    2024     6 6/30/20…  33936 2548        5570  2311  14411     NA\n 6 212427 USD    2024     6 6/30/20…  30060 2834       19531   192   5208     NA\n 7 101601 AUD    2024     6 6/30/20…  27309  923        8585  1165  10113     NA\n 8 226744 AUD    2024     6 6/30/20…  20894  754.       5275.  606. 10332.    NA\n 9 220244 AUD    2024     6 6/30/20…  20564 2761         294   600   5991     NA\n10 017525 AUD    2024     6 6/30/20…  20454  608        9489    68   3310     NA\n# ℹ 190 more rows\n# ℹ 19 more variables: ebit &lt;dbl&gt;, netprofit &lt;dbl&gt;, pstk &lt;dbl&gt;, sale &lt;dbl&gt;,\n#   epsexcon &lt;dbl&gt;, nicon &lt;dbl&gt;, conm &lt;chr&gt;, fic &lt;chr&gt;, conml &lt;chr&gt;,\n#   ggroup &lt;dbl&gt;, gind &lt;dbl&gt;, gsector &lt;dbl&gt;, gsubind &lt;dbl&gt;, sector &lt;chr&gt;,\n#   indgroup &lt;chr&gt;, industry &lt;chr&gt;, subind &lt;chr&gt;, debt &lt;dbl&gt;,\n#   invested_capital &lt;dbl&gt;\n\n\nOur data frame is a tibble that has 200 rows (one for each of the 200-largest companies on the ASX in 2024) and 30 columns (each of which is a variable or identifier that contains information on that row’s company). Most of these columns contain financial statement information that describe the companies’ operations (i.e., the core economic activities that companies undertake to generate profits) and capital structure (i.e., how companies use debt and equity to fund their operations). Because our data frame is wide, many of these columns are ‘cut off’ and so cannot be viewed in our simple HTML table.\nLet’s view our tibble another way to shed light on each column and its contents:\n\nglimpse(asx_200_2024)\n\nRows: 200\nColumns: 30\n$ gvkey            &lt;chr&gt; \"013312\", \"210216\", \"223003\", \"212650\", \"100894\", \"21…\n$ curcd            &lt;chr&gt; \"USD\", \"AUD\", \"USD\", \"AUD\", \"AUD\", \"USD\", \"AUD\", \"AUD…\n$ fyear            &lt;dbl&gt; 2024, 2024, 2024, 2024, 2024, 2024, 2024, 2024, 2024,…\n$ fyr              &lt;dbl&gt; 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,…\n$ datadate         &lt;chr&gt; \"6/30/2024\", \"6/30/2024\", \"6/30/2024\", \"6/30/2024\", \"…\n$ at               &lt;dbl&gt; 102362.00, 45550.00, 38022.00, 36694.00, 33936.00, 30…\n$ capx             &lt;dbl&gt; 9273.000, 2288.000, 849.000, 104.000, 2548.000, 2834.…\n$ shareequity      &lt;dbl&gt; 49120.000, 17352.000, 19401.000, 11678.000, 5570.000,…\n$ dlc              &lt;dbl&gt; 2084.000, 4228.000, 944.000, 1590.000, 2311.000, 192.…\n$ dltt             &lt;dbl&gt; 18634.000, 12740.000, 11239.000, 18596.000, 14411.000…\n$ dvp              &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ebit             &lt;dbl&gt; 22771.000, 3712.000, 3896.000, 1132.000, 3100.000, 85…\n$ netprofit        &lt;dbl&gt; 9601.000, 1788.000, 2714.000, 376.000, 117.000, 5664.…\n$ pstk             &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 252.2, 0.0, 0.0, 0…\n$ sale             &lt;dbl&gt; 55658.000, 22928.000, 14690.000, 4119.000, 67922.000,…\n$ epsexcon         &lt;dbl&gt; 1.5582, 0.1405, 5.4699, 0.1055, 0.0885, 1.8476, 2.256…\n$ nicon            &lt;dbl&gt; 7897.000, 1622.000, 2642.000, 326.000, 108.000, 5683.…\n$ conm             &lt;chr&gt; \"BHP GROUP LTD\", \"TELSTRA GROUP LIMITED\", \"CSL LTD\", …\n$ fic              &lt;chr&gt; \"AUS\", \"AUS\", \"AUS\", \"AUS\", \"AUS\", \"AUS\", \"AUS\", \"AUS…\n$ conml            &lt;chr&gt; \"BHP Group Ltd\", \"Telstra Group Limited\", \"CSL Ltd\", …\n$ ggroup           &lt;dbl&gt; 1510, 5010, 3520, 2030, 3010, 1510, 2550, 3510, 2030,…\n$ gind             &lt;dbl&gt; 151040, 501010, 352010, 203050, 301010, 151040, 25503…\n$ gsector          &lt;dbl&gt; 15, 50, 35, 20, 30, 15, 25, 35, 20, 55, 30, 55, 15, 5…\n$ gsubind          &lt;dbl&gt; 15104020, 50101020, 35201010, 20305020, 30101030, 151…\n$ sector           &lt;chr&gt; \"Materials\", \"Communication Services\", \"Health Care\",…\n$ indgroup         &lt;chr&gt; \"Materials\", \"Telecommunication Services\", \"Pharmaceu…\n$ industry         &lt;chr&gt; \"Metals & Mining\", \"Diversified Telecommunication Ser…\n$ subind           &lt;chr&gt; \"Diversified Metals & Mining\", \"Integrated Telecommun…\n$ debt             &lt;dbl&gt; 20718.000, 16968.000, 12183.000, 20186.000, 16722.000…\n$ invested_capital &lt;dbl&gt; 69838.00, 34320.00, 31584.00, 31864.00, 22292.00, 249…\n\n\nGiven this week’s topic, much of our analysis will focus on ebit. This variable is our profit measure, Earnings Before Interest and Taxes.\n\n\n\n\n\n\nNoteDefinition: EBIT\n\n\n\nIn accounting and finance, we refer to profit as the ‘bottom line’ because it is the money left over after the business has covered its expenses and which in turn can then be paid out to the business’ owners (i.e., shareholders). EBIT tells us how much profit a company makes from its core business activities before it pays interest (on debt) and taxes.\n\\[\nEBIT = \\text{Revenue} - \\text{Cost of Goods Sold} - \\text{Operating Expenses}\n\\]\nBecause EBIT can be used to compare companies without worrying about differences in tax strategies or capital structure, investors and analysts commonly use EBIT for valuation purposes (without getting into the details of discounted cash flow models, investors pay more for companies that they expect will produce greater profits in the future, and current profits are often a good, but not perfect, indicator of future profits).\n\n\nPutting EBIT as a measure of firm profits to one side, we will define and interpret the other financial variables in our data frame as they arise during the data wrangling process.",
    "crumbs": [
      "Getting Our First Insights from Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Wrangling for Business Analytics</span>"
    ]
  },
  {
    "objectID": "insights/data_manipulation.html#using-transformations-to-wrangle-data",
    "href": "insights/data_manipulation.html#using-transformations-to-wrangle-data",
    "title": "5  Data Wrangling for Business Analytics",
    "section": "5.4 Using transformations to wrangle data",
    "text": "5.4 Using transformations to wrangle data\nFor this week’s material, you will learn to use the primary dplyr verbs to manipulate our data frame in some basic, but useful ways. dpylr verbs operate on data frames in one of four ways: on rows, on columns, on groups, and on tables. We can use pipes (|&gt;) to combine multiple dplyr verbs, and so perform more complex data manipulations.\n\nOperating on rows with filter() and arrange()\nYou only need to skim the business section of any broadsheet newspaper to observe that the mining sector dominates the Australian economy.\n\n\n\nThe Australian Financial Review has a dedicated mining section\n\n\nAnd so, to begin our analysis of firm profits, it makes sense to start off by comparing the operating performance of mining companies and non-mining companies for our sample of the 200-largest ASX-listed firms. To do so, we will make use of two dplyr verbs that operate on rows: filter() and arrange().\n\n\n\n\n\n\nImportantWhat do filter() and arrange() ‘do’?\n\n\n\nfilter() allows you to keep rows based on the values of the columns, and so changes which rows are present without changing their order. In contrast, arrange() changes the order of rows without changing which are present. Neither function affects the columns in the data frame.\n\n\nWe can use filter() to identify and separately store the mining companies that appear in our data frame:\n\n# Keep only the 'Metals & Mining' firms from the ASX 200 dataset for 2024\nbig_mining_2024 &lt;- asx_200_2024 |&gt;\n  filter(industry == \"Metals & Mining\")\n\nWe use arrange() to list these mining companies in descending order so that those mining companies that have the largest profits appear first and those that have the the largest losses (i.e., negative profits) appear last:\n\n# Sort Metals & Mining firms in descending order of EBIT (highest profit first)\nbig_mining_2024 &lt;- big_mining_2024 |&gt;\n  arrange(desc(ebit))\n\nWe can now easily show the 10 mining companies from the ASX that produced the largest profits using a simple table:\n\n# Keep only the top 10 most profitable mining firms (by EBIT)\nt10_mining_profits &lt;- big_mining_2024 |&gt;\n  slice_head(n = 10)\n\n# Display the resulting data set\nt10_mining_profits\n\n# A tibble: 10 × 30\n   gvkey curcd fyear   fyr datadate     at  capx shareequity    dlc   dltt   dvp\n   &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1 0133… USD    2024     6 6/30/20… 1.02e5 9273       49120  2.08e3 1.86e4    NA\n 2 2124… USD    2024     6 6/30/20… 3.01e4 2834       19531  1.92e2 5.21e3    NA\n 3 2522… AUD    2024     6 6/30/20… 1.57e4  963.      11286. 1.91e2 5.31e2    NA\n 4 2596… AUD    2024     6 6/30/20… 1.31e4 1440        8791. 1.52e2 1.19e3    NA\n 5 2533… AUD    2024     6 6/30/20… 8.82e3  918.       4142. 1.27e2 1.89e3    NA\n 6 2713… USD    2024     6 6/30/20… 1.99e3  121.       1780. 1.70e0 1.52e0    NA\n 7 2869… AUD    2024     6 6/30/20… 4.31e3  810.       3244. 1.36e2 4.20e2    NA\n 8 2051… AUD    2024     6 6/30/20… 1.22e4 4133        3584  2.55e2 5.08e3    NA\n 9 2564… AUD    2024     6 6/30/20… 1.59e3  123.       1329. 9.08e0 1.39e0    NA\n10 2108… AUD    2024     6 6/30/20… 3.36e3  335.       1788. 1.71e1 9.11e2    NA\n# ℹ 19 more variables: ebit &lt;dbl&gt;, netprofit &lt;dbl&gt;, pstk &lt;dbl&gt;, sale &lt;dbl&gt;,\n#   epsexcon &lt;dbl&gt;, nicon &lt;dbl&gt;, conm &lt;chr&gt;, fic &lt;chr&gt;, conml &lt;chr&gt;,\n#   ggroup &lt;dbl&gt;, gind &lt;dbl&gt;, gsector &lt;dbl&gt;, gsubind &lt;dbl&gt;, sector &lt;chr&gt;,\n#   indgroup &lt;chr&gt;, industry &lt;chr&gt;, subind &lt;chr&gt;, debt &lt;dbl&gt;,\n#   invested_capital &lt;dbl&gt;\n\n\nThe above doesn’t quite work how we want it to because it cuts off the company’s EBIT and name (to view a wide data frame such as ours we need to open it in the dedicated viewer using View()).\nAs an alternative, Figure 5.1 concisely and effectively conveys which Australian mining firms have the largest profits (much of the code below will look familiar from last week’s module on using ggplot for data visualization). Before producing this plot, we first want to fix our color scheme so that across plots the same industry is always the same color (even though for the time being we are only looking at one industry-i.e., mining-and so only need one color at this stage, later we will look at other industries, and so need to map different industries to different colors).\n\n# Extract all unique industries and sort them alphabetically\nindustry_levels &lt;- asx_200_2024$industry |&gt;\n  unique() |&gt;\n  sort()\n\n# Assign a colour to each industry using the Set2 palette\nindustry_colors &lt;- setNames(\n  # Generate as many colours as there are industries\n  colorRampPalette(RColorBrewer::brewer.pal(8, \"Set2\"))(length(industry_levels)),\n  # Name each colour with the corresponding industry\n  industry_levels\n)\n\n\n# Reorder company names so bars appear in descending EBIT order\nt10_mining_profits &lt;- t10_mining_profits |&gt;\n  mutate(conml = reorder(conml, -ebit))\n\n# Create a bar chart showing EBIT by company\nt10_mining_profits |&gt;\n  ggplot(aes(x = conml, y = ebit, fill = industry)) +\n  # Draw bars with heights equal to EBIT values\n  geom_bar(stat = \"identity\") +\n  # Use the custom Set2-based industry palette\n  scale_fill_manual(values = industry_colors) +\n  # Add a y-axis label with units in millions\n  labs(\n    x = NULL,\n    y = \"EBIT (Million AUD)\",\n    title = \"ASX-Listed Mining Companies\",\n    subtitle = \"Top 10 by EBIT in 2024\"\n  ) +\n  # Format y-axis numbers with comma separators\n  scale_y_continuous(labels = scales::comma) +\n  # Apply a clean minimal theme with larger base text size\n  theme_minimal(base_size = 14) +\n  # Rotate x-axis labels for readability\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\nFigure 5.1: BHP’s profit dwarfs those of other large Australian companies.\n\n\n\n\n\n\n\n\n\n\n\nNoteDiscussion: BHP - Australia’s mining giant\n\n\n\nHeadquartered in Melbourne, BHP is one of the world’s largest diversified mining companies. Founded in 1885 as Broken Hill Proprietary Company Limited, its name reflects its origins in the Broken Hill mining region. It produces a wide range of commodities including iron ore, copper, coal, and petroleum. BHP is known for its scale, operational efficiency, and global footprint, competing closely with other mining giants like Rio Tinto and Vale. The company plays a crucial role in supplying raw materials for industries worldwide and is a key player in the global mining sector.\n\n\n\n\n10 min\n\nModify the code above to produce an equivalent plot but for the non-mining companies in our data set. To do so, take the following steps:\n\nUse filter() to create a new data frame that contains only the non-mining companies in our data set (!= means ‘does not’ equal in a conditional statement). Use arrange() to sort these companies in descending order of EBIT. Keep the first ten observations in a new data frame named t10_other_profits.\nUse ggplot() to create a bar chart that shows the EBIT of the ten non-mining companies with the largest profits.\n\n\n\nSolution\n\n\nStep 1:\n\n# Create a data frame of the top 10 most profitable non-mining companies\nt10_other_profits &lt;- asx_200_2024 |&gt;\n  # Exclude Metals & Mining companies\n  filter(industry != \"Metals & Mining\") |&gt;\n  # Sort by EBIT in descending order\n  arrange(desc(ebit)) |&gt;\n  # Keep only the top 10 rows\n  slice_head(n = 10)\n\nStep 2:\n\n# Reorder company names so bars appear in descending EBIT order\nt10_other_profits &lt;- t10_other_profits |&gt;\n  mutate(conml = reorder(conml, -ebit))\n\n# Create a bar chart of the ten most profitable non-mining companies\nt10_other_profits |&gt;\n  ggplot(aes(x = conml, y = ebit, fill = industry)) +\n  # Draw bars with heights equal to EBIT values\n  geom_bar(stat = \"identity\") +\n  # Use the Set2-based industry palette\n  scale_fill_manual(values = industry_colors) +\n  # Add labels in clear, title case\n  labs(\n    x = NULL,\n    y = \"EBIT (Million AUD)\",\n    title = \"ASX-Listed Non-Mining Companies\",\n    subtitle = \"Top 10 by EBIT in 2024\"\n  ) +\n  # Format y-axis numbers with comma separators\n  scale_y_continuous(labels = scales::comma) +\n  # Apply a clean minimal theme with larger text\n  theme_minimal(base_size = 14) +\n  # Rotate x-axis labels and style legend\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.position = c(0.98, 0.98),\n    legend.justification = c(\"right\", \"top\"),\n    legend.title = element_blank(),\n    legend.text = element_text(size = 10),\n    legend.background = element_rect(fill = scales::alpha(\"white\", 0.7), color = NA),\n    legend.key.size = grid::unit(0.4, \"cm\"),\n    legend.spacing.y = grid::unit(0.1, \"cm\")\n  )  \n\n\n\n\n\n\n\nFigure 5.2: CSL’s headquarters is across the road from FBE’s The Spot\n\n\n\n\n\n\n\nComparing Figure 5.2 to Figure 5.1 reveals that mining companies dominate the Australian economy: the profits of the mining companies with the largest profits are much larger than those of the non-mining companies with the largest profits. However, to get the complete picture in a single plot, let’s take a step back, look at our full sample again, and plot the ten firms across all industries that have the largest profits in 2024:\n\n# Select the top 10 companies by EBIT\nt10_all_profits &lt;- asx_200_2024 |&gt;\n  arrange(desc(ebit)) |&gt;\n  slice_head(n = 10)\n\n# Reorder company names so bars appear in descending EBIT order\nt10_all_profits &lt;- t10_all_profits |&gt;\n  mutate(conml = reorder(conml, -ebit))\n\n# Create a bar chart of the top 10 most profitable companies\nbar_plot &lt;- t10_all_profits |&gt;\n  ggplot(aes(x = conml, y = ebit, fill = industry)) +\n  # Draw bars with heights equal to EBIT values\n  geom_bar(stat = \"identity\") +\n  # Use clear, title-cased labels; no x-axis label\n  labs(\n    x = NULL,\n    y = \"EBIT (Million AUD)\",\n    title = \"Top 10 Most Profitable ASX-Listed Companies, 2024\",\n    subtitle = \"Ranked by EBIT\"\n  ) +\n  # Format y-axis numbers with comma separators\n  scale_y_continuous(labels = scales::comma) +\n  # Apply the provided industry palette\n  scale_fill_manual(values = industry_colors) +\n  # Use a clean minimal theme with slightly larger text\n  theme_minimal(base_size = 14) +\n  # Rotate x-axis labels and style legend\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.position = c(0.98, 0.98),\n    legend.justification = c(\"right\", \"top\"),\n    legend.title = element_blank(),\n    legend.text = element_text(size = 10),\n    legend.background = element_rect(fill = scales::alpha(\"white\", 0.7), color = NA),\n    legend.key.size = grid::unit(0.4, \"cm\"),\n    legend.spacing.y = grid::unit(0.1, \"cm\")\n  )\n\n# Display the plot\nbar_plot\n\n\n\n\n\n\n\nFigure 5.3: The top mining companies are staggeringly profitable\n\n\n\n\n\nFigure 5.3 shows that although non-mining companies outnumber mining companies in our top ten (two to eight), the combined profits of the latter are larger than the combined profits of the former. And so, at least on the basis of profits, mining companies really do dominate the Australian economy.\n\n\n10 min\n\nModify the code above to produce an equivalent plot but for the ten companies that produced the worst losses in 2024. To do so, take the following steps:\n\nUse arrange() to place in ascending order by profits all companies in our data frame. Keep the first ten observations in a new data frame named t10_all_losses.\nUse ggplot() to create a bar plot that shows the EBIT of the ten companies with the worst losses. Use fill = industry to color firms by their industry.\n\n\n\nSolution\n\n\nStep 1:\n\n# Select the 10 companies with the largest losses (lowest EBIT values)\nt10_all_losses &lt;- asx_200_2024 |&gt;\n  # Sort by EBIT in ascending order so loss-makers appear first\n  arrange(ebit) |&gt;\n  # Keep only the first 10 rows\n  slice_head(n = 10)\n\nStep 2:\n\n# Reorder company names so bars appear from largest to smallest loss\nt10_all_losses &lt;- t10_all_losses |&gt;\n  mutate(conml = reorder(conml, ebit))\n\n# Create a bar chart of the 10 companies with the largest losses\nt10_all_losses |&gt;\n  ggplot(aes(x = conml, y = ebit, fill = industry)) +\n  # Draw bars with heights equal to EBIT values\n  geom_bar(stat = \"identity\") +\n  # Add labels in clear, title case\n  labs(\n    x = NULL,\n    y = \"EBIT (Million AUD)\",\n    title = \"Top 10 ASX-Listed Companies by Losses\",\n    subtitle = \"Largest Loss-Makers in 2024\"\n  ) +\n  # Format y-axis numbers with comma separators\n  scale_y_continuous(labels = scales::comma) +\n  # Apply the provided industry palette\n  scale_fill_manual(values = industry_colors) +\n  # Use a clean minimal theme with larger base font\n  theme_minimal(base_size = 14) +\n  # Rotate x-axis labels and position legend bottom-right\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.position = c(0.98, 0.02),\n    legend.justification = c(\"right\", \"bottom\"),\n    legend.title = element_blank(),\n    legend.text = element_text(size = 10),\n    legend.background = element_rect(fill = scales::alpha(\"white\", 0.7), color = NA),\n    legend.key.size = grid::unit(0.4, \"cm\"),\n    legend.spacing.y = grid::unit(0.1, \"cm\")\n  )\n\n\n\n\n\n\n\nFigure 5.4: Plenty of mining companies also make bad losses\n\n\n\n\n\n\n\nInterestingly, seven of the ten companies that produced the worst losses in 2024 are mining companies. Taken together, Figure 5.3 and Figure 5.4 reveal a key feature of the Australian economy: Mining is a boom-or-bust business.\n\n\n\n\n\n\nNoteDiscussion: Mining and the ‘Wild West’\n\n\n\nMining has long been a cornerstone of the Australian economy, especially in the vast and remote regions of Western Australia. This frontier-style industry is marked by bold ventures, high risks, and the promise of enormous rewards — a dynamic that continues to shape the business landscape today.\nBanks and other small lenders typically lack both the risk appetite and the capital base needed to fund mining ventures. As a result, mining companies must turn to the stock market to raise the substantial funds required for land acquisition and leases, and plant, property, and heavy equipment — none of which come cheaply. Consequently, mining companies dominate the ASX, both in terms of listings (as we will show later) and in representing the firms that generate the largest profits and the largest losses.\nThis extreme variation in EBIT reflects the high operating risk inherent in mining — it is, quite literally, a boom-or-bust business. The volatility arises from several sources: the speculative nature of prospecting (it’s difficult to predict where and how much resource will be found), the cyclical fluctuations in global commodity prices, and the immense costs associated with exploration and extraction.\n\n\n\n\nOperating on columns with mutate() and select()\nWe need to keep in mind that profit is a dollar-dominated measure of operating performance, and so it may be the case that those firms that produce the largest profits or the largest losses are simply the largest firms in our sample (i.e., undertake the most economic activity and so by function of their size make or lose lots of money). This distinction matters because investors ultimately want to allocate their capital to companies that generate the best returns (i.e., the most profit per dollar of investment), which may not necessarily be the firms that produce the most profit in absolute terms.\nLet’s observe how returns vary within the Australian economy across industries, and within industries across firms. For this analysis, we must first calculate each company’s return on investment in our sample and then focus in on this measure of operating performance. To do so, we will make use of two dplyr verbs that operate on columns: mutate() and select().\n\n\n\n\n\n\nImportantWhat do mutate() and select() ‘do’?\n\n\n\nmutate() allows you to add new columns that are created from values in existing columns, and so ‘widens’ your data frame. In contrast, select() allows you to isolate specific columns and drop others, and so ‘narrows’ your data frame. Neither function affects the rows in the data frame.\n\n\nWe will use mutate() to create our measure of firm profitability, Return on Invested Capital (ROIC):\n\n# Calculate Return on Invested Capital (ROIC) for firms with valid invested capital\nreturns &lt;- asx_200_2024 |&gt;\n  # Exclude firms with zero or negative invested capital\n  filter(invested_capital &gt; 0) |&gt;\n  # Create new ROIC variable: EBIT divided by invested capital\n  mutate(roic = ebit / invested_capital)\n\n\n\n\n\n\n\nNoteDefinition: Profit vs Profitability - Why ROIC?\n\n\n\nROIC captures how effectively a company is using the funds invested by its shareholders and debt holders to generate profits. We calculate ROIC as follows, where Invested Capital is defined as debt plus equity as per the book values in the firm’s balance sheet: \\[\nROIC = \\frac{EBIT}{\\text{Invested Capital}}\n\\] Consider Company A, which receives $10 in investment and produces $100 in profit. Consider Company B, which receives $100 in investment and produces $101 in profit. While company B produces a larger profit, Company A is a much better investment because it generates much more profit per dollar of investment - i.e., it provides investors with a much higher return on their capital. ROIC makes this clear: Company A’s ROIC is 10, while Company B’s ROIC is 1.01.\n\n\nYou should notice that in the above chunk of code before creating our ROIC variable and assigning this to our returns data frame we drop observations from our sample where invested capital is missing or equal to zero. We do so to avoid creating a variable where we may for some observations need to divide by zero (which of course cannot be done).\nSo, which ten Australian companies produced the best operating returns in 2024?\n\n# Return the 10 companies with the highest ROIC\nreturns |&gt;\n  # Sort in descending order of ROIC\n  arrange(desc(roic)) |&gt;\n  # Keep the top 10 rows\n  slice_head(n = 10)\n\n# A tibble: 10 × 31\n   gvkey  curcd fyear   fyr datadate       at     capx shareequity    dlc   dltt\n   &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 242679 AUD    2024     6 6/30/2024    866.    0.607        74.9 3.99e0 1.66e1\n 2 248015 AUD    2024     6 6/30/2024   2655     4.2        1579.  8.8 e0 2.72e2\n 3 212427 USD    2024     6 6/30/2024  30060  2834         19531   1.92e2 5.21e3\n 4 241419 AUD    2024     9 9/30/2024    757.    6.32        379.  7.10e0 5.26e1\n 5 013312 USD    2024     6 6/30/2024 102362  9273         49120   2.08e3 1.86e4\n 6 289353 AUD    2024     6 6/30/2024    624.   65.6         309.  6.03e1 2.38e1\n 7 220244 AUD    2024     6 6/30/2024  20564  2761           294   6   e2 5.99e3\n 8 333913 AUD    2024     6 6/30/2024    338.   40.7         160.  1.88e1 5.09e1\n 9 318878 AUD    2024     6 6/30/2024    533.   38.5          80.3 5.84e1 3.01e2\n10 364417 AUD    2024     6 6/30/2024    362.    3.92         82.9 3.73e1 6.91e1\n# ℹ 21 more variables: dvp &lt;dbl&gt;, ebit &lt;dbl&gt;, netprofit &lt;dbl&gt;, pstk &lt;dbl&gt;,\n#   sale &lt;dbl&gt;, epsexcon &lt;dbl&gt;, nicon &lt;dbl&gt;, conm &lt;chr&gt;, fic &lt;chr&gt;,\n#   conml &lt;chr&gt;, ggroup &lt;dbl&gt;, gind &lt;dbl&gt;, gsector &lt;dbl&gt;, gsubind &lt;dbl&gt;,\n#   sector &lt;chr&gt;, indgroup &lt;chr&gt;, industry &lt;chr&gt;, subind &lt;chr&gt;, debt &lt;dbl&gt;,\n#   invested_capital &lt;dbl&gt;, roic &lt;dbl&gt;\n\n\nAs we discussed briefly already, tables such as the one above are too wide for our purposes. mutate() adds columns to the right-hand side of your data frame, where information is often cutoff from view in dynamic documents such as this one. Given that our focus is on identifying the companies in our top ten and these companies’ returns, we can use select() to ‘narrow’ our data frame, and so produce a table that only reports the columns that we currently interested in.\n\n# Return the 10 companies with the highest ROIC\nreturns |&gt;\n  # Sort in descending order of ROIC\n  arrange(desc(roic)) |&gt;\n  # Keep the top 10 rows\n  slice_head(n = 10) |&gt;\n  # Select only columns relevant to operating returns in 2024\n  select(fyear, conml, roic, ebit, invested_capital)\n\n# A tibble: 10 × 5\n   fyear conml                 roic    ebit invested_capital\n   &lt;dbl&gt; &lt;chr&gt;                &lt;dbl&gt;   &lt;dbl&gt;            &lt;dbl&gt;\n 1  2024 Data3 Ltd            0.558    53.2             95.5\n 2  2024 REA Group Ltd        0.382   711.            1860  \n 3  2024 Fortescue Ltd        0.342  8520            24931  \n 4  2024 Technology One Ltd   0.334   147.             439. \n 5  2024 BHP Group Ltd        0.326 22771            69838  \n 6  2024 Capricorn Metals Ltd 0.322   127.             393. \n 7  2024 Qantas Airways Ltd   0.319  2198             6885  \n 8  2024 Mader Group Ltd      0.313    72.0            230. \n 9  2024 Lovisa Holdings Ltd  0.297   131.             439. \n10  2024 Symal Group Limited  0.292    55.3            189. \n\n\nThis table is much better as it focuses on reporting the information that is most relevant to our current research question. You may notice two interesting takeaways from this table.\n\nWhile Data3’s profit is relatively small as a dollar amount, it generates this profit using a very small amount of invested capital such that the company produces the best return for all ASX-listed firms in our sample.\nBHP’s profit is very large as a dollar amount, but it generates this profit using a very large amount of invested capital. As such, BHP produces a strong return but not the strongest.\n\nComparing these two companies makes clear the distinction between measures of operating performance that capture profit vs profitability.\n\n\n5 min\n\nQuestion:\nWhat features of Data#3’s business model and the broader economics of the IT and cloud services industry help explain its high ROIC?\nIn your answer, consider:\n\nHow much capital (land, plant, equipment) a company like Data#3 needs to operate\nThe role of intangible assets, such as software, service contracts, and technical know-how\nHow cloud-based business models affect scalability and margins\nWhy return on capital might be high even if margins are not extreme\n\nWrite a few sentences explaining how these factors contribute to Data#3’s strong performance.\n\nSolution\n\n\nData#3’s high ROIC reflects the economics of the IT and cloud services industry. Unlike mining or manufacturing firms, Data#3 does not need to invest heavily in physical assets—its operations rely on skilled labor, software systems, and cloud infrastructure managed by third parties. This means low invested capital.\nAt the same time, cloud-based services and IT solutions can scale efficiently, allowing the firm to serve many clients without a matching increase in costs. As a result, Data#3 generates high earnings from a relatively small capital base, which drives up ROIC even if profit margins are moderate.\n\n\n\n\n\n10 min\n\nModify the code above to produce an equivalent table but for the ten companies that achieved the worst returns in 2024. To do so, take the following steps:\n\nUse arrange() to place in ascending order by returns all companies in our data frame. Use head() to keep the first ten observations in a new data frame named b10_returns.\nUse select() to extract and store from this new, shorter data frame the following columns: fyear, conml, roic, ebit, and invested_capital.\n\n\n\nSolution\n\n\nStep 1:\n\n# Select the 10 companies with the lowest ROIC values\nb10_returns &lt;- returns |&gt;\n  # Sort in ascending order of ROIC (worst performers first)\n  arrange(roic) |&gt;\n  # Keep only the first 10 rows\n  slice_head(n = 10)\n\nStep 2:\n\n# Show the 10 companies with the lowest ROIC, keeping only relevant columns\nb10_returns |&gt;\n  select(fyear, conml, roic, ebit, invested_capital)\n\n# A tibble: 10 × 5\n   fyear conml                               roic   ebit invested_capital\n   &lt;dbl&gt; &lt;chr&gt;                              &lt;dbl&gt;  &lt;dbl&gt;            &lt;dbl&gt;\n 1  2024 Strandline Resources Ltd         -0.705  -193.              273.\n 2  2024 The Star Entertainment Group Ltd -0.515  -584.             1135.\n 3  2024 St Barbara Ltd                   -0.224   -80.0             357.\n 4  2024 IGO Limited                      -0.182  -593.             3258 \n 5  2024 Amplitude Energy Limited         -0.166  -112.              673.\n 6  2024 Beach Energy Ltd                 -0.145  -594              4107.\n 7  2024 Sayona Mining Ltd                -0.139  -115.              826.\n 8  2024 Pantoro Ltd                      -0.130   -60.9             469.\n 9  2024 Western Areas Ltd                -0.121  -394.             3258 \n10  2024 Mesoblast Ltd                    -0.0967  -58.0             599.\n\n\n\n\n\n\n\n\n\n\nNoteDiscussion: Star Entertainment\n\n\n\nOne name from this list-Star Entertainment Group-should immediately jump out if you regularly read the Australian Financial Review. Much has been written in the Australian business press about the poor operating performance of Star Entertainment, a gambling and entertainment company that operates several high-profile casinos.\nThe Star Entertainment Group is one of Australia’s largest casino operators, running major venues in Sydney, Brisbane, and the Gold Coast. For years, it relied heavily on high-roller gambling and large-scale property developments to drive profits. The company’s fortunes have deteriorated sharply over the past few years, initially due to the pandemic’s hit to tourism, then more severely from a series of damaging regulatory inquiries into money laundering and governance failures. These have resulted in costly compliance measures, reduced high-roller business, and reputational damage.\nIn 2025, Star Entertainment remains in a precarious position — it has bought some time through emergency funding, but without new asset sales, debt relief, or a regulatory settlement, the risk of collapse is still on the table.\n\n\nWhile short, narrow tables such as the above are useful for identify and describing limited features of a small subset of a longer, wider data frame, often we want to more fully describe and summarize these longer, wider data frames, and so we are better off relying on visualizations and/or descriptive statistics such as measures of central tendency and dispersion.\nFor example, what return did the typical large Australian public company produce in 2024? How commonly did firms produce returns in excess of 40%? And returns worse than -20%? To illustrate the benefits of visualization for generating insight, we can use a histogram to examine the distribution of firm profitability for all firms in our sample (i.e., the largest 200 companies on the ASX in 2024) and so answer these questions:\n\n# Create a histogram of ROIC across firms\nhistogram &lt;- returns |&gt;\n  ggplot(aes(x = roic)) +\n  # Draw histogram with 30 bins, semi-transparent blue fill, and black border\n  geom_histogram(fill = \"skyblue\", color = \"black\", alpha = 0.7, bins = 30) +\n  # Add clear labels and title case\n  labs(\n    x = \"Return on Invested Capital\",\n    y = \"Frequency\",\n    title = \"Distribution of Operating Returns\",\n    subtitle = \"Large Australian Public Companies in 2024\"\n  ) +\n  # Apply a clean minimal theme\n  theme_minimal() +\n  # Limit y-axis range to 0–50 without dropping data\n  coord_cartesian(ylim = c(0, 50))\n\n# Display the histogram\nhistogram\n\n\n\n\n\n\n\nFigure 5.5: Is this distribution one you would expect in an open market economy such Australia’s?\n\n\n\n\n\nFigure 5.5 reveals several important insights. First, the typical large Australian public company in 2024 achieved returns for investors of approximately 10%. Second, the vast majority of large Australian public companies in 2024 achieved returns of between 0-15%. Third, very few (&lt; 4) large Australian public companies in 2024 achieved returns worse than -20%, and only one large Australian companies achieved returns better than 40%.\n\n\nOperating on groups with group_by() and summarise()\nEarlier, we said that mining companies are subject to greater operating risk than firms in other industries (mining is highly speculative and prospectors cannot perfectly predict sites that contain resources; extracting those resources can be very challenging and costly, and commodity prices are very volatile). A key insight in finance is that investors in order to take on greater risk expect to be compensated with higher returns. And so, we should observe that mining companies returns, while more dispersed, are on average higher than non-mining companies returns, which are less dispersed.\nTo establish whether this is the case, we need to examine how returns vary across industries for our sample of large Australian public companies. To do so, we will make use of two dplyr functions that operate on groups within our data frame: group_by() and summarize().\n\n\n\n\n\n\nImportantWhat do group_by() and summarize() ‘do’?\n\n\n\ngroup_by() divides our data frame into groups on which we can then perform subsequent operations. One such operation is summarize(), which reduces the data frame to have a single row for each group, where each row contains summary statistics for that group. This is useful for aggregating data and calculating statistics such as means, medians, or counts. Another useful summary is n(), which returns the number of rows in each group. These functions affect the rows and columns in a data frame - e.g., group_by() then summarize() ‘shortens’ and ‘narrows’.\n\n\nBefore examining how returns vary across industries, we should first get a sense of the composition of the largest 200 Australian public companies in 2024. Which industry is most represented in our sample? Which is least represented? Did a small number of industries make up most of the Australian stock market in 2024? To answer these questions, we use group_by() to group our data frame by industry, and then summarize to count the number of observations associated with each industry in this group:\n\nreturns |&gt;\n  # Group firms by industry\n  group_by(industry) |&gt;\n  # Count the number of observations in each industry\n  summarise(obs = n()) |&gt;\n  # Sort industries from most to fewest observations\n  arrange(desc(obs))\n\n# A tibble: 45 × 2\n   industry                           obs\n   &lt;chr&gt;                            &lt;int&gt;\n 1 Metals & Mining                     42\n 2 Specialty Retail                    14\n 3 Construction & Engineering          11\n 4 Hotels, Restaurants & Leisure       11\n 5 Oil, Gas & Consumable Fuels          9\n 6 Commercial Services & Supplies       7\n 7 Food Products                        7\n 8 Health Care Providers & Services     6\n 9 Trading Companies & Distributors     5\n10 Broadline Retail (New Name)          4\n# ℹ 35 more rows\n\n\nThe above table shows that while we observe 45 distinct industries in our sample, seven industries account for more than half of the 200 largest Australian public companies (and mining alone accounts for almost one quarter of these companies). Given the importance of these seven ‘big’ industries to the Australian economy-Metals & Mining; Specialty Retail; Construction & Engineering; Hotels, Restaurants, & Leisure; Oil, Gas, & Consumable Fuels; Commercial Services & Supplies; and, Food Products-let’s focus on them for the remainder of our industry-level analyses.\nWe use filter() to isolate observations from this sub-set of industries, and then use group_by() to group our data frame by industry. After this, we then use summarize() to calculate each industry’s average returns and standard deviation of returns for our sub sample of firms drawn from the 200 largest Australian public companies in 2024:\n\n# Define the set of 'big' industries to focus on\nbig_industries &lt;- c(\n  \"Metals & Mining\",\n  \"Specialty Retail\",\n  \"Construction & Engineering\",\n  \"Hotels, Restaurants & Leisure\",\n  \"Oil, Gas & Consumable Fuels\",\n  \"Commercial Services & Supplies\",\n  \"Food Products\"\n)\n\n# Calculate mean and standard deviation of ROIC for the big industries\nreturns |&gt;\n  # Keep only rows from the selected industries\n  filter(industry %in% big_industries) |&gt;\n  # Group by industry\n  group_by(industry) |&gt;\n  # Compute average and standard deviation of ROIC\n  summarise(\n    ave_roic = mean(roic, na.rm = TRUE),\n    sd_roic = sd(roic, na.rm = TRUE)\n  ) |&gt;\n  # Sort from highest to lowest average ROIC\n  arrange(desc(ave_roic))\n\n# A tibble: 7 × 3\n  industry                       ave_roic sd_roic\n  &lt;chr&gt;                             &lt;dbl&gt;   &lt;dbl&gt;\n1 Construction & Engineering      0.144    0.0597\n2 Specialty Retail                0.140    0.0810\n3 Commercial Services & Supplies  0.133    0.0950\n4 Food Products                   0.0714   0.0413\n5 Metals & Mining                 0.0357   0.177 \n6 Hotels, Restaurants & Leisure   0.0349   0.201 \n7 Oil, Gas & Consumable Fuels    -0.00279  0.127 \n\n\nThis table is useful, but we can use a box plot to provide a visualization that more richly describes how returns are distributed within industries, and how these distributions differ across industries.\n\n\n\n\n\n\nTipWhat is a box plot?\n\n\n\n\n\nA box plot is a statistical chart that summarizes the distribution of a numeric variable using five key values: the minimum, first quartile (Q1), median, third quartile (Q3), and maximum. The “box” shows the middle 50% of the data, with a line inside marking the median. The “whiskers” extend to the smallest and largest values within a set range, and points outside are plotted individually as outliers. Box plots are useful for quickly comparing distributions, spotting skewness, and identifying extreme values across different groups.\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.6: Mining has the most extreme outliers in either tail\n\n\n\n\n\nHmm, this plot presents results that are only partially consistent with the story we have been telling ourselves so far. Returns in mining are very highly dispersed compared to other industries, but expected returns in mining are lower than those in many other industries. And so, it seems that while mining companies do face heightened operating risk, investors in mining companies do not appear to achieve returns commensurate with this level of risk.\nIn fact, if anything, the plot as a whole suggests an inverted relationship between expected returns and risk-i.e., those industries that produce the best returns on average also produce returns that are less dispersed. One reason for why this is the case is that in sectors like mining, high volatility in ROIC often stems from cyclical demand, volatile input prices, and the heavy fixed costs associated with capital-intensive operations. These factors amplify losses in bad years but place natural limits on upside in good years, pulling down the long-run average ROIC.\n\n\n\n\n\n\nNoteDiscussion: Construction & Engineering – Profits in a High-Pressure Industry\n\n\n\nThe Construction & Engineering sector in Australia is currently enjoying strong operating returns, reflecting a combination of sustained infrastructure investment, private development activity, and the ability of firms to maintain healthy margins. Major public transport upgrades (including the soon-to-open Parkville train station), renewable energy projects, and resource-related construction have kept order books full, while the limited number of large-scale contractors capable of delivering such projects has given established players considerable pricing power.\nSeveral structural factors help keep margins elevated. A limited supply of skilled labour from engineers to specialist trades — allows firms to bid at higher prices, confident that competitors face similar cost pressures. The complexity and scale of many projects also create high barriers to entry, reducing competition. Additionally, the prevalence of long-term government contracts often locks in profit margins and shifts some risk to the client, particularly in cost-plus or staged-delivery arrangements.\nThat said, these strong returns are not without risk. Supply chain disruptions, industrial relations disputes, and changes in government spending priorities can still impact profitability. Nonetheless, for now, the sector’s combination of robust demand and constrained supply continues to underpin its strong operating performance.\n\n\nBefore we conclude that our plot provides suﬀicient evidence to refute the idea of the risk- return trade-off that lies at the heart of modern financial theory and practice, we need to think careful about limitations in our data and analyses. First, our dataframe only contains observations from 2024, and so perhaps our analysis is simply noisy (for example, commodity prices in 2024 may have been unexpectedly low). One way to address this issue is to broaden our sample to include observations from earlier years, and average out noise that affects specific industries in certain years. Second, our data frame only contains observations from the largest 200 Australian public companies, and so our results do not reflect the performance of small mining companies, which may generate very high returns given their lower levels of investment. We could address this issue in our analysis by broadening our sample to include observations from all ASX-listed firms in a given year.",
    "crumbs": [
      "Getting Our First Insights from Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Wrangling for Business Analytics</span>"
    ]
  },
  {
    "objectID": "insights/data_manipulation.html#bringing-it-all-together-summarizing-our-findings",
    "href": "insights/data_manipulation.html#bringing-it-all-together-summarizing-our-findings",
    "title": "5  Data Wrangling for Business Analytics",
    "section": "5.5 Bringing it all together: Summarizing our findings",
    "text": "5.5 Bringing it all together: Summarizing our findings\nWe have applied the core dpylr verbs-filter(), arrange(), mutate(), select(), group_by(), and summarize()-to wrangle financial statements data for the 200 largest Australian public companies in 2024. In doing so, we have generated several insights that shed light on the level and composition of firm profits and investor returns in the Australian economy.\nFirst, we showed that in 2024, the two most profitable mining companies together earned more than the eight most profitable non-mining companies combined. Second, in terms of frequency, we showed that a broad cross-section of industries make up the most profitable companies in our sample, and that we do not see a single industry dominate the top ten most profitable companies in 2024.\n\n\n\n\n\n\n\n\n\nThird, we showed that the typical large Australian public company in 2024 achieved returns for investors of approximately 10%, and that the vast majority of large Australian public companies in 2024 achieved returns of between 0-15%. In our sample, the companies with the highest profitability produced returns of approximately 40%, while the companies with the worst profitability produced returns of approximately -20%.\n\n\n\n\n\n\n\n\n\nFourth, we showed that while mining companies are subject to greater operating risk than firms in other industries, they do not appear to produce returns commensurate with this level of risk. In fact, if anything, the plot as a whole suggests an inverted relationship between expected returns and risk-i.e., those industries such as Construction & Engineering that produce the best returns on average also produce returns that are less dispersed.\n\n\n\n\n\n\n\n\n\n\n\n\n3 min\n\nUse the patchwork package to combine Figure 5.3, Figure 5.5, and Figure 5.6 into a single figure that shows all three plots. Do so such that the bar plot is on top, the histogram is on the bottom left, and the box plot is on the right.\n\n\nSolution\n\n\n\n# Combine the plots into patchwork\n(bar_plot) / #bar_plot on top\n  (histogram | box_plot) # histogram on bottom left, box plot on right",
    "crumbs": [
      "Getting Our First Insights from Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Wrangling for Business Analytics</span>"
    ]
  },
  {
    "objectID": "insights/data_manipulation.html#conclusion",
    "href": "insights/data_manipulation.html#conclusion",
    "title": "5  Data Wrangling for Business Analytics",
    "section": "5.6 Conclusion",
    "text": "5.6 Conclusion\nIn this chapter, we have explored how to use the dplyr package to manipulate financial data in R. We have learned how to filter, arrange, mutate, select, group, and summarize data to extract meaningful insights from financial statements. By applying these techniques to the 200 largest Australian public companies in 2024, we have gained a deeper understanding of the level and composition of firm profits and investor returns in the Australian economy. We have also built on last chapter’s material and seen how to visualize these insights using histograms and box plots, which can help us better understand the distribution of returns across industries and firms.\nIn the next chapter, we will we will explore how to use tidyr and dplyr to reshape and combine financial data in R. We will learn how to pivot data between wide and long formats, and how to join data frames using keys to integrate information from multiple sources. By applying these techniques to the 200 largest Australian public companies in 2024, we will merge financial statements with industry classifications and market performance data, creating richer datasets for analysis. Building on the previous chapter’s work, we will also see how these reshaped datasets can serve as the foundation for deeper comparisons across industries and firms, enabling more flexible and targeted visualizations.",
    "crumbs": [
      "Getting Our First Insights from Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Wrangling for Business Analytics</span>"
    ]
  },
  {
    "objectID": "insights/data_tidy.html",
    "href": "insights/data_tidy.html",
    "title": "6  Shaping and Combining Data for Business Analytics",
    "section": "",
    "text": "6.1 The Business Challenge",
    "crumbs": [
      "Getting Our First Insights from Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Shaping and Combining Data for Business Analytics</span>"
    ]
  },
  {
    "objectID": "insights/data_tidy.html#the-business-challenge",
    "href": "insights/data_tidy.html#the-business-challenge",
    "title": "6  Shaping and Combining Data for Business Analytics",
    "section": "",
    "text": "The Topic: How Well Do Earnings Explain What Investors Pay for Firms?\nWhen investors buy shares, they are not just purchasing a slice of today’s profits — they are buying into expectations of future earnings. A company’s reported earnings per share (EPS) and the multiple of those earnings that investors are willing to pay, the price-to-earnings (P/E) ratio, are two of the most widely used measures in equity markets. Together, they capture the link between operating performance and market valuation.\nYet these measures are rarely straightforward to analyze. Financial data is often recorded in messy, inconsistent formats, and the information we need is usually spread across multiple data sets. To understand how investors value Australian firms, we need to tidy and combine stock price data, earnings data, and industry classifications into a single, coherent data set.\nThe business questions we focus on are: How closely do earnings per share align with stock prices? How do P/E ratios differ across industries? What patterns emerge when we track these measures over time?\nBy reshaping messy data sets with pivots and merging them with joins, we can create a tidy, integrated data set that allows us to explore how investors interpret earnings and how valuations vary across the Australian economy.",
    "crumbs": [
      "Getting Our First Insights from Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Shaping and Combining Data for Business Analytics</span>"
    ]
  },
  {
    "objectID": "insights/data_tidy.html#the-data-stock-prices-and-market-performance-from-yahoo-finance",
    "href": "insights/data_tidy.html#the-data-stock-prices-and-market-performance-from-yahoo-finance",
    "title": "6  Shaping and Combining Data for Business Analytics",
    "section": "The Data: Stock Prices and Market Performance from Yahoo Finance",
    "text": "The Data: Stock Prices and Market Performance from Yahoo Finance\nTo answer the questions in this chapter, we draw on firm-year ASX stock price data from Yahoo Finance. We will also examine the ASX financial statements data set that we explored in last week’s lecture and tutorial. Unlike this data set, the ASX stock price data set that we introduce this week captures market-based measures of firm performance over multiple years (i.e., not just 2024).\nStock prices in our data set are recorded as the closing price at the end of each firm’s fiscal year. This allows us to align market performance with the accounting data reported in annual financial statements. Our data set contains the following variables:\n\nCompany name (conm) – the firm’s name as listed on the ASX\n\nFirm identifier (gvkey) – a unique numeric code that tracks companies over time\n\nFiscal year (fyear) – the accounting year associated with the stock price and earnings\n\nIn addition to raw prices, our data set also contains two derived financial measures that are widely applied by analysts and investors:\n\nStock price (price) – the firm’s share price at fiscal year-end, measured in Australian dollars\n\nEarnings per share (eps) – net earnings available to common shareholders divided by the number of shares outstanding:\n\\[\nEPS = \\frac{\\text{Earnings}}{\\text{Shares Outstanding}}\n\\]\n\nPrice-to-earnings ratio (pe) – the ratio of the firm’s stock price to its earnings per share:\n\\[\nP/E = \\frac{\\text{Price}}{EPS}\n\\]\n\nThese variables allow us to link operating performance with market performance, and to investigate the relationship between what firms earn and what investors are willing to pay. By working with these data, we can replicate the kinds of comparisons analysts use to value companies and industries.\n\nThe Method: Using Pivots and Joins to Reshape and Combine Data\n\n“Data that is loved tends to survive.” — Kurt Bollacker (computer scientist and digital archivist)\n\nWhile tidy data makes analysis straightforward, most real-world data arrives messy, spread across multiple files, or structured in ways that do not align with analytical needs. To work effectively, we need tools that can both reshape individual datasets and combine information across multiple sources.\nIn R, the tidyr package provides functions for reshaping data:\n\npivot_longer() to lengthen “short and wide” datasets by stacking values into a single column,\n\npivot_wider() to shorten “long and narrow” datasets by spreading values into multiple columns.\n\nThese pivots allow us to convert messy structures into tidy data, where variables are columns, observations are rows, and values are cells.\nThe dplyr package complements this by providing join functions that merge datasets:\n\nMutating joins (left_join(), right_join(), inner_join(), full_join()) add variables from one table to another,\n\nFiltering joins (semi_join(), anti_join()) filter observations based on whether they appear in another table.\n\nAll joins rely on keys — primary keys that uniquely identify observations in one dataset, and foreign keys that reference those identifiers in another. Understanding and using keys correctly is critical for combining information without introducing errors or duplicates.\nTogether, pivots and joins form the backbone of data wrangling workflows. They allow us to take messy, fragmented business data — such as financial statements, stock prices, and industry classifications — and transform it into a single tidy data set that supports reliable analysis, clear visualization, and sound decision-making.\n\n\n\n\n\n\nNoteBeyond R\n\n\n\nPivoting and joining are not unique to R. The same concepts appear in other tools:\n- SQL provides PIVOT/UNPIVOT clauses and join operations,\n- Excel offers PivotTables and lookup functions,\n- Python (pandas) implements .pivot(), .melt(), and .merge().\nMastering these techniques in R builds transferable skills that apply across the full range of data analysis environments.",
    "crumbs": [
      "Getting Our First Insights from Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Shaping and Combining Data for Business Analytics</span>"
    ]
  },
  {
    "objectID": "insights/data_tidy.html#the-game-plan-what-were-aiming-for",
    "href": "insights/data_tidy.html#the-game-plan-what-were-aiming-for",
    "title": "6  Shaping and Combining Data for Business Analytics",
    "section": "6.2 The Game Plan: What we’re aiming for",
    "text": "6.2 The Game Plan: What we’re aiming for\nRecall our workflow:\n\nDefine the Business Question(s) & Plan Outputs\n\nAcquire and Prepare Data\n\nExplore and Visualize Patterns\n\nAnalyze and Interpret Findings\n\nCommunicate Insights and Recommendations\n\nSketch of the plan:\n\nDefine the Business Questions: How do earnings explain stock prices? How do P/E ratios vary across industries and over time?\n\nLoad stock price, earnings, and sub-industry lookup data from Yahoo Finance\n\nWrangle data by reshaping with pivot_longer() and pivot_wider() to ensure tidy structure\n\nCombine data frames using joins (left_join(), inner_join(), anti_join()) to link financials, stock prices, and industry codes\n\nDiagnose missing values (explicit and implicit) and assess data coverage\n\nVisualize P/E ratios and price–earnings patterns across firms, years, and industries (we will tackle in the tutorial)\nInterpret what these patterns reveal about firm performance and investor expectations (we will tackle in the tutorial)\n\n\nLet’s get started!",
    "crumbs": [
      "Getting Our First Insights from Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Shaping and Combining Data for Business Analytics</span>"
    ]
  },
  {
    "objectID": "insights/data_tidy.html#getting-set-up-and-loading-the-data",
    "href": "insights/data_tidy.html#getting-set-up-and-loading-the-data",
    "title": "6  Shaping and Combining Data for Business Analytics",
    "section": "6.3 Getting Set Up and Loading the Data",
    "text": "6.3 Getting Set Up and Loading the Data\n\nR packages for today\nMuch of this week’s material will involve our using the tidyr and dplyr packages, which are core parts of the tidyverse. These two packages work seamlessly together, as well as with others such as ggplot2 and stringr, making them essential tools for data wrangling and visualization.\nAnd so to begin, we need to load these packages and others that we will use for this chapter.\n\n# Load the tidyverse for data manipulation and visualization\nlibrary(tidyverse)\n\n# Load scales for axis formatting and transformations\nlibrary(scales)\n\n# Load ggokabeito for a colour-blind-friendly palette (available if you choose to use it)\nlibrary(ggokabeito)\n\n# Load ggthemes for additional polished ggplot themes\nlibrary(ggthemes)\n\n# Load patchwork to combine multiple ggplots into one figure\nlibrary(patchwork)\n\n# Load stringr for consistent string manipulation helpers\nlibrary(stringr)\n\n\n\n\n\n\n\nTipWhat is scales?\n\n\n\n\n\nYou may notice that we continue to use many of the visualization tools introduced earlier in the course. One additional package that appears this week is scales. This package provides a consistent set of functions for formatting numbers, dates, and other values in plots. For example, it can transform raw dollar amounts into currency labels, percentages into readable strings, or continuous variables into log-scaled axes.\nIn this week’s lecture and tutorial, we will use scales to format axes and labels in our plots, ensuring that visualizations are clear, professional, and easy to interpret.\nSome examples include:\n- label_percent() – convert proportions like 0.25 into \"25%\"\n- label_dollar() – format numbers like 1000000 into \"$1,000,000\"\n- label_comma() – add thousands separators for readability (e.g., 10000 → \"10,000\")\n- label_date() – format dates consistently for time series plots\nThese functions help ensure our plots communicate results in a way that is accessible and professional.\n\n\n\n\n\n6.3.1 Loading the financial statements and price data\nWe now import the data sets that we will use in this week’s lecture. The first is the ASX financial statements data set we examined last week, which should be familiar to you by now:\n\n# Read in the 2024 ASX 200 data set\nasx_200_2024 &lt;- \n    read_csv(\"data/asx_200_2024.csv\") |&gt;\n    # Code our new variable fyear as a character type for subsequent joins\n    mutate(fyear = as.character(fyear)) \n\nWe also import a ‘messy’ snapshot of this data set, which we will tidy as part of this module:\n\n# Read in the wide version of the 2024 ASX 200 data set\nfinancials_messy &lt;- \n    read_csv(\"data/financials_messy.csv\")\n\nWe next import the ASX price data in two forms, a ‘wide’ snapshot and a ‘messy’ snapshot, both of which we will tidy as part of this module:\n\n# Read in the wide version of the ASX price data\nprices_wide &lt;- \n    read_csv(\"data/prices_wide.csv\")\n\n\n# Read in the messy version of the ASX price data\nprices_messy &lt;- \n    read_csv(\"data/prices_messy.csv\")\n\nFinally, we import a look-up table that links industry codes to industry names. We will use this identify each firm’s industry in our sample:\n\n# Read in the GICS industry look-up file\ngics_industry &lt;- read_csv(\"data/GICS_industry.csv\")",
    "crumbs": [
      "Getting Our First Insights from Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Shaping and Combining Data for Business Analytics</span>"
    ]
  },
  {
    "objectID": "insights/data_tidy.html#using-pivots-to-tidy-messy-data-frames",
    "href": "insights/data_tidy.html#using-pivots-to-tidy-messy-data-frames",
    "title": "6  Shaping and Combining Data for Business Analytics",
    "section": "6.4 Using pivots to tidy messy data frames",
    "text": "6.4 Using pivots to tidy messy data frames\n\n6.4.1 Tidy Data\n\n\n\n\n\n\nImportantWhat is Tidy Data?\n\n\n\nA tidy data set has the following properties:\n\nEach variable is a column; each column is a variable.\n\nEach observation is a row; each row is an observation.\n\nEach value is a cell; each cell is a single value.\n\n\n\n\nWe can see that our ASX financial statements data set, the one we examined in the last module, is tidy. Here’s a snapshot of a few rows and columns:\n\n# Select a few key variables: company name, year, EBIT, and industry\nasx_200_2024 |&gt; \n  select(conm, fyear, ebit, industry) |&gt; \n\n  # Preview the first 10 rows of the data\n  head(10)\n\n# A tibble: 10 × 4\n   conm                   fyear   ebit industry                                 \n   &lt;chr&gt;                  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;                                    \n 1 BHP GROUP LTD          2024  22771  Metals & Mining                          \n 2 TELSTRA GROUP LIMITED  2024   3712  Diversified Telecommunication Services   \n 3 CSL LTD                2024   3896  Biotechnology                            \n 4 TRANSURBAN GROUP       2024   1132  Transportation Infrastructure            \n 5 WOOLWORTHS GROUP LTD   2024   3100  Consumer Staples Distribution & Retail (…\n 6 FORTESCUE LTD          2024   8520  Metals & Mining                          \n 7 WESFARMERS LTD         2024   3849  Broadline Retail (New Name)              \n 8 RAMSAY HEALTH CARE LTD 2024    939. Health Care Providers & Services         \n 9 QANTAS AIRWAYS LTD     2024   2198  Passenger Airlines (New name)            \n10 ORIGIN ENERGY LTD      2024    952  Electric Utilities                       \n\n\nEach variable is a column; each column is a variable (e.g., firm name, year, EBIT, etc). Each observation is a row; each row is an observation (e.g., BHP’s financials in 2024, etc). And, each value is a cell; each cell is a single value (BHP’s EBIT in 2024 was $22,771 million).\nStoring data using this tidy structure makes it easier to manipulate and examine the data because it has an underlying uniformity. Such a structure also takes advantage of R’s vectorized nature - i.e., most R functions work with vectors of values (e.g., columns and rows). In fact, dplyr, ggplot, and all the packages in the tidyverse are explicitly designed to work with tidy data (hence the name…).\nUnfortunately, however, most real data is untidy (often because people who collect and store data are not the ones who analyse and use the data for decision making). And so, we often need to tidy a data set before we can start using the data to answer empirical questions and solve business problems.\nPivoting is the primary tool that we will use for tidying messy data. Pivoting allows you to change the form of your data without changing any of its values-you pivot your data so that variables are structured as columns, and observations as rows.\n\n\n\n\n\n\nImportantWhat are Pivots?\n\n\n\nA pivot reshapes data by turning rows into columns or columns into rows, depending on the question you want to answer.\nIn R, we use the tidyr functions:\n\npivot_longer() – stack columns into rows (e.g., years across columns → one “year” column).\n\npivot_wider() – spread rows into columns (e.g., turn a key-value pair into new columns).\n\nThe same concept exists in other tools:\n\nSQL: PIVOT and UNPIVOT clauses reshape data in queries.\n\nExcel: PivotTables reorganize and summarize data.\n\nPython (pandas): .pivot(), .pivot_table(), and .melt() work like R’s pivots.\n\nApplications include converting messy wide data into tidy long form for analysis, or producing summary tables by category and time.\nIn short, pivoting is a universal wrangling skill: learning it in R helps you apply it in SQL, Excel, and beyond.\n\n\n\n\n6.4.2 ‘Gathering’: Lengthening short data frames\nIn practice, we often need to work with data that is ‘short and wide’-i.e., where observations are stretched across rather than down columns. To tidy such data, we can use pivot_longer(), which folds observations down columns.\nIf we open and examine our ‘wide’ ASX price data, we can see that this data set is short and wide:\n\nprices_wide |&gt;\n    head(10)\n\n# A tibble: 10 × 6\n   gvkey  conm                         `2021`  `2022`  `2023`  `2024`\n   &lt;chr&gt;  &lt;chr&gt;                         &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 005302 LABYRINTH RESOURCES LIMITED   0.034   0.02    0.006   0.017\n 2 013312 BHP GROUP LTD                37.6    38.5    44.2    46.0  \n 3 014242 ANSELL LTD                   34.2    25.2    22.1    31.8  \n 4 016602 THE IQ GROUP GLOBAL LTD       0.2    NA      NA      NA    \n 5 017525 ORIGIN ENERGY LTD             4.73    5.17    8.78   10.0  \n 6 018043 NEWS CORP                    23.5    15.1    20.1    26.6  \n 7 019731 ORBITAL CORP LTD              0.65    0.215   0.16    0.079\n 8 023681 SENEX ENERGY LTD              3.69   NA      NA      NA    \n 9 024433 ASTUTE METALS NL              0.004   0.004   0.037   0.027\n10 031887 RESMED INC                  264.    218.    148.    244.   \n\n\nInstead of structuring each firm-year observation as a row, this data set stretches firm-year observations across the data frame, so that each row is a firm and the columns store the corresponding values for that year (e.g., BHP’s closing stock price at fiscal year-end in 2021, in 2022, etc).\nAn issue with this data structure is that we have data in the column names (i.e., the year of the observation), rather than in the cells. To more easily apply the tidyverse functions to this data and so better facilitate analyses, we need to lengthen and narrow this data frame so that we store all the data in our data frame in cells rather than column names (i.e., we have a column for identifying the year of observation and a single column for stock price, and so that the cells under each column contain data, and no data is contained in the column names themselves).\nWe can use pivot_longer() function to lengthen and narrow our stock price data in this manner:\n\nprices_tidy &lt;- prices_wide |&gt;\n  pivot_longer(\n    # Select the year columns we want to reshape from wide to long\n    cols = c(\"2021\", \"2022\", \"2023\", \"2024\"),\n    # Name of the new column that will store the former column names (years)\n    names_to = \"fyear\",\n    # Name of the new column that will store the values (prices)\n    values_to = \"price\"\n  )\n\n# Preview the first 10 rows of the reshaped (tidy) data\nprices_tidy |&gt;\n  head(10)\n\n# A tibble: 10 × 4\n   gvkey  conm                        fyear  price\n   &lt;chr&gt;  &lt;chr&gt;                       &lt;chr&gt;  &lt;dbl&gt;\n 1 005302 LABYRINTH RESOURCES LIMITED 2021   0.034\n 2 005302 LABYRINTH RESOURCES LIMITED 2022   0.02 \n 3 005302 LABYRINTH RESOURCES LIMITED 2023   0.006\n 4 005302 LABYRINTH RESOURCES LIMITED 2024   0.017\n 5 013312 BHP GROUP LTD               2021  37.6  \n 6 013312 BHP GROUP LTD               2022  38.5  \n 7 013312 BHP GROUP LTD               2023  44.2  \n 8 013312 BHP GROUP LTD               2024  46.0  \n 9 014242 ANSELL LTD                  2021  34.2  \n10 014242 ANSELL LTD                  2022  25.2  \n\n\nGreat. Our ASX price data is now tidy - each column is a variable (e.g., price), each row an observation (e.g., BHP’s stock price in 2021), and each cell a value (e.g., BHP’s closing stock price at year-end 2021 is $37.61).\n\n\n10 min\n\nYou’re given a messy ASX prices data frame, prices_messy, where firm-year information is spread across columns (e.g., price_2023, eps_2024):\n\nprices_messy |&gt;\n    head(10)\n\n# A tibble: 10 × 6\n   gvkey  conm                      price_2023 price_2024 eps_2023 eps_2024\n   &lt;chr&gt;  &lt;chr&gt;                          &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 327267 1414 DEGREES LTD               0.04       0.067  -0.0091  -0.0107\n 2 349318 29METALS LIMITED               0.375     NA      -0.799   NA     \n 3 284698 3D ENERGI LIMITED              0.056      0.07    0.0129  -0.0075\n 4 317959 3P LEARNING LTD                1.27       1.02    0.023   -0.208 \n 5 339106 4DMEDICAL LTD                  0.45       0.675  -0.105   -0.0971\n 6 314650 4DS MEMORY LTD                 0.135      0.084  -0.0038  -0.0031\n 7 324960 5E ADVANCED MATERIALS INC      2.26       0.54   -0.7     -1.18  \n 8 325746 5G NETWORKS LIMITED            0.125      0.148  -0.0579  -0.0836\n 9 253429 88 ENERGY LTD                  0.005     NA      -0.0006  NA     \n10 323980 8COMMON LTD                    0.075      0.041  -0.0147  -0.0114\n\n\nYour goal is to lengthen and narrow this data so that:\n\nEach row is a firm–year observation.\n\nColumn names are variables (e.g., price, eps, year), and no data is stored in column names.\n\nMissing year-specific values are dropped.\n\nUsing pivot_longer(), reshape prices_messy into a tidy data frame named prices_tidier with columns: gvkey, conm, year, price, and eps.\nFinally, show the first 10 rows.\n\n\nSolution\n\n\n\n# Tidy the wide, mixed-name columns (e.g., price_2023, eps_2024) into long form\nprices_tidier &lt;-\n  prices_messy |&gt;\n  tidyr::pivot_longer(\n    # Select all columns that start with \"price\" or \"eps\"\n    cols = c(dplyr::starts_with(\"price\"), dplyr::starts_with(\"eps\")),\n    # Split column names into two parts:\n    #  - \".value\" maps to new column names (price, eps)\n    #  - \"year\" captures the year suffix\n    names_to   = c(\".value\", \"year\"),\n    names_sep  = \"_\",\n    # Drop rows where either price or eps is missing\n    values_drop_na = TRUE\n  ) |&gt;\n  dplyr::mutate(year = as.integer(year))\n\n# Preview the first 10 rows\nprices_tidier |&gt;\n  dplyr::slice_head(n = 10)\n\n# A tibble: 10 × 5\n   gvkey  conm               year price     eps\n   &lt;chr&gt;  &lt;chr&gt;             &lt;int&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 327267 1414 DEGREES LTD   2023 0.04  -0.0091\n 2 327267 1414 DEGREES LTD   2024 0.067 -0.0107\n 3 349318 29METALS LIMITED   2023 0.375 -0.799 \n 4 284698 3D ENERGI LIMITED  2023 0.056  0.0129\n 5 284698 3D ENERGI LIMITED  2024 0.07  -0.0075\n 6 317959 3P LEARNING LTD    2023 1.27   0.023 \n 7 317959 3P LEARNING LTD    2024 1.02  -0.208 \n 8 339106 4DMEDICAL LTD      2023 0.45  -0.105 \n 9 339106 4DMEDICAL LTD      2024 0.675 -0.0971\n10 314650 4DS MEMORY LTD     2023 0.135 -0.0038\n\n\n\n\n\n6.4.3 ‘Spreading’: Widening narrow data frames\nMessy data can also occur in practice in the form of data frames that store individual observations across multiple rows. In order to tidy these, we use pivot_wider() to collapse multiple rows into a single row that stretches across additional columns (i.e., we take our messy data frame that is too ‘long’ and too ‘narrow’ and ‘shorten’ and ‘widen’ to tidy it).\nLet’s move away from our ASX price data back to our financial statements data. The following is a messy snapshot of our ASX financial statements data, messy in the sense that it is too long and narrow:\n\nfinancials_messy |&gt;\n    head(10)\n\n# A tibble: 10 × 5\n   gvkey  conm                  fyear measure_name measure_value\n   &lt;chr&gt;  &lt;chr&gt;                 &lt;dbl&gt; &lt;chr&gt;                &lt;dbl&gt;\n 1 013312 BHP GROUP LTD          2024 at                  102362\n 2 013312 BHP GROUP LTD          2024 sale                 55658\n 3 013312 BHP GROUP LTD          2024 ebit                 22771\n 4 210216 TELSTRA GROUP LIMITED  2024 at                   45550\n 5 210216 TELSTRA GROUP LIMITED  2024 sale                 22928\n 6 210216 TELSTRA GROUP LIMITED  2024 ebit                  3712\n 7 223003 CSL LTD                2024 at                   38022\n 8 223003 CSL LTD                2024 sale                 14690\n 9 223003 CSL LTD                2024 ebit                  3896\n10 212650 TRANSURBAN GROUP       2024 at                   36694\n\n\nImmediately, we can see that we have firm-year observations showing up across multiple rows (e.g., BHP in 2024 is represented three times, one for each financial measure). We can also see that we have variable names as values in a single column (i.e., the column measure_name stores the variables names at, sales, and ebit.\nAs we explain above, to tidy this data frame, we need to collapse our firm-year observations into a single row, and then widen our data frame so that the variables stored in the column ‘measure_name’ each become their own columns. These new columns can then store the values that these measures take.\nWe use pivot_wider() to transform our data frame’s structure in this manner. As we see in the code below, pivot_wider() uses the argument names_from to take the new column names from the pre-existing measure_name column, and then uses the values_from argument to take the values for each observation for these new columns from the pre-existing measure_value column:\n\nfinancials_tidy &lt;- financials_messy |&gt;\n  pivot_wider(\n    # Column names will be taken from the values in 'measure_name'\n    names_from = measure_name,\n    # Cell values will be filled from 'measure_value'\n    values_from = measure_value\n  )\n\n# Preview the first 10 rows of the tidied financials\nfinancials_tidy |&gt;\n  head(10)\n\n# A tibble: 10 × 6\n   gvkey  conm                   fyear     at   sale   ebit\n   &lt;chr&gt;  &lt;chr&gt;                  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 013312 BHP GROUP LTD           2024 102362 55658  22771 \n 2 210216 TELSTRA GROUP LIMITED   2024  45550 22928   3712 \n 3 223003 CSL LTD                 2024  38022 14690   3896 \n 4 212650 TRANSURBAN GROUP        2024  36694  4119   1132 \n 5 100894 WOOLWORTHS GROUP LTD    2024  33936 67922   3100 \n 6 212427 FORTESCUE LTD           2024  30060 18220   8520 \n 7 101601 WESFARMERS LTD          2024  27309 44189   3849 \n 8 226744 RAMSAY HEALTH CARE LTD  2024  20894 16660.   939.\n 9 220244 QANTAS AIRWAYS LTD      2024  20564 20114   2198 \n10 017525 ORIGIN ENERGY LTD       2024  20454 16138    952 \n\n\nGreat. Look at BHP: the mining company is now represented as a single row, and we have columns for assets, sales, and ebit that store BHP’s values for those financial measures in 2024. As such, we can see that we have successfully shortened and widened our financial statements data frame-i.e., each firm-year observation shows up as a single row in our data frame, and each financial measure is stored in its own column and each of these columns takes this measure’s value for a given firm-year observation.\nIn summary, we have used two core tidyr functions, pivot_longer() and pivot_wider()-to tidy messy data sets and so ensure that our data has variables in columns and observations in rows (and so values in cells). While tidying data in this manner may not seem especially interesting or exciting, it is hugely important because tidy data makes cleaning, combining, and analysing data to extract insights much more efficient, straight forward, and robust.",
    "crumbs": [
      "Getting Our First Insights from Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Shaping and Combining Data for Business Analytics</span>"
    ]
  },
  {
    "objectID": "insights/data_tidy.html#using-joins-to-make-one-data-frame-from-many",
    "href": "insights/data_tidy.html#using-joins-to-make-one-data-frame-from-many",
    "title": "6  Shaping and Combining Data for Business Analytics",
    "section": "6.5 Using joins to make one data frame from many",
    "text": "6.5 Using joins to make one data frame from many\nSo far we have been working with individual data frames, but in practice single data frames rarely contain all the information necessary for the analyses that we would like to conduct in order to answer important, interesting and timely business questions. Instead, we commonly need to use multiple data frames to answer these questions, and so we must join these data frames together before we can manipulate and analyze their (combined) contents.\n\n\n\n\n\n\nImportantMutating vs Filtering Joins\n\n\n\nFor the remainder of this module, we will introduce and explore two important types of joins:\n\nMutating joins add new variables to one data frame from matching observations in another.\n\nFiltering joins filter observations from one data frame based on whether or not they match an observation in another.\n\nThe former adds columns, while the latter removes rows.\nJoins use a pair of keys—a primary key and a foreign key—to combine tables.\nA primary key is a variable (or set of variables) that uniquely identifies each observation, while a foreign key links back to that identifier in another data frame.\nBefore we can understand and employ joins, we need to understand how keys connect data frames.\n\n\n\n6.5.1 Understanding how keys connect data frames\nLet’s grab a snapshot of the now familiar data frame that contains financials for the 200-largest firms in the ASX in 2024. In this data, the primary key is the gvkey, a six-digit code that uniquely identifies each firm in the data frame (note, gvkey in our data is a character type, not a numeric type).\n\n# Select only the key identifiers and industry columns from the ASX 200 financials\nfirms &lt;- \n  asx_200_2024 |&gt; \n  select(gvkey, conm, fyear, gind)\n\n# Preview the first 10 rows of the reduced firms data frame\nfirms |&gt; \n  head(10)\n\n# A tibble: 10 × 4\n   gvkey  conm                   fyear   gind\n   &lt;chr&gt;  &lt;chr&gt;                  &lt;chr&gt;  &lt;dbl&gt;\n 1 013312 BHP GROUP LTD          2024  151040\n 2 210216 TELSTRA GROUP LIMITED  2024  501010\n 3 223003 CSL LTD                2024  352010\n 4 212650 TRANSURBAN GROUP       2024  203050\n 5 100894 WOOLWORTHS GROUP LTD   2024  301010\n 6 212427 FORTESCUE LTD          2024  151040\n 7 101601 WESFARMERS LTD         2024  255030\n 8 226744 RAMSAY HEALTH CARE LTD 2024  351020\n 9 220244 QANTAS AIRWAYS LTD     2024  203020\n10 017525 ORIGIN ENERGY LTD      2024  551010\n\n\nNow, let’s compare the above to gics_industry, a look-up table of six-digit industry codes, gind, each of which uniquely identifies an industry in the economy. This six-digit code, gind, is the primary key for our industry look-up table:\n\ngics_industry |&gt;\n    head(10)\n\n# A tibble: 10 × 2\n     gind industry                   \n    &lt;dbl&gt; &lt;chr&gt;                      \n 1 101010 Energy Equipment & Services\n 2 101020 Oil, Gas & Consumable Fuels\n 3 151010 Chemicals                  \n 4 151020 Construction Materials     \n 5 151030 Containers & Packaging     \n 6 151040 Metals & Mining            \n 7 151050 Paper & Forest Products    \n 8 201010 Aerospace & Defense        \n 9 201020 Building Products          \n10 201030 Construction & Engineering \n\n\nAs we defined it above, a foreign key is a variable (or set of variables) that corresponds to a primary key in another table or data frame. Staying with our examples above, gind is the foreign key for firms in that it corresponds to gind, the primary key of gics_industry.\nBefore using our primary and foreign keys to join tables, we first should verify that our primary keys uniquely identify observations in their associated data frames:\n\n# Count how many times each gvkey appears in firms\nfirms |&gt;\n  count(gvkey) |&gt;\n  # Keep only gvkeys that appear more than once\n  filter(n &gt; 1)\n\n# A tibble: 0 × 2\n# ℹ 2 variables: gvkey &lt;chr&gt;, n &lt;int&gt;\n\n\nAnd, for our gics industry data frame:\n\n# Count how many times each gind appears in gics_industry\ngics_industry |&gt;\n  count(gind) |&gt;\n  # Keep only ginds that appear more than once\n  filter(n &gt; 1)\n\n# A tibble: 0 × 2\n# ℹ 2 variables: gind &lt;dbl&gt;, n &lt;int&gt;\n\n\nGreat. We have confirmed that our primary keys uniquely identify each observation in these data frames. It is worth noting here that while gind is the primary key for gics_industry (and so uniquely identifies observations in this data frame), it does not uniquely identify observations in firms, where (as we mentioned above) gind serves as the foreign key. We can easily show this:\n\n# Count the number of times each gind appears in firms\nfirms |&gt;\n  count(gind) |&gt;\n  # Arrange in descending order to check whether gind uniquely identifies observations\n  arrange(desc(n))\n\n# A tibble: 46 × 2\n     gind     n\n    &lt;dbl&gt; &lt;int&gt;\n 1 151040    42\n 2 255040    14\n 3 201030    11\n 4 253010    11\n 5 101020     9\n 6 202010     7\n 7 302020     7\n 8 351020     7\n 9 201070     5\n10 151010     4\n# ℹ 36 more rows\n\n\n\n\n2 min\n\nBefore using our keys to join data sets, confirm there are no missing values in the primary key columns. Otherwise, those observations can’t be identified. Do so for gvkey in firms and for gind in gics_industry.\n\n\nSolution\n\n\n\n# Check whether any observations in firms have a missing gvkey (should be none)\nfirms |&gt;\n  dplyr::filter(is.na(gvkey))\n\n# A tibble: 0 × 4\n# ℹ 4 variables: gvkey &lt;chr&gt;, conm &lt;chr&gt;, fyear &lt;chr&gt;, gind &lt;dbl&gt;\n\n# Check whether any observations in gics_industry have a missing gind (should be none)\ngics_industry |&gt;\n  dplyr::filter(is.na(gind))\n\n# A tibble: 0 × 2\n# ℹ 2 variables: gind &lt;dbl&gt;, industry &lt;chr&gt;\n\n\n\n\n\n6.5.2 Combining variables from two data frames with mutating joins\n\n\n\n\n\n\nImportantMutating Joins: The left_join()\n\n\n\nIn dplyr, join functions share a common interface: they take a pair of data frames (x, y) and return a data frame.\nWe begin with left_join(), a mutating join that combines variables from two data frames.\n\nMutating joins first match observations by keys, then copy columns from one table to the other.\n\nleft_join(x, y, by = ...) adds columns from y to x and guarantees the output has the same number of rows as x (the “left” table).\n\n\n\nLet’s return to our examples from above, and see how we can use left_join() to add a column that identifies the industry in which each firm operates in our firm names data frame:\n\n# Join the firms data with the GICS industry lookup table\n# Match on gind (the foreign key in firms, primary key in gics_industry)\nfirms |&gt; \n  left_join(\n    gics_industry, \n    by = join_by(gind)\n  )\n\n# A tibble: 200 × 5\n   gvkey  conm                   fyear   gind industry                          \n   &lt;chr&gt;  &lt;chr&gt;                  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;                             \n 1 013312 BHP GROUP LTD          2024  151040 Metals & Mining                   \n 2 210216 TELSTRA GROUP LIMITED  2024  501010 Diversified Telecommunication Ser…\n 3 223003 CSL LTD                2024  352010 Biotechnology                     \n 4 212650 TRANSURBAN GROUP       2024  203050 Transportation Infrastructure     \n 5 100894 WOOLWORTHS GROUP LTD   2024  301010 Consumer Staples Distribution & R…\n 6 212427 FORTESCUE LTD          2024  151040 Metals & Mining                   \n 7 101601 WESFARMERS LTD         2024  255030 Broadline Retail (New Name)       \n 8 226744 RAMSAY HEALTH CARE LTD 2024  351020 Health Care Providers & Services  \n 9 220244 QANTAS AIRWAYS LTD     2024  203020 Passenger Airlines (New name)     \n10 017525 ORIGIN ENERGY LTD      2024  551010 Electric Utilities                \n# ℹ 190 more rows\n\n\nSurprisingly straight forward, right? As the above output shows, left_join() has used gics_industry to add a new variable, industry, to the right-hand end of firms. No other columns have been added, and all existing columns are as they were before the join. Furthermore, we have the same number of rows in our firms name data frame post-join as we had pre-join.\n\n\n\n\n\n\nNoteDefaults in left_join()\n\n\n\nleft_join(x, y) does several things by default:\n\nIt uses all variables that appear in both data frames as the join key.\n\nYou can override this by explicitly specifying the variable(s) to use as the join key.\n\nIt adds all columns from data frame y that do not already appear in data frame x.\n\nYou can override this by specifying exactly which columns from y should be joined to x.\n\n\n\n\n\nLet’s try something more ambitious: using a join to add our price data to our financial statements data for the 200-largest ASX-listed firms in 2024. First, let’s again pull up and examine our financial statements data frame:\n\nasx_200_2024 |&gt;\n    head(10)\n\n# A tibble: 10 × 30\n   gvkey  curcd fyear   fyr datadate     at  capx shareequity   dlc   dltt   dvp\n   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1 013312 USD   2024      6 6/30/20… 102362 9273       49120  2084  18634     NA\n 2 210216 AUD   2024      6 6/30/20…  45550 2288       17352  4228  12740     NA\n 3 223003 USD   2024      6 6/30/20…  38022  849       19401   944  11239     NA\n 4 212650 AUD   2024      6 6/30/20…  36694  104       11678  1590  18596     NA\n 5 100894 AUD   2024      6 6/30/20…  33936 2548        5570  2311  14411     NA\n 6 212427 USD   2024      6 6/30/20…  30060 2834       19531   192   5208     NA\n 7 101601 AUD   2024      6 6/30/20…  27309  923        8585  1165  10113     NA\n 8 226744 AUD   2024      6 6/30/20…  20894  754.       5275.  606. 10332.    NA\n 9 220244 AUD   2024      6 6/30/20…  20564 2761         294   600   5991     NA\n10 017525 AUD   2024      6 6/30/20…  20454  608        9489    68   3310     NA\n# ℹ 19 more variables: ebit &lt;dbl&gt;, netprofit &lt;dbl&gt;, pstk &lt;dbl&gt;, sale &lt;dbl&gt;,\n#   epsexcon &lt;dbl&gt;, nicon &lt;dbl&gt;, conm &lt;chr&gt;, fic &lt;chr&gt;, conml &lt;chr&gt;,\n#   ggroup &lt;dbl&gt;, gind &lt;dbl&gt;, gsector &lt;dbl&gt;, gsubind &lt;dbl&gt;, sector &lt;chr&gt;,\n#   indgroup &lt;chr&gt;, industry &lt;chr&gt;, subind &lt;chr&gt;, debt &lt;dbl&gt;,\n#   invested_capital &lt;dbl&gt;\n\n\nAs we discussed above, gvkey appears in asx_200_2024. Now, let’s pull up and examine our stock price data frame:\n\nprices_tidy |&gt;\n    head(10)\n\n# A tibble: 10 × 4\n   gvkey  conm                        fyear  price\n   &lt;chr&gt;  &lt;chr&gt;                       &lt;chr&gt;  &lt;dbl&gt;\n 1 005302 LABYRINTH RESOURCES LIMITED 2021   0.034\n 2 005302 LABYRINTH RESOURCES LIMITED 2022   0.02 \n 3 005302 LABYRINTH RESOURCES LIMITED 2023   0.006\n 4 005302 LABYRINTH RESOURCES LIMITED 2024   0.017\n 5 013312 BHP GROUP LTD               2021  37.6  \n 6 013312 BHP GROUP LTD               2022  38.5  \n 7 013312 BHP GROUP LTD               2023  44.2  \n 8 013312 BHP GROUP LTD               2024  46.0  \n 9 014242 ANSELL LTD                  2021  34.2  \n10 014242 ANSELL LTD                  2022  25.2  \n\n\nWe also see gvkey in our stock price data frame. So, why don’t we examine whether gvkey uniquely identifies the observations in prices_tidy, in which case we can use gvkey for our join.\n\n# Count how many times each gvkey appears in prices_tidy\n# Filter for gvkeys that appear more than once (i.e., not unique)\nprices_tidy |&gt; \n  count(gvkey) |&gt; \n  filter(n &gt; 1)\n\n# A tibble: 1,835 × 2\n   gvkey      n\n   &lt;chr&gt;  &lt;int&gt;\n 1 005302     4\n 2 010991     4\n 3 013312     4\n 4 014242     4\n 5 014802     4\n 6 015362     4\n 7 015889     4\n 8 016560     4\n 9 016602     4\n10 017525     4\n# ℹ 1,825 more rows\n\n\nLucky we checked because gvkey does not uniquely identify each observation in prices_tidy. Instead, we observe in many cases four instances of each gvkey value in our data frame. This occurs because while our financial statements data, asx_200_2024, covers a single firm-year (2024), our stock price data, prices_tidy, covers four firm-years (2021-2024). And so, we must use gvkey and fyear to join our data frames.\nBefore making this join, let’s confirm that for each data frame, this compound key (gvkey and fyear) uniquely identifies each observation (we refer to a primary key that uses a set of variables as a compound key):\n\n# Count how many times each gvkey–fyear pair appears in asx_200_2024\n# If the compound key is unique, all counts should equal 1\nasx_200_2024 |&gt; \n  count(gvkey, fyear) |&gt; \n  arrange(desc(n))\n\n# A tibble: 200 × 3\n   gvkey  fyear     n\n   &lt;chr&gt;  &lt;chr&gt; &lt;int&gt;\n 1 013312 2024      1\n 2 014242 2024      1\n 3 017525 2024      1\n 4 100251 2024      1\n 5 100442 2024      1\n 6 100461 2024      1\n 7 100894 2024      1\n 8 101392 2024      1\n 9 101513 2024      1\n10 101568 2024      1\n# ℹ 190 more rows\n\n\n\n# Count how many times each gvkey–fyear pair appears in prices_tidy\n# If the compound key is unique, all counts should equal 1\nprices_tidy |&gt; \n  count(gvkey, fyear) |&gt; \n  arrange(desc(n))\n\n# A tibble: 7,340 × 3\n   gvkey  fyear     n\n   &lt;chr&gt;  &lt;chr&gt; &lt;int&gt;\n 1 005302 2021      1\n 2 005302 2022      1\n 3 005302 2023      1\n 4 005302 2024      1\n 5 010991 2021      1\n 6 010991 2022      1\n 7 010991 2023      1\n 8 010991 2024      1\n 9 013312 2021      1\n10 013312 2022      1\n# ℹ 7,330 more rows\n\n\nGreat. We can use the compound key gvkey and fyear to join our data frames.\n\n# Join the financial statements data (asx_200_2024) with stock prices (prices_tidy)\n# Use gvkey and fyear together as the compound key\n# Keep all rows from asx_200_2024 and add matching price column from prices_tidy\nfinancials_prices_lj &lt;- \n  asx_200_2024 |&gt; \n  left_join(\n    prices_tidy |&gt; select(gvkey, fyear, price),\n    by = join_by(gvkey, fyear)\n  )\n\n# Reorder columns so price appears first, followed by the rest of the data\nfinancials_prices_lj |&gt; \n  select(price, everything())\n\n# A tibble: 200 × 31\n    price gvkey curcd fyear   fyr datadate     at  capx shareequity   dlc   dltt\n    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1  46.0  0133… USD   2024      6 6/30/20… 102362 9273       49120  2084  18634 \n 2   3.88 2102… AUD   2024      6 6/30/20…  45550 2288       17352  4228  12740 \n 3 286.   2230… USD   2024      6 6/30/20…  38022  849       19401   944  11239 \n 4  13.1  2126… AUD   2024      6 6/30/20…  36694  104       11678  1590  18596 \n 5  33.3  1008… AUD   2024      6 6/30/20…  33936 2548        5570  2311  14411 \n 6  20.7  2124… USD   2024      6 6/30/20…  30060 2834       19531   192   5208 \n 7  70.4  1016… AUD   2024      6 6/30/20…  27309  923        8585  1165  10113 \n 8  41.6  2267… AUD   2024      6 6/30/20…  20894  754.       5275.  606. 10332.\n 9   7.42 2202… AUD   2024      6 6/30/20…  20564 2761         294   600   5991 \n10  10.0  0175… AUD   2024      6 6/30/20…  20454  608        9489    68   3310 \n# ℹ 190 more rows\n# ℹ 20 more variables: dvp &lt;dbl&gt;, ebit &lt;dbl&gt;, netprofit &lt;dbl&gt;, pstk &lt;dbl&gt;,\n#   sale &lt;dbl&gt;, epsexcon &lt;dbl&gt;, nicon &lt;dbl&gt;, conm &lt;chr&gt;, fic &lt;chr&gt;,\n#   conml &lt;chr&gt;, ggroup &lt;dbl&gt;, gind &lt;dbl&gt;, gsector &lt;dbl&gt;, gsubind &lt;dbl&gt;,\n#   sector &lt;chr&gt;, indgroup &lt;chr&gt;, industry &lt;chr&gt;, subind &lt;chr&gt;, debt &lt;dbl&gt;,\n#   invested_capital &lt;dbl&gt;\n\n\nGreat. Now we have financial and stock price data, and so we can examine questions about the relationship between firms’ operating performance and capital market performance - e.g., do investors pay more for firms that generate higher earnings? We will leave questions like this one to this week’s tutorial exercises.\nBefore moving on, one thing you might have noticed, however, is that in combining our financial and stock price data frames left_join() keeps all the rows in the financial statements data frame and only those rows; that is, it does not combine stock price data for firms that appear in our stock price data frame but that do not appear in our financial statements data frame (if you look closely at the output above, you can see that while asx_200_2024 contains 200 rows-one for each of the 200-largest ASX firms in 2024-prices_tidy contains 7,340 rows-one for each firm-year for all ASX listed firms for the period 2021-2024). While all other mutating joins employ the same interface as left_join(), they differ in terms of which rows they keep.\nFor example, we can run the code below to see which rows right_join(), another type of mutating join, keeps:\n\n# Join the financial statements data (asx_200_2024) with stock prices (prices_tidy)\n# Use gvkey and fyear as the compound key\n# Keep all rows from prices_tidy (the \"right\" table), adding financials where available\nfinancials_prices_rj &lt;- \n  asx_200_2024 |&gt; \n  right_join(\n    prices_tidy |&gt; select(gvkey, fyear, price),\n    by = join_by(gvkey, fyear)\n  )\n\n# Reorder columns so price appears first, followed by the rest of the data\nfinancials_prices_rj |&gt; \n  select(price, everything())\n\n# A tibble: 7,340 × 31\n    price gvkey curcd fyear   fyr datadate     at  capx shareequity   dlc   dltt\n    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1  46.0  0133… USD   2024      6 6/30/20… 102362 9273       49120  2084  18634 \n 2   3.88 2102… AUD   2024      6 6/30/20…  45550 2288       17352  4228  12740 \n 3 286.   2230… USD   2024      6 6/30/20…  38022  849       19401   944  11239 \n 4  13.1  2126… AUD   2024      6 6/30/20…  36694  104       11678  1590  18596 \n 5  33.3  1008… AUD   2024      6 6/30/20…  33936 2548        5570  2311  14411 \n 6  20.7  2124… USD   2024      6 6/30/20…  30060 2834       19531   192   5208 \n 7  70.4  1016… AUD   2024      6 6/30/20…  27309  923        8585  1165  10113 \n 8  41.6  2267… AUD   2024      6 6/30/20…  20894  754.       5275.  606. 10332.\n 9   7.42 2202… AUD   2024      6 6/30/20…  20564 2761         294   600   5991 \n10  10.0  0175… AUD   2024      6 6/30/20…  20454  608        9489    68   3310 \n# ℹ 7,330 more rows\n# ℹ 20 more variables: dvp &lt;dbl&gt;, ebit &lt;dbl&gt;, netprofit &lt;dbl&gt;, pstk &lt;dbl&gt;,\n#   sale &lt;dbl&gt;, epsexcon &lt;dbl&gt;, nicon &lt;dbl&gt;, conm &lt;chr&gt;, fic &lt;chr&gt;,\n#   conml &lt;chr&gt;, ggroup &lt;dbl&gt;, gind &lt;dbl&gt;, gsector &lt;dbl&gt;, gsubind &lt;dbl&gt;,\n#   sector &lt;chr&gt;, indgroup &lt;chr&gt;, industry &lt;chr&gt;, subind &lt;chr&gt;, debt &lt;dbl&gt;,\n#   invested_capital &lt;dbl&gt;\n\n\nSo, on face value the output looks much like that which we got from left_join(), but on closer inspection we now have 7,340 rows-the same as we have in prices_tidy.\nWe can see that right_join(x, y) keeps all rows in the y data frame. As such, our output from right_join() has many, many rows where we have missing values for the columns from asx_200_2024 and only values for the price column (i.e., firms for which we have stock price data but not financial statements data).\nWe can observe this by counting the number of missing values in financials_prices_rj for any of columns from asx_200_2024. For example, we expect (and do find) that we have many missing values for ebit:\n\nsum(is.na(financials_prices_rj$ebit))\n\n[1] 7145\n\n\n\n\n10 min\n\nAs the name suggests, full_join() keeps all rows in either x or y. That is, the output contains all observations from x (even if no match in y) and all observations from y (even if no match in x).\nUsing gvkey and fyear as a compound key, join financial statements data (asx_200_2024) with stock prices (prices_tidy), then reorder columns so price appears first.\nFinally, report the number of rows produced and explain why it can exceed the row count from right_join().\n\n\nSolution\n\n\n\n# Full join on compound key (gvkey, fyear)\nfinancials_prices_fj &lt;-\n  asx_200_2024 |&gt;\n  full_join(\n    prices_tidy |&gt; select(gvkey, fyear, price),\n    by = join_by(gvkey, fyear)\n  )\n\n# Reorder columns so price is first\nfinancials_prices_fj |&gt;\n  select(price, everything())\n\n# A tibble: 7,343 × 31\n    price gvkey curcd fyear   fyr datadate     at  capx shareequity   dlc   dltt\n    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1  46.0  0133… USD   2024      6 6/30/20… 102362 9273       49120  2084  18634 \n 2   3.88 2102… AUD   2024      6 6/30/20…  45550 2288       17352  4228  12740 \n 3 286.   2230… USD   2024      6 6/30/20…  38022  849       19401   944  11239 \n 4  13.1  2126… AUD   2024      6 6/30/20…  36694  104       11678  1590  18596 \n 5  33.3  1008… AUD   2024      6 6/30/20…  33936 2548        5570  2311  14411 \n 6  20.7  2124… USD   2024      6 6/30/20…  30060 2834       19531   192   5208 \n 7  70.4  1016… AUD   2024      6 6/30/20…  27309  923        8585  1165  10113 \n 8  41.6  2267… AUD   2024      6 6/30/20…  20894  754.       5275.  606. 10332.\n 9   7.42 2202… AUD   2024      6 6/30/20…  20564 2761         294   600   5991 \n10  10.0  0175… AUD   2024      6 6/30/20…  20454  608        9489    68   3310 \n# ℹ 7,333 more rows\n# ℹ 20 more variables: dvp &lt;dbl&gt;, ebit &lt;dbl&gt;, netprofit &lt;dbl&gt;, pstk &lt;dbl&gt;,\n#   sale &lt;dbl&gt;, epsexcon &lt;dbl&gt;, nicon &lt;dbl&gt;, conm &lt;chr&gt;, fic &lt;chr&gt;,\n#   conml &lt;chr&gt;, ggroup &lt;dbl&gt;, gind &lt;dbl&gt;, gsector &lt;dbl&gt;, gsubind &lt;dbl&gt;,\n#   sector &lt;chr&gt;, indgroup &lt;chr&gt;, industry &lt;chr&gt;, subind &lt;chr&gt;, debt &lt;dbl&gt;,\n#   invested_capital &lt;dbl&gt;\n\n\nUsing full_join(), our output contains 7,343 rows - three more than in financials_prices_fj, the output that right_join() created.\nfull_join() returns every key that appears in either table. That means it keeps:\n\nall matched firm–years (where both financials and prices exist),\nplus firm–years that appear only in prices,\nplus firm–years that appear only in financials.\n\nBecause it retains both sets of unmatched keys, the row count can be larger than what you get from right_join() (which only guarantees all rows from the right-hand table).\n\n\n\n\n\n\n\nNoteIdentifying Non-Matching Observations\n\n\n\nThese three additional rows are observations from asx_200_2024, our financial statements data frame, that do not have a match in prices_tidy, the stock price data frame. In other words, they are gvkey–year combinations that appear in the financial statements but not in the stock price data.\nTo identify these cases, we use a specific type of join: anti_join(). Unlike the mutating joins we have used so far, anti_join() is a filtering join. It removes matching rows and keeps only the non-matching ones — more on this distinction later.\n\n\nWe can use anti_join(x, y) to identify and keep only those observations that appear in asx_200_2024 (i.e., x) but that do not appear in prices_tidy (i.e., y):\n\n# Use anti_join() to keep only rows from asx_200_2024 \n# that do NOT have a matching gvkey–fyear in prices_tidy\n# This helps identify implicit missing values (observations present in x but absent in y)\nfinancials_prices_aj &lt;- \n  asx_200_2024 |&gt; \n  anti_join(\n    prices_tidy,\n    by = join_by(gvkey, fyear)\n  )\n\n# Display the unmatched observations\nfinancials_prices_aj\n\n# A tibble: 3 × 30\n  gvkey  curcd fyear   fyr datadate     at  capx shareequity   dlc   dltt   dvp\n  &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 267188 AUD   2024      6 6/30/2024 3477.  7.3        324.  340.     1.3    NA\n2 270244 AUD   2024      7 7/31/2024 2376. 52          255   162.  1467.     NA\n3 364417 AUD   2024      6 6/30/2024  362.  3.92        82.9  37.3   69.1    NA\n# ℹ 19 more variables: ebit &lt;dbl&gt;, netprofit &lt;dbl&gt;, pstk &lt;dbl&gt;, sale &lt;dbl&gt;,\n#   epsexcon &lt;dbl&gt;, nicon &lt;dbl&gt;, conm &lt;chr&gt;, fic &lt;chr&gt;, conml &lt;chr&gt;,\n#   ggroup &lt;dbl&gt;, gind &lt;dbl&gt;, gsector &lt;dbl&gt;, gsubind &lt;dbl&gt;, sector &lt;chr&gt;,\n#   indgroup &lt;chr&gt;, industry &lt;chr&gt;, subind &lt;chr&gt;, debt &lt;dbl&gt;,\n#   invested_capital &lt;dbl&gt;\n\n\nWe see that our output from using anti_join() contains three rows, exactly the number we expected given the difference in the number of rows found in the output from when we used right_join() and when we used full_join().\nTo confirm that these observations are in fact those from x that have no match in y, we can examine whether these gvkey-year combinations from our financial statements data frame appear in our stock price data frame:\n\n# Check whether gvkey 267188 in year 2024 appears in prices_tidy\nprices_tidy |&gt; \n  filter(gvkey == \"267188\", fyear == \"2024\")\n\n# A tibble: 0 × 4\n# ℹ 4 variables: gvkey &lt;chr&gt;, conm &lt;chr&gt;, fyear &lt;chr&gt;, price &lt;dbl&gt;\n\n# Check whether gvkey 270244 in year 2024 appears in prices_tidy\nprices_tidy |&gt; \n  filter(gvkey == \"270244\", fyear == \"2024\")\n\n# A tibble: 0 × 4\n# ℹ 4 variables: gvkey &lt;chr&gt;, conm &lt;chr&gt;, fyear &lt;chr&gt;, price &lt;dbl&gt;\n\n# Check whether gvkey 364417 in year 2024 appears in prices_tidy\nprices_tidy |&gt; \n  filter(gvkey == \"364417\", fyear == \"2024\")\n\n# A tibble: 0 × 4\n# ℹ 4 variables: gvkey &lt;chr&gt;, conm &lt;chr&gt;, fyear &lt;chr&gt;, price &lt;dbl&gt;\n\n\nGreat. Those observations from asx_200_2024 that anti_join() showed us to be missing from prices_tidy are in fact missing when we manually search for these observations using their unique gvkey-year combination.\nThe final mutating join that we consider is the inner_join(), which is effectively the inverse of anti_join()-that is, inner_join(x, y) only keeps rows that appear in both x and y and then adds columns from y to x. Let’s examine our output when we use inner_join() to combine our financial statements data and our stock price data:\n\n# Use inner_join() to keep only rows where gvkey–fyear combinations \n# appear in BOTH asx_200_2024 (financials) and prices_tidy (stock prices).\n# This is effectively the inverse of anti_join().\nfinancials_prices_ij &lt;- \n  asx_200_2024 |&gt; \n  # Join with prices_tidy to add the stock price column\n  inner_join(\n    # Select only the join keys and the price variable\n    prices_tidy |&gt; \n      select(gvkey, fyear, price),\n    # Match on the compound key (gvkey + fyear)\n    by = join_by(gvkey, fyear)\n  )\n\n# Display the resulting joined data\nfinancials_prices_ij\n\n# A tibble: 197 × 31\n   gvkey  curcd fyear   fyr datadate     at  capx shareequity   dlc   dltt   dvp\n   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1 013312 USD   2024      6 6/30/20… 102362 9273       49120  2084  18634     NA\n 2 210216 AUD   2024      6 6/30/20…  45550 2288       17352  4228  12740     NA\n 3 223003 USD   2024      6 6/30/20…  38022  849       19401   944  11239     NA\n 4 212650 AUD   2024      6 6/30/20…  36694  104       11678  1590  18596     NA\n 5 100894 AUD   2024      6 6/30/20…  33936 2548        5570  2311  14411     NA\n 6 212427 USD   2024      6 6/30/20…  30060 2834       19531   192   5208     NA\n 7 101601 AUD   2024      6 6/30/20…  27309  923        8585  1165  10113     NA\n 8 226744 AUD   2024      6 6/30/20…  20894  754.       5275.  606. 10332.    NA\n 9 220244 AUD   2024      6 6/30/20…  20564 2761         294   600   5991     NA\n10 017525 AUD   2024      6 6/30/20…  20454  608        9489    68   3310     NA\n# ℹ 187 more rows\n# ℹ 20 more variables: ebit &lt;dbl&gt;, netprofit &lt;dbl&gt;, pstk &lt;dbl&gt;, sale &lt;dbl&gt;,\n#   epsexcon &lt;dbl&gt;, nicon &lt;dbl&gt;, conm &lt;chr&gt;, fic &lt;chr&gt;, conml &lt;chr&gt;,\n#   ggroup &lt;dbl&gt;, gind &lt;dbl&gt;, gsector &lt;dbl&gt;, gsubind &lt;dbl&gt;, sector &lt;chr&gt;,\n#   indgroup &lt;chr&gt;, industry &lt;chr&gt;, subind &lt;chr&gt;, debt &lt;dbl&gt;,\n#   invested_capital &lt;dbl&gt;, price &lt;dbl&gt;\n\n\nInteresting! The output from the inner join contains 197 observations, three fewer than we find in asx_200_2024, our financial statements data frame. You guessed it: these ‘missing’ observations from our financial statements data frame are those and only those that appear in our output when we use anti_join(). Neat, now we have shown how the different mutating joins work to combine data from a pair of data frames into a single data frame.\n\n\n6.5.3 Selecting rows from a data frame with filtering joins\n\n\n\n\n\n\nImportantWhat are Filtering Joins?\n\n\n\nAs the name suggests, filtering joins work by filtering rows, and unlike mutating joins they do not add columns to a data frame. Instead, they determine which observations to keep or drop based on whether matches exist between two data frames.\nThe function anti_join(x, y) is a type of filtering join. It returns all rows in x that do not have a match in y. Filtering joins also include semi_join(x, y), which works in the opposite way: it keeps rows in x only if they have a match in y. Together, semi_join() and anti_join() provide powerful tools for examining overlap between data sets and identifying gaps in coverage.\n\n\n\n\n3 min\n\nUse colnames() to compare the variables in the data frame created with anti_join() (financials_prices_aj) to those in the original asx_200_2024 data frame.\n\nWhat do you observe about the column sets?\n\nWhat does this imply about how filtering joins like anti_join() work compared to mutating joins such as inner_join() or full_join()?\n\n\n\nSolution\n\n\n\n# Column names in the anti_join result\nfinancials_prices_aj |&gt;\n  colnames()\n\n [1] \"gvkey\"            \"curcd\"            \"fyear\"            \"fyr\"             \n [5] \"datadate\"         \"at\"               \"capx\"             \"shareequity\"     \n [9] \"dlc\"              \"dltt\"             \"dvp\"              \"ebit\"            \n[13] \"netprofit\"        \"pstk\"             \"sale\"             \"epsexcon\"        \n[17] \"nicon\"            \"conm\"             \"fic\"              \"conml\"           \n[21] \"ggroup\"           \"gind\"             \"gsector\"          \"gsubind\"         \n[25] \"sector\"           \"indgroup\"         \"industry\"         \"subind\"          \n[29] \"debt\"             \"invested_capital\"\n\n# Column names in the original financial statements data\nasx_200_2024 |&gt;\n  colnames()\n\n [1] \"gvkey\"            \"curcd\"            \"fyear\"            \"fyr\"             \n [5] \"datadate\"         \"at\"               \"capx\"             \"shareequity\"     \n [9] \"dlc\"              \"dltt\"             \"dvp\"              \"ebit\"            \n[13] \"netprofit\"        \"pstk\"             \"sale\"             \"epsexcon\"        \n[17] \"nicon\"            \"conm\"             \"fic\"              \"conml\"           \n[21] \"ggroup\"           \"gind\"             \"gsector\"          \"gsubind\"         \n[25] \"sector\"           \"indgroup\"         \"industry\"         \"subind\"          \n[29] \"debt\"             \"invested_capital\"\n\n\nThe two sets of column names are identical. This occurs because filtering joins (anti_join(), semi_join()) only filter rows of the x data frame using y; they do not add variables from y.\nBy contrast, mutating joins (inner_join(), left_join(), right_join(), full_join()) combine columns from both data frames when keys match.\nThis comparison makes clear that filtering joins are useful when the goal is to select or exclude observations, while mutating joins are used to add new information from another data frame.\n\n\n\n\n\n\n\nNoteMissing values and anti_join()?\n\n\n\nAt first glance, anti_join() may not seem especially useful. However, it is a powerful tool for solving a common problem when working with real data: identifying implicit missing values.\nMost of the time, missing values show up explicitly as NA in a data frame. But sometimes the issue is more subtle: an observation is missing entirely and therefore leaves no trace in the data. These gaps are often just as important as the data that is present.\nBy comparing two related data frames, anti_join() lets us find these implicit missing values. This makes anti_join() particularly valuable for:\n\nDiagnosing incomplete data coverage\n\nChecking for firms, years, or categories that should appear but don’t\n\nEnsuring consistency when merging multiple sources of information\n\nIn practice, anti_join(x, y) returns rows in x that have no match in y—exactly the kind of check you need when searching for hidden gaps.\n\n\nLet’s look at an example of implicit missing values in asx_200_2024, our stock price data frame. We identified these when we used anti_join() earlier to create the following data frame:\n\nfinancials_prices_aj\n\n# A tibble: 3 × 30\n  gvkey  curcd fyear   fyr datadate     at  capx shareequity   dlc   dltt   dvp\n  &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 267188 AUD   2024      6 6/30/2024 3477.  7.3        324.  340.     1.3    NA\n2 270244 AUD   2024      7 7/31/2024 2376. 52          255   162.  1467.     NA\n3 364417 AUD   2024      6 6/30/2024  362.  3.92        82.9  37.3   69.1    NA\n# ℹ 19 more variables: ebit &lt;dbl&gt;, netprofit &lt;dbl&gt;, pstk &lt;dbl&gt;, sale &lt;dbl&gt;,\n#   epsexcon &lt;dbl&gt;, nicon &lt;dbl&gt;, conm &lt;chr&gt;, fic &lt;chr&gt;, conml &lt;chr&gt;,\n#   ggroup &lt;dbl&gt;, gind &lt;dbl&gt;, gsector &lt;dbl&gt;, gsubind &lt;dbl&gt;, sector &lt;chr&gt;,\n#   indgroup &lt;chr&gt;, industry &lt;chr&gt;, subind &lt;chr&gt;, debt &lt;dbl&gt;,\n#   invested_capital &lt;dbl&gt;\n\n\nNow, if we look at our stock price data frame, we have many rows that contain missing values, but these observations do not include those that we identified by using anti_join():\n\nprices_tidy |&gt;\n  filter(is.na(price))\n\n# A tibble: 1,133 × 4\n   gvkey  conm                    fyear price\n   &lt;chr&gt;  &lt;chr&gt;                   &lt;chr&gt; &lt;dbl&gt;\n 1 016602 THE IQ GROUP GLOBAL LTD 2022     NA\n 2 016602 THE IQ GROUP GLOBAL LTD 2023     NA\n 3 016602 THE IQ GROUP GLOBAL LTD 2024     NA\n 4 023681 SENEX ENERGY LTD        2022     NA\n 5 023681 SENEX ENERGY LTD        2023     NA\n 6 023681 SENEX ENERGY LTD        2024     NA\n 7 037798 TRITIUM DCFC LTD        2024     NA\n 8 039960 IRIS ENERGY LIMITED     2021     NA\n 9 040702 LOCAFY LTD              2021     NA\n10 040702 LOCAFY LTD              2024     NA\n# ℹ 1,123 more rows\n\n\nThe gvkeys in financials_prices_aj do not appear in the above, not even as NA (check for using filter()).\nComparing these two data frames should make clear that we have two types of missing data: missing values that are explicitly recorded as such in a data frame, and so are directly observable-i.e., those above; and implicit missing values that we identify by their absence-i.e., those that we identify above using anti_join().\nWe now consider our final join for this week’s topic, semi_join(), which is a filtering join that keeps all rows in x that have a match in y. Let’s see it in action:\n\n# Use semi_join() to keep only those rows in asx_200_2024 (x)\n# that also have a matching gvkey–fyear combination in prices_tidy (y)\nfinancials_prices_sj &lt;- \n  asx_200_2024 |&gt; \n  semi_join(\n    prices_tidy,\n    by = join_by(gvkey, fyear)\n  )\n\n# Display the resulting data frame\n# This shows the subset of financial statements that also have matching stock price data\nfinancials_prices_sj\n\n# A tibble: 197 × 30\n   gvkey  curcd fyear   fyr datadate     at  capx shareequity   dlc   dltt   dvp\n   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1 013312 USD   2024      6 6/30/20… 102362 9273       49120  2084  18634     NA\n 2 210216 AUD   2024      6 6/30/20…  45550 2288       17352  4228  12740     NA\n 3 223003 USD   2024      6 6/30/20…  38022  849       19401   944  11239     NA\n 4 212650 AUD   2024      6 6/30/20…  36694  104       11678  1590  18596     NA\n 5 100894 AUD   2024      6 6/30/20…  33936 2548        5570  2311  14411     NA\n 6 212427 USD   2024      6 6/30/20…  30060 2834       19531   192   5208     NA\n 7 101601 AUD   2024      6 6/30/20…  27309  923        8585  1165  10113     NA\n 8 226744 AUD   2024      6 6/30/20…  20894  754.       5275.  606. 10332.    NA\n 9 220244 AUD   2024      6 6/30/20…  20564 2761         294   600   5991     NA\n10 017525 AUD   2024      6 6/30/20…  20454  608        9489    68   3310     NA\n# ℹ 187 more rows\n# ℹ 19 more variables: ebit &lt;dbl&gt;, netprofit &lt;dbl&gt;, pstk &lt;dbl&gt;, sale &lt;dbl&gt;,\n#   epsexcon &lt;dbl&gt;, nicon &lt;dbl&gt;, conm &lt;chr&gt;, fic &lt;chr&gt;, conml &lt;chr&gt;,\n#   ggroup &lt;dbl&gt;, gind &lt;dbl&gt;, gsector &lt;dbl&gt;, gsubind &lt;dbl&gt;, sector &lt;chr&gt;,\n#   indgroup &lt;chr&gt;, industry &lt;chr&gt;, subind &lt;chr&gt;, debt &lt;dbl&gt;,\n#   invested_capital &lt;dbl&gt;\n\n\nThis subset of observations from our financial statements data frame should look familiar because these 197 observations are the same that we outputted when we used inner_join(), except the difference here is that semi_join() does not add any columns to these observations (whereas when we used inner_join() we added the column price to asx_200_2024 for these observations).",
    "crumbs": [
      "Getting Our First Insights from Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Shaping and Combining Data for Business Analytics</span>"
    ]
  },
  {
    "objectID": "insights/data_tidy.html#bringing-it-all-together-reshaping-and-combining-data",
    "href": "insights/data_tidy.html#bringing-it-all-together-reshaping-and-combining-data",
    "title": "6  Shaping and Combining Data for Business Analytics",
    "section": "6.6 Bringing it all together: Reshaping and Combining Data",
    "text": "6.6 Bringing it all together: Reshaping and Combining Data\n\n\n\n\n\n\nNoteWhy reshaping and joining matter\n\n\n\nIn practice, no single dataset ever contains all the information needed to answer a business question. Some data arrives in messy, inconvenient formats that must be reshaped before analysis, while other data needs to be linked across multiple sources. Mastering pivots and joins ensures that analysts can prepare clean, consistent data sets ready for deeper analysis and visualization.\n\n\nIn this module, we have introduced two essential families of tools for working with business data: pivots for reshaping messy data into tidy form, and joins for combining multiple data frames into one.\nFirst, we showed how pivot_longer() can be used to tidy “short and wide” stock price data by stacking year columns into a single year variable, with prices stored in one column. We also extended this to more complex messy data where column names contained both variable names and values (e.g., price_2023, eps_2023), demonstrating how the .value argument helps split these into tidy variables.\nSecond, we showed how pivot_wider() can tidy “long and narrow” financials data by collapsing repeated firm-year rows into a single row with columns for each financial measure. This ensured that each observation (a firm-year) was represented once, with measures like assets, sales, and EBIT stored as variables in their own columns.\nThird, we turned to joins. We used left_join() to enrich a data frame of firms with information from a lookup table of industries, illustrating how primary keys and foreign keys connect observations across data frames. We then joined financial statement data with stock price data, showing how compound keys (gvkey and fyear) are often necessary when a single variable does not uniquely identify observations.\nFourth, we explored how different joins handle matching and non-matching observations. left_join() kept all rows from the financial statements; right_join() kept all rows from the stock prices; and full_join() kept rows from both. To dig deeper into non-matching cases, we used anti_join() to identify gvkey–year combinations present in one data frame but absent in another, demonstrating how anti-joins help reveal implicit missing values. Conversely, inner_join() and semi_join() kept only matching rows, letting us focus on overlap between datasets.\n\n\n\n\n\n\nImportantKey Takeaway\n\n\n\nPivots allow us to reshape messy data into a tidy, consistent structure that the tidyverse is built to handle, while joins allow us to combine information from multiple sources into a single analytic data set. These two skills — tidying with pivots and joining with keys — form the foundation for reliable, transparent, and efficient business analytics.",
    "crumbs": [
      "Getting Our First Insights from Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Shaping and Combining Data for Business Analytics</span>"
    ]
  },
  {
    "objectID": "insights/data_tidy.html#conclusion",
    "href": "insights/data_tidy.html#conclusion",
    "title": "6  Shaping and Combining Data for Business Analytics",
    "section": "6.7 Conclusion",
    "text": "6.7 Conclusion\nIn this chapter, we have explored how to use the tidyr and dplyr packages to reshape and combine financial data in R. We learned how to tidy messy datasets using pivot_longer() and pivot_wider(), ensuring that variables are stored in columns and observations in rows. We then turned to joins, using keys to link together information from multiple sources. Through left_join(), right_join(), full_join(), and inner_join(), as well as filtering joins like semi_join() and anti_join(), we saw how different types of joins either add variables or filter rows, depending on the analytical task. By applying these techniques to data on the 200 largest Australian public companies in 2024, we created richer datasets that combined firm financials, industry classifications, and stock market performance.\nIn the next chapter, we will shift our focus to the formats in which data is stored and exchanged. We will identify common formats such as CSV, JSON, SQL, and Parquet, and consider when each is most appropriate for business use. We will practice loading, exploring, and manipulating data from flat files and nested JSON, and use tools like dbplyr and arrow to query data in DuckDB and Parquet.",
    "crumbs": [
      "Getting Our First Insights from Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Shaping and Combining Data for Business Analytics</span>"
    ]
  },
  {
    "objectID": "data_management/data_storage.html",
    "href": "data_management/data_storage.html",
    "title": "7  Storing and Using Existing Data",
    "section": "",
    "text": "7.1 The Business Challenge\nOnline marketplaces like eBay host millions of auctions each year, connecting buyers and sellers across countless product categories. Behind the scenes, this generates complex, nested, and high-volume data. For a business analyst, turning this raw data into useful insights—and storing it in a way that’s efficient, accessible, and reusable—is a foundational challenge.\nSuppose you’ve just joined the data team at this fast-growing online marketplace. Right now, every team — from marketing to data engineering and customer support — is handling auction data in different ways: some use Excel sheets, others parse logs from an API that stores information in a format called JSON, and data engineers keep things in a database (but no one else knows how to read them!).\nThis chaos is slowing everyone down. Your first task as a new analyst is to propose a better data storage solution that balances usability, scalability, and efficiency for teams across the business.\nBut how do you choose the right format? Should you push everyone to use Excel spreadsheets? Why not convince everyone to move everything into a database?\nBefore you decide, you’ll need to understand the strengths and trade-offs of each major data storage format. That is what we will cover in this chapter.",
    "crumbs": [
      "Finding and Storing Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Storing and Using Existing Data</span>"
    ]
  },
  {
    "objectID": "data_management/data_storage.html#the-business-challenge",
    "href": "data_management/data_storage.html#the-business-challenge",
    "title": "7  Storing and Using Existing Data",
    "section": "",
    "text": "About the Data\nTo guide your decision, we’ll work with a sample dataset of online auctions. Each auction record includes:\n\nItem metadata: ID, title, category, seller info\nSeller details: user ID, rating\nBidding history: a nested list of bids, each with a bidder ID, amount, and timestamp\n\nWe have stored and structured the dataset in multiple formats — Excel, flat files (CSV), JSON and DuckDB — so you can compare how different formats impact usability and performance. Along the way, you’ll learn not just how to load data, but how to think about storing and transforming it for long-term use in a business setting.\n\n\n\n\n\n\nNoteWhich of data formats have you heard of?\n\n\n\n\n\nWhich of the data formats that we will cover in this chapter have you already heard of?\n\n\nExcel\nCSV files\nA database\nJSON\n\n\nWhere did you learn about the formats you are aware of? What limitations have you found when using them?",
    "crumbs": [
      "Finding and Storing Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Storing and Using Existing Data</span>"
    ]
  },
  {
    "objectID": "data_management/data_storage.html#spreadsheets-excel-friends",
    "href": "data_management/data_storage.html#spreadsheets-excel-friends",
    "title": "7  Storing and Using Existing Data",
    "section": "7.2 Spreadsheets (Excel & Friends)",
    "text": "7.2 Spreadsheets (Excel & Friends)\nFor many of us, spreadsheets like Microsoft Excel or Google Sheets are the default way we have worked with data until starting our business analytics journey.\n\nReading Spreadsheets in R\n\n\n\n\n\n\n\n\nFigure 7.1: Auctions Data in Excel Format\n\n\n\n\n\nThe figure above shows what the spreadsheet we’re going to read into R looks like in Excel. We see that there is data across three separate sheets:\n\nauctions contains information about the each auction,\nbids contains information about bids in each auction; and\nsellers contains information about the seller of each item\n\nTo load the spreadsheet data into R, we use the readxl package, which allows us to read .xlsx files directly without requiring Excel to be installed.\n\n\n# A tibble: 6 × 8\n  item_id  title category start_time          end_time            seller_user_id\n  &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;    &lt;dttm&gt;              &lt;dttm&gt;              &lt;chr&gt;         \n1 AUC00000 Sams… Electro… 2025-07-19 00:00:00 2025-07-20 00:00:00 pray          \n2 AUC00001 Casi… Fashion  2025-04-24 00:00:00 2025-04-27 00:00:00 vgomez        \n3 AUC00002 Loui… Fashion  2025-03-20 00:00:00 2025-03-22 00:00:00 rodrigueznich…\n4 AUC00003 The … Books    2025-03-24 00:00:00 2025-03-26 00:00:00 tylercook     \n5 AUC00004 Mono… Toys & … 2025-08-14 00:00:00 2025-08-15 00:00:00 benjamin55    \n6 AUC00005 Dyso… Home & … 2025-06-25 00:00:00 2025-06-28 00:00:00 khuff         \n# ℹ 2 more variables: final_price &lt;dbl&gt;, winner_id &lt;chr&gt;\n\n\n\nlibrary(readxl)\n\n# Read a spreadsheet file\nauctions_excel &lt;- read_excel(\"data/ebay_auctions.xlsx\")\n\n# Peek at the first few rows\nhead(auctions_excel)\n\nWe can see that by default, R loads the auctions sheet which is the first sheet in the Excel workbook. If we want to specify the sheet we want to load, we can do that as follows:\n\n\n# A tibble: 6 × 4\n  item_id  bidder_id      bid_amount bid_time           \n  &lt;chr&gt;    &lt;chr&gt;               &lt;dbl&gt; &lt;dttm&gt;             \n1 AUC00000 istevens             94.6 2025-07-19 00:00:00\n2 AUC00000 mendozajasmine       97.7 2025-07-19 00:00:00\n3 AUC00000 james22             103.  2025-07-19 00:00:00\n4 AUC00000 wnunez              119.  2025-07-19 00:00:00\n5 AUC00000 paynejoshua         124.  2025-07-19 00:00:00\n6 AUC00000 yolanda40           127.  2025-07-19 00:00:00\n\n\n\n# Read a specific sheet\nexcel_bids &lt;- read_excel(\"data/ebay_auctions.xlsx\", sheet = \"bids\")\n\n# Or use sheet number\nexcel_bids &lt;- read_excel(\"data/ebay_auctions.xlsx\", sheet = 2)\n\nhead(excel_bids)\n\n\n\nIssues with Spreadsheets\nSpreadsheets are widely used in business and education, and offer an approachable interface for organizing tables, applying formulas, and making quick charts. This makes spreadsheets a great entry point into the world of data—but also a format with serious limitations when it comes to analytics at scale.\nSpreadsheets don’t record your steps, meaning you can’t always retrace how a certain number or figure was calculated. There’s no built-in version history (at least not one that’s easy to audit), and formulas often vary invisibly from cell to cell—especially if you’ve copied or dragged them across a range. This makes it difficult to ensure consistency and nearly impossible to reproduce or validate your results.\nEven small errors—like accidentally referencing the wrong column—can go unnoticed. A single cell with a broken formula can distort an entire analysis. These issues are especially problematic when figures or charts are being used for decisions or presentations.\nTo make matters worse, analysts often inherit spreadsheets from others—colleagues, clients, or external partners. These files might contain:\n\ninconsistent naming conventions\nmanual totals\nhidden formatting or comments\nmerged cells\nduplicated values across tabs\n\nall of which make using them for further analysis difficult.\nDespite these flaws, spreadsheets remain a powerful tool for quick exploration and lightweight collaboration. They’re useful for small teams, early prototyping, or downloading data from platforms like Shopify, Facebook Ads, or Google Analytics. But when your work moves toward reproducibility, scalability, or automation, it’s time to adopt formats designed for analysis.\n\n\nWriting to Excel\nDespite not wanting to use Excel files as an output, we might want to export the final results for an Excel workbook to pass on to a less technical colleague. To do this, we need the library writexl to save datasets to an Excel format. Let’s create a small data frame that we can then write out to a file. Specifically, lets filter all bids from the auction with the ID AUC00001 and save them in a file names small_bids.xlsx:\n\nlibrary(tidyverse)\nlibrary(writexl)\n\ncolnames(excel_bids)\n\nbids_to_export &lt;-\n    excel_bids |&gt; \n    filter(item_id == \"AUC00001\")\n\nwrite_xlsx(bids_to_export, \"small_bids.xlsx\")\n\n\n\n\n\n\n\nTipReading and Writing from Google Sheets\n\n\n\n\n\nGoogle Sheets is another widely used spreadsheet program. It’s free and web-based. Just like with Excel, in Google Sheets data are organized in worksheets (also called sheets) inside of spreadsheet files.\nYou can load data from a Google Sheet with the googlesheets4 package. This package is non-core tidyverse as well, you need to load it explicitly.\nThe first argument to read_sheet() is the URL of the file to read, and it returns a tibble: https://docs.google.com/spreadsheets/d/1V1nPp1tzOuutXFLb3G9Eyxi3qxeEhnOXUzL5_BcCQ0w. These URLs are not pleasant to work with, so you’ll often want to identify a sheet by its ID.\nlibrary(googlesheets4)\ngs4_deauth()\n\nsheet_id &lt;- \"1V1nPp1tzOuutXFLb3G9Eyxi3qxeEhnOXUzL5_BcCQ0w\"\ngoogle_df &lt;- read_sheet(sheet_id)\nYou can also write from R to Google Sheets with write_sheet(). The first argument is the data frame to write, and the second argument is the name (or other identifier) of the Google Sheet to write to:\n\nwrite_sheet(bids_to_export, ss = \"bids_auc10001\")\n\nIf you’d like to write your data to a specific work-sheet inside a Google Sheet, you can specify that with the sheet argument as well.\n\nwrite_sheet(bake_sale, ss = \"bids_auc10001\", sheet = \"bids\")\n\n\n\n\n\n\n\n\n\n\nCautionAuthentication with Google Sheets\n\n\n\n\n\nWhile you can read from a public Google Sheet without authenticating with your Google account and with gs4_deauth(), reading a private sheet or writing to a sheet requires authentication so that googlesheets4 can view and manage your Google Sheets.\nWhen you attempt to read in a sheet that requires authentication, googlesheets4 will direct you to a web browser with a prompt to sign in to your Google account and grant permission to operate on your behalf with Google Sheets. However, if you want to specify a specific Google account, authentication scope, etc. you can do so with gs4_auth(), e.g., gs4_auth(email = \"mine@example.com\"), which will force the use of a token associated with a specific email. For further authentication details, we recommend reading the documentation googlesheets4 auth vignette: https://googlesheets4.tidyverse.org/articles/auth.html.",
    "crumbs": [
      "Finding and Storing Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Storing and Using Existing Data</span>"
    ]
  },
  {
    "objectID": "data_management/data_storage.html#flat-files-csv-tsv-and-other-delimited-files",
    "href": "data_management/data_storage.html#flat-files-csv-tsv-and-other-delimited-files",
    "title": "7  Storing and Using Existing Data",
    "section": "7.3 Flat Files (CSV, TSV and other delimited files)",
    "text": "7.3 Flat Files (CSV, TSV and other delimited files)\nWhen we need to store and share structured data reliably, especially in a way that can be reproduced by code, an alternative format often works better: flat files. Flat files look similar to spreadsheets on the surface—just rows and columns of data—but under the hood they behave very differently. A flat file is stored as plain text. Each line represents a row of data, and the values are separated by a special character like a comma or tab. There are no formulas, no styling, no hidden or merged cells - just the raw information. There is another difference compared to Spreadsheets like Excel, flat files store each data set as a separate file, so there is no concept of multiple sheets if information stored in the same file.\nOne of the most common flat file types is the CSV, which stands for Comma-Separated Values. You have already encountered files with a .csv extension in the earlier chapter in this book, all the data we have worked with so far has been stored as CSV files.\nFor our auction example this means there are 3 CSV files, one each for auctions, bids and sellers\nBecause CSVs simply store raw information, we can see the content of the file when we open it in a text editor. Here’s what the first few lines of bids.csv looks like when opened in a text editor:\nWe can see that we can view and read the data with our own eyes. As such flat files are easy to inspect manually. The first row, commonly called the header row, gives the column names, and the following six rows provide the data. The columns are separated, aka delimited, by commas. They are also easy to edit, as we can manually add or edit rows and save the results if desired. These are not the only advantages of flat files, they are supported by nearly every data analytics tool so they are extremely portable which makes them a common default choice when exporting data from software such as R.\n\n\n\n\n\n\nWarningWhat do Excel Files Look Like in a Text Editor?\n\n\n\n\n\nNot all files can be viewed as cleanly as flat file formats such as csv in a text editor. If we tried to do the same thing, and view the first few lines of ebay_auctions.xlsx in a text editor, we will get the following output:\nThat’s not so useful!\n\n\n\n\n7.3.1 Reading Flat Files in R\nIn R, we can use the readr package to import flat files quickly and reliably. We can read this file into R using read_csv(). The first argument is the most important: the path to the file. You can think about the path as the address of the file: the file is called bids.csv and it lives in the data folder.\n\nlibrary(readr)\n# Read a CSV file into a tibble\nbids_flat &lt;- read_csv(\"data/bids.csv\")\n\n\n\nRows: 13414 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): item_id, bidder_id\ndbl  (1): bid_amount\ndttm (1): bid_time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWhen you run read_csv(), it prints out a message telling you the number of rows and columns of data, the delimiter that was used, and the column specifications (names of columns organized by the type of data the column contains). It also prints out some information about retrieving the full column specification and how to quiet this message.\n\n\n\n\n\n\nTipDelimiters and File Extensions\n\n\n\nThe character that separates columns in a flat file is called a delimiter. Common ones include:\n\nComma (,) — for .csv files\nTab ( — for .tsv files\nPipe (|) — occasionally used for exports from databases\n\nIf we wanted to load the the file bids.tsv which uses Tab separated values:\n\nhead ../data/bids.tsv\n\nWe would use the read_delim() function and specify that columns are tab separated:\n\nbids_tab &lt;- read_delim(\"data/bids.tsv\", delim = \"\\t\")\n\n\n\n\n\n7.3.2 Writing to a file\nreadr also comes with functions for writing data back to disk: write_csv() and write_delim(). The most important arguments to these functions are the data frame we want to save and the location where we want to save the file. Suppose we want to take the bids from AUC0001 and save them in a CSV file bids_auc0001.csv located in the data directory:\nauctions_filtered &lt;-\n    bids |&gt; \n    filter(item_id == \"AUC0001\")\n    \nwrite_csv(auctions_filtered, \"data/bids_auc0001.csv\")",
    "crumbs": [
      "Finding and Storing Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Storing and Using Existing Data</span>"
    ]
  },
  {
    "objectID": "data_management/data_storage.html#json-files-when-rectangular-data-might-not-be-enough",
    "href": "data_management/data_storage.html#json-files-when-rectangular-data-might-not-be-enough",
    "title": "7  Storing and Using Existing Data",
    "section": "7.4 JSON Files: When rectangular data might not be enough",
    "text": "7.4 JSON Files: When rectangular data might not be enough\nFlat files are great when every observation fits neatly into one row. But sometimes our dataset comes with a sort of hierarchy, its mini-datasets nested inside a larger dataset. In our bids data, for example, one could imagine rather than having each row of data being one bid from one auction, and storing the item_id in each row along with the bid information, we could think about storing the set of bids for each auction as a mini-dataset attached to each item ID.\nTo handle these cases, we can use nested data formats — and one of the most popular is JSON. JSON (JavaScript Object Notation) is a format designed to store structured data. It can be especially useful when one record contains multiple layers of information—for example, an auction that has several bids attached to it as we have here.\nUnlike flat files, JSON allows us to store all related data in one object, without repeating fields. It’s human-readable, flexible, and widely used in real-world business data systems.\nJSON stores data as a set of key-value pairs. Think of it like a list of labeled boxes — each label is the name of a variable, and inside each box is the value. In the context of our auction dataset, one auction might look like this in JSON format:\n[\n  {\n    \"item_id\": \"AUC00000\",\n    \"title\": \"Samsung Galaxy S22\",\n    \"category\": \"Electronics\",\n    \"start_time\": \"2025-07-19T08:04:31.226153\",\n    \"end_time\": \"2025-07-20T08:04:31.226153\",\n    \"seller\": {\n      \"user_id\": \"pray\",\n      \"rating\": 4.63,\n      \"country\": \"AE\"\n    },\n    \"bids\": [\n      {\n        \"bidder_id\": \"istevens\",\n        \"amount\": 94.61,\n        \"time\": \"2025-07-19T10:41:31.226153\"\n      },\n      {\n        \"bidder_id\": \"mendozajasmine\",\n        \"amount\": 97.73,\n        \"time\": \"2025-07-19T13:27:31.226153\"\n      },\n      {\n        \"bidder_id\": \"james22\",\n        \"amount\": 103.28,\n        \"time\": \"2025-07-19T16:10:31.226153\"\n      },\n      {\n        \"bidder_id\": \"wnunez\",\n        \"amount\": 119.04,\n        \"time\": \"2025-07-19T16:34:31.226153\"\n      }\n    ],\n    \"final_price\": 119.04,\n    \"winner_id\": \"wnunez\"\n  }\n]\nLet’s break it down:\n\nThe top-level keys (item_id, title, category) describe the auction itself.\nThe seller key contains a nested object with seller details.\nThe bids key holds a list of bid objects, each with its own bidder, amount, and time.\n\nThis structure keeps all related information grouped together. That’s (one of) the power(s) of JSON: it stores complex relationships without flattening the data.\n\n\n\n\n\n\nNoteJSON data does NOT have to be nested\n\n\n\n\n\nJust because JSON data can feature nesting structures does not mean that it has to. JSON data can have a flat structure. An example of a flat JSON dataset, that stores key value-pairs for two auctions is:\n[\n  {\n    \"item_id\": \"AUC10001\",\n    \"title\": \"Nike Air Force 1\",\n    \"category\": \"Fashion\",\n    \"start_time\": \"2025-08-15T09:00:00\",\n    \"end_time\": \"2025-08-17T09:00:00\",\n    \"seller_user_id\": \"alvarez\",\n    \"seller_rating\": 4.87,\n    \"seller_country\": \"US\",\n    \"final_price\": 145.50,\n    \"winner_id\": \"sophiaw\"\n  },\n  {\n    \"item_id\": \"AUC10002\",\n    \"title\": \"LEGO Star Wars Millennium Falcon\",\n    \"category\": \"Toys & Games\",\n    \"start_time\": \"2025-08-20T14:30:00\",\n    \"end_time\": \"2025-08-22T14:30:00\",\n    \"seller_user_id\": \"zhangwei\",\n    \"seller_rating\": 4.95,\n    \"seller_country\": \"CN\",\n    \"final_price\": 349.99,\n    \"winner_id\": \"marklee\"\n  }\n]\nHere the data is just a flat list of records, where each attribute sits side-by-side in a similar manner to a row of a flat file.\n\n\n\n\nWorking with Nested JSON Auction Data in R\nNext, we’ll learn how to work with nested JSON data. Each auction record includes top-level information (like item title and category), a nested list of seller information, and a nested list of bids placed by users. Our goal is to explore how to load and manipulate this nested structure using our tidyverse tools. We’ll move step by step, from the raw JSON to a tidy dataset.\n\nStep 1: Load the JSON File\nWe’ll start by reading in the data using the read_json() function from the jsonlite package. This preserves the nested list structure exactly as it appears in the JSON file.\n\nlibrary(jsonlite)\n\nauctions_nested &lt;- read_json(\"data/ebay_auctions.json\")\n\ntypeof(auctions_nested)\n\nThe object auctions_nested is a list of auction records.\n\n\nStep 2: Peek Inside a Single Record\nLet’s take a closer look at the structure of the first auction record. If we inspect the first element of the list, can see the keys associated with the auction:\n\nnames(auctions_nested[[1]])\n\nWe look at one of the top-level lists, such as the title which stores the type of product being auctioned:\n\nauctions_nested[[1]]$title\n\nAnd if wanted to peek into the one of the the keys that contains a nested object, such as bids, we can see the names of the keys inside the bids list:\n\nnames(auctions_nested[[1]]$bids[[1]])\n\nAnd if we wanted to see the amount bid:\n\nauctions_nested[[1]]$bids[[1]]$amount\n\nThus, what we see here is when we load the data into R using read_json the hierarchical structure is preserved.\n\n\nStep 3: Wrap the List into a Tibble\nTo start working with this in tidyverse pipelines, we’ll wrap the list of auctions into a tibble with one column:\n\nauctions_tbl &lt;- tibble(record = auctions_nested)\n\nEach row of auctions_tbl now contains one full auction record:\n\nhead(auctions_tbl)\n\nand there are 1000 rows of auction data:\n\nnrow(auctions_tbl)\n\n\n\n7.4.0.1 Step 4: Rectangling non-rectangular data\nNow our task is to enagage in “data rectangling”: taking data that is fundamentally hierarchical, or tree-like, and converting it into a rectangular data frame made up of rows and columns. To start rectangling the data, we want to take the top level keys from our JSON data and convert them to columns of data. We make this conversion by unnesting the lists we have stored in the record column, and making our data wider, thus the function we want to use is unnest_wider(). This process of removing hierarchy is also known as flattening the data. Let’s do it:\n\nauctions_tbl &lt;- \n    auctions_tbl |&gt; \n    unnest_wider(record)\n\nglimpse(auctions_tbl)\n\nNow we have columns like item_id, title, category, seller, and bids, and our data looks more rectangular than it did prior to this unnesting step. However, both seller and bids are still lists, we need to engage in further flattening.\nWhen each row has the same number of elements with the same names, like with seller, it’s natural to further widen the data and put each component into its own column with a second application of unnest_wider(). When we do, the new columns will have a two-part name, seller-&lt;SOMETHING&gt;, because we are unnesting a list from the seller column. The  will be the name of each of the keys within the sublist. We will separate the two part name with an underscore:\n\nauctions_tbl &lt;- \n    auctions_tbl |&gt; \n    unnest_wider(seller, names_sep = \"_\")\n\nglimpse(auctions_tbl)\n\nNow seller information like user_id and rating is available as separate columns.\n\n\nStep 5a: Flattening the Bid Lists to Columns\nWhat about the bids column? Each auction has its own list of bids, and each bid is a small list too. A natural next step, given what we have done above would be to break it out into wide format:\n\nauctions_tbl_wide &lt;- \n    auctions_tbl |&gt; \n    unnest_wider(bids, names_sep = \"_\")\n\nglimpse(auctions_tbl_wide)    \n\nThis gives us one column for each bid: bids_1, bids_2, etc. But these are still lists. We start to unnest them as well:\nauctions_tbl_wide &lt;- \n    auctions_tbl_wide |&gt; \n    unnest_wider(bids_1, names_sep = \"_\", names_repair = \"unique\") %&gt;%\n    unnest_wider(bids_2, names_sep = \"_\", names_repair = \"unique\")\nThis gets tedious if there are many bids (and tough if we didn’t know the number of bids per auction). This might suggest that continuing to widen the data is not the ‘right’ approach. As we’ve advocated throughout the book, when we have to manually repeat actions, the structure of the data we are working with, or trying to create, is likely not the most natural one.\n\n\n\n\n\n\nTipContinuing to widen the data\n\n\n\n\n\nSuppose you really wanted to keep widening the data via unnest_wider() even though the approach might now be first-best. How would we do it? We’d need to follow these steps:\n\nDetermine the names of all the bids_ columns, ideally in an automated way that R can do for us\nApply a function that takes all of these column names in the auctions_tbl_wide data and apply unnest_wider on them.\n\nHere’s how you can do that. It uses the reduce() function in Step 2 to apply a function iteratively that unnest’s each of the columns we identified in the first.\n\n# Step 1: Find all column names that \n# we want to apply unnest_wider() to\nbid_cols &lt;- \n    auctions_tbl_wide |&gt; \n    select(starts_with(\"bids_\")) |&gt; \n    select(where(is.list)) |&gt; \n    names()\n\nauctions_tbl_wide &lt;- \n    reduce(\n        bid_cols,\n        .init = auctions_tbl_wide,\n        .f = function(df, col) {\n            unnest_wider(df, \n                         !!sym(col), \n                         names_sep = \"_\", \n                         names_repair = \"unique\"\n                         )\n            }\n    )\n\nglimpse(auctions_tbl_wide)\n\nDon’t worry, we won’t expect you to remember those steps. That’s a pretty advanced pipeline. But it shows this route can be pursued if need be.\n\n\n\n\n\nStep 5b: Unnesting bids to new rows\nSo far, our rectangling has made the data wider. But rectangles have two dimensions, width and length. Thus, instead of going wider, we can go longer. This means we unnest the bids data into new rows, with each new row being one bid, via the unnest_longer() function. The beauty in this is that we do not need to know the number of new rows we need to create per auction:\n\nauctions_tbl_long &lt;- \n    auctions_tbl |&gt; \n    unnest_longer(bids) \n\nglimpse(auctions_tbl_long)\n\nNow we have a tidy table where each row is one bid and it includes the item ID and other metadata. However, the bids column is still a list. But now each list is simply the of one bid. We can unnest bids list wider, to get each of the bid’s characteristics as new columns:\n\nauctions_tbl_long &lt;-\n    auctions_tbl_long |&gt; \n    unnest_wider(bids, names_sep = \"_\")\n\nglimpse(auctions_tbl_long)",
    "crumbs": [
      "Finding and Storing Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Storing and Using Existing Data</span>"
    ]
  },
  {
    "objectID": "data_management/data_storage.html#databases-with-duckdb",
    "href": "data_management/data_storage.html#databases-with-duckdb",
    "title": "7  Storing and Using Existing Data",
    "section": "7.5 Databases with DuckDB",
    "text": "7.5 Databases with DuckDB\nA huge amount of business data lives in databases, so it’s important that you know how to access it. The reason lots of business data lives inside of a database, is that databases are efficient at storing the data and there is a widely used language, known as SQL (Structured Query Language), for performing data wrangling on databases. Luckily for us, our knowlegde of R and the tidyverse in particular, means we have most of tools we need to work with data that lives in pre-existing databases. This is because the packages within the tidyverse know how to translate and run dplyr commands on a database. This means that once we know how to connect to a database, we can apply the knowledge from previous chapters to extract insights with relative ease.\n\n\n\n\n\n\nTipDatabase Management Systems\n\n\n\n\n\nDatabases are run by database management systems (DBMS’s for short), which come in three basic forms:\n\nClient-server DBMS’s run on a powerful central server, which you connect to from your computer (the client). They are great for sharing data with multiple people in an organization. Popular client-server DBMS’s include PostgreSQL, MariaDB, SQL Server, and Oracle.\nCloud DBMS’s, like Snowflake, Amazon’s RedShift, and Google’s BigQuery, are similar to client server DBMS’s, but they run in the cloud. This means that they can easily handle extremely large datasets and can automatically provide more compute resources as needed.\nIn-process DBMS’s, like SQLite or duckdb, run entirely on your computer. They’re great for working with large datasets where you’re the primary user.\n\n\n\n\n\nConnecting to a database\nTo connect to a database from R, you’ll use a pair of packages:\n\nDBI as a database interface\nduckdb as database engine that stores and queries data in the database\n\nWhenever you work with a database in R, you’ll always use DBI (database interface) because it provides a set of generic functions that connect to the database, upload data, and run queries. duckdb is a high-performance database that’s designed very much for the needs of a data scientist. We use it here because it’s very easy to get started with, but it’s also capable of handling gigabytes of data with great speed. Depending on your some specifics of the database you are looking to connect to, you may need to use a different engine. However, the the only difference between using duckdb and any other DBMS is how you’ll connect to the database. This makes it great to teach with because you can easily run this code as well as easily take what you learn and apply it elsewhere.\nThe database we want to connect to is the auctions data is located in the file data/auctions.duckdb. We want to use the duckdb as the engine with which we interface with the data:\n\nlibrary(DBI)\nlibrary(duckdb)\n\ncon &lt;- dbConnect(duckdb(), dbdir = \"data/auctions.duckdb\")\n\nprint(con)\n\nGreat. Now we have a connection called con. As we move forward, we’ll always have to use this connection to interact with the database. For example, if we want to see which tables of data are stored in our database, we can use the dbListTables() function:\n\ndbListTables(con)\n\nWhich shows we have three distinct tables of data stored. This is kind of like what we had with our Excel spreadsheet, multiple sheets of data living inside one location. If we then want to look at the contents of each table, we can ask R to return to us a glimpse of one of the tables:\n\ntbl(con, \"bids\") |&gt; \n    glimpse()\n\nIn the code above, we first had to use tbl() to create an object that represents a database table. Whenever we want to work with a table in the database, we will need to use the tbl() function with the connection con as its first argument. You can tell this object represents a database query because it prints the DBMS name at the top, and while it tells you information about each of of columns, it typically doesn’t know the exact number of rows. This object is lazy; and finding the total number of rows of a table stored in a database (or as the result of a set of data-wrangling tasks) could be (very) time consuming. Thus by default, we only see you a small preview of the data — a handful of rows along with the column definitions — while postponing the actual work of counting or retrieving everything until we explicitly ask for it.\n\n\n\n\n\n\nWarningClosing Your Database connection\n\n\n\nWhen you have finished working with your database, you should close the connection:\n\ndbDisconnect(con)\n\n\n\n\n\ndbplyr: dplyr for databases\nNow that we’ve connected to a database and loaded up some data, we can start to wrangle with the data. Data wrangling with databases in the tidyverse uses a package called dbplyr. dbplyr is a dplyr backend, which means that we keep writing dplyr code but behind the scenes executes it differently. In our database, dbplyr translates the code to SQL - the language for operating on structured databases.\nLet’s now complete a task that is all too familiar in this chapter, selecting all bids from the auction labelled AUC0001. The dplyr code that would do that for us is:\n\nbids_filtered &lt;-\n    tbl(con, \"bids\") |&gt;\n    filter(item_id == \"AUC00001\") \n\nglimpse(bids_filtered)\n\nHmmmm, nothing gets returned here. This comes back to the lazyness of the tbl() command. If we want our code to be executed against the database, we have to add a collect() command as a final step:\n\nbids_filtered &lt;-\n    tbl(con, \"bids\") |&gt;\n    filter(item_id == \"AUC00001\") |&gt; \n    collect()\n\nglimpse(bids_filtered)\n\nThis matches what we had in previous sections, as expected.\n\n\n\n\n\n\nTipWhat’s happening behind the scenes?\n\n\n\n\n\nBehind the scenes, dplyr generates the SQL code that is equivalent to what we wrote, then calls dbGetQuery() to run the query against the database, then turns the result into a tibble.\nTo see this, we can use show_query() to see the SQL version of our dplyr code:\n\nquery &lt;- \n    tbl(con, \"bids\") |&gt;\n    filter(item_id == \"AUC00001\")\n\nshow_query(query)\n\nand then run that SQL query against the database:\n\ndbGetQuery(con, \"SELECT * FROM bids WHERE (item_id = 'AUC00001')\")\n\nwhich yields exactly the same dataset.\n\n\n\nTypically, you’ll use dbplyr to select the data you want from the database, performing basic filtering and aggregation using the translations described below. Then, once you’re ready to analyse the data with functions that are unique to R, you’ll collect() the data to get an in-memory tibble, and (possibly) continue your work with pure R code.\nTo see this in action, let’s wrangle the data in the database to produce a table very similar to the one we produced by flattening the JSON data.\n\n\n7.5.1 Working with the Auctions database\nTo start to aggregate our data into one table, we can join the data in the auctions table to the seller information in sellers. We will do this using a left_join(). First let’s look at each of the datasets structures to find a common variable to join on:\n\ntbl(con, \"auctions\") |&gt; \n    glimpse()\n\n\ntbl(con, \"sellers\") |&gt; \n    glimpse()\n\nSo we can use seller_user_id as the common variable to join on:\n\nauctions_sellers &lt;-\n    tbl(con, \"auctions\") |&gt;\n    left_join(tbl(con, \"sellers\"),\n                by = \"seller_user_id\")\n\nNotice how we do not use a collect() statement, because we do not yet need to return the results. We only need the results once we join this data to the bids data. To join auctions_sellers to the bids data, we use the item_id as the common key:\n\nres &lt;-\n    auctions_sellers |&gt; \n    left_join(tbl(con, \"bids\"),\n              by = \"item_id\") |&gt; \n    collect()\n\nglimpse(res)\n\nWhich has the same rows and columns as the rectangled data from the JSON file.",
    "crumbs": [
      "Finding and Storing Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Storing and Using Existing Data</span>"
    ]
  },
  {
    "objectID": "data_management/data_storage.html#wrap-up-which-format-would-you-use",
    "href": "data_management/data_storage.html#wrap-up-which-format-would-you-use",
    "title": "7  Storing and Using Existing Data",
    "section": "7.6 Wrap-up: Which Format Would You Use?",
    "text": "7.6 Wrap-up: Which Format Would You Use?\nNow that we’ve seen four alternative data storage formats:\n\nSpreadsheets\nFlat Files\nJSON\nDatabases\n\nwhich one do you think is best for the online auction data we have here?\nIn the next chapter, rather than working with existing data we are going to explore one method of collecting new data called webscraping. We’ll look to harvest data from websites and then store the information for future analysis.",
    "crumbs": [
      "Finding and Storing Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Storing and Using Existing Data</span>"
    ]
  },
  {
    "objectID": "data_management/data_collection.html",
    "href": "data_management/data_collection.html",
    "title": "8  Collected Structured and Unstructured Data",
    "section": "",
    "text": "8.1 The Business Challenge\nData is often called the “new oil” of the digital economy. Yet, just as not all oil flows neatly through pipelines, not all data comes in ready-to-use formats. Some data is nicely organized in databases, spreadsheets, or CSV files, while other data appears in messy, irregular, or unconventional formats.\nSo far in this subject, we have mainly worked with structured data. In practice, however, such well-organized data is something of a luxury. It is costly to collect, standardize, and store, and organizations often invest heavily in information systems just to maintain it. At the same time, vast amounts of potentially valuable data exist outside these traditional formats.\nIn this chapter, we will explore structured, semi-structured, and unstructured data. You will learn the defining characteristics of each category, why they matter for business analytics, and how analysts can collect and make use of data from different sources.",
    "crumbs": [
      "Finding and Storing Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Collected Structured and Unstructured Data</span>"
    ]
  },
  {
    "objectID": "data_management/data_collection.html#structured-semi-structured-and-unstructured-data",
    "href": "data_management/data_collection.html#structured-semi-structured-and-unstructured-data",
    "title": "8  Collected Structured and Unstructured Data",
    "section": "8.2 Structured, Semi-structured, and Unstructured Data",
    "text": "8.2 Structured, Semi-structured, and Unstructured Data\nJust as crude oil must be refined before it becomes useful, raw data only generates value when it can be systematically collected, processed, and analyzed. Depending on how well-defined its format is, data is typically classified into three broad categories:\n\nStructured data\nUnstructured data\nSemi-structured data\n\n\nStructured Data\nStructured data, as the name suggests, refers to highly organized data that follows a predefined schema. It is stored in formats such as relational databases, spreadsheets, and CSV files, where every piece of data is clearly defined and fits into a particular row and column. Most data that we have worked with so far in this subject belongs to this category.\nA well-defined structure for data often consists of:\n\nRows (Records, Observations): Each row corresponds to one unique instance of the entity being described (e.g., a single customer, a single sales transaction).\nColumns (Fields, Variables): Columns represent attributes or characteristics of the entity (e.g., customer age, transaction amount).\nTables: Data is organized into tables that can be linked to each other through shared variables or keys (e.g., linking different customer data using customer ID).\n\nBecause of their consistency, structured datasets can be easily managed and analyzed with tools like SQL, R’s Tidyverse, or Python’s Pandas. These tools allow us to perform operations easily on structured data by doing filtering, grouping, aggregating, merging, and reshaping data efficiently.\n\n\n\n\n\n\nNoteExamples\n\n\n\n\nBanking transaction records (date, amount, sender, recipient)\nInventory databases (product ID, stock quantity, unit price)\nEmployee HR records (employee ID, salary, department)\n\n\n\nStructured data’s strength lies in its precision and reliability. However, it is also limited in scope and volume. Many real-world data and information, such as consumer opinions, video content, or machine logs, cannot be easily captured in neat rows and columns.\n\n\nUnstructured Data\nIn reality, structured data represents only a small fraction of all data. As predicted by International Data Corporation (IDC) and Seagate, the global data endowment will continue growing exponentially and reach 163 zettabytes (i.e., 1 trillion gigabytes) by the end of 2025, and among all data, more than 80% is unstructured.\nUnstructured data, in stark contrast to structured data, lacks a fixed or consistent schema. It is generated in diverse formats and is often messy, irregular, or context-dependent. Broadly speaking, all information that we generate, unless it comes with predefined structures, belongs to unstructured data. Unlike structured data, it cannot be easily represented in tables of rows and columns.\n\n\n\n\n\n\nNoteExamples\n\n\n\n\nTextual data: emails, chat messages, social media posts, product reviews.\nMultimedia data: images, audios, videos, surveillance cameras recordings, readings/images from medical devices.\n\n\n\nUnstructured data is pervasive because much of the information we generate in daily life does not naturally follow a rigid, tabular structure. It posits significant challenges in collecting and processing data:\n\nLack of standardization.\n\nUnstructured data is in very different categories. Even within the same category, unstructured data may be recorded in different formats.\nComputers and programs cannot easily tackle data without standardized formats and patterns,\n\nVolume and variety\n\nThe scale of unstructured data is massive: billions of social media posts per day, countless images and videos uploaded online, etc.\nThe variety of formats requires specialized tools for each type (e.g., natural language processing for text, computer vision for images).\n\nCollection challenges\n\nUnstructured data is often dispersed across platforms and devices. Collecting it may involve:\n\nMassive manual work (i.e., human labelling of data, CAPTCHA)\nWeb scraping (can be technically challenging and sometimes legally restricted)\nLarge storage infrastructures (extremly costly)\n\n\nProcessing requirements\n\nBefore analysis, unstructured data usually must undergo heavy preprocessing and transformation (e.g., machine learning algorithms and AI):\n\nText: tokenization, stop-word removal, stemming, sentiment analysis (note that Google BERT and GPT have largely facilitated the processing of texts and natural language)\nImages: resizing, labeling, feature extraction\nAudio: noise reduction, speech-to-text transcription\n\n\n\n\n\nSemi-Structured Data\nBetween these two extremes is semi-structured data. Different from structured data, semi-structured data does not fit neatly into tables, but it still carries markers or tags that provide some structure. Compared with purely unstructured data, semi-structured data has a flexible but interpretable format.\n\n\n\n\n\n\nNoteExamples\n\n\n\n\nJSON file (as we discussed in the previous week).\nHTML documents: HTML tags (&lt;div&gt;, &lt;h1&gt;, &lt;p&gt;) give structures, but the actual text and images inside the tags are unstructured.\nEmails: contain structured fields (sender, recipient, timestamp, subject) but also unstructured free-form text in the body.\nChat logs: often include metadata (user ID, time) plus free text content.\n\n\n\n\n\n\n\n\n\nTipSummary\n\n\n\n\n\n\nStructured data is easy to store, query, and analyze, but costly to maintain and often limited in scope.\nUnstructured data is abundant and rich but difficult to process without specialized tools.\nSemi-structured data sits in the middle: it provides some structure but still requires significant preprocessing before analysis.\n\n\n\n\n\n\n\n\n\n\nTipRemark\n\n\n\nFor structured data, we have databases to manage them and apply to them relatively standardized data wrangling tools (e.g., dplyr). However, we do not have such luxury for less structured data. The ways of collecting and using less structured data usually vary across different cases. Do not be panic if the methods that we use in our examples do not work in a different case. What matters to us is to get familiar with the workflow.",
    "crumbs": [
      "Finding and Storing Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Collected Structured and Unstructured Data</span>"
    ]
  },
  {
    "objectID": "data_management/data_collection.html#getting-data-using-application-program-interface-api",
    "href": "data_management/data_collection.html#getting-data-using-application-program-interface-api",
    "title": "8  Collected Structured and Unstructured Data",
    "section": "8.3 Getting Data Using Application Program Interface (API)",
    "text": "8.3 Getting Data Using Application Program Interface (API)\nIn library, we may not always be able to go and look for the books directly - especially when some books may be stored in the staff-only area or difficult to find. In such cases, we can submit requests to a librarian, and the librarian gets the books for us from the shelves.\nThis process is a nice analogy to what happens when we acquire data via application program interface (API):\n\nYou are a data user\nThe librarian is the API\nThe staff-only area is the internal system or database you don’t have access to\nThe request for a book is the message your program sends to the API\nThe book the librarian brings back is the data or service the API provides\n\n\nDefinitions of API\nTechnically speaking, Application program interface (API) builds a “connection” between computer and computer programs and specifies a standard set of rules or procedures that an application program will do. This is in contrast to user interfaces (UI) where users directly interact with computers or programs.\n\n\n\n\n\n\nNoteExample: Uber’s Usage of Google Map API\n\n\n\nWe use web-based Google Map or Google Map App to search for locations and navigate to places we want to go. In such scenarios, we, as human users, are directly interact with the user interface (UI) of Google Map to acquire data and information.\nAs a giant in ride-share and door-to-door delivery services, Uber uses Google Map to obtain data and information that they need for their apps (e.g., map, address information, routes, estimated time of arrival, etc.). In contrast to our usage of Google Map, Uber gets access to Google Map data by using programs to interact with Google Map’s application program interface (API). This allows Uber to get real-time data from Google Map and use such data in real time. We cannot imagine Uber would be able to kick start its business without the Google Map API.\nOf course, the access to Google Map via API is not free. Uber pays millions to Google in order to use their services.\n\n\n\n\n\nAdvantages of APIs\nNowadays, many data providers set up APIs as a preferred way to provide data to data users, and there are several advantages of doing so:\n\nControlled Access. APIs allow data providers to share only selected data or functions, rather than giving users full access to internal systems and data servers. This helps protect sensitive data and system integrity.\nStandardization. By providing a standardized way to access data, APIs reduce the need to handle custom requests or build separate solutions for each user, thus reducing the costs.\nInnovation and Ecosystem Growth. APIs allow third-party developers to build new apps or services that enhance the value of data (e.g., Google Maps used in ride-sharing apps).\nUsage Tracking and Monetization. APIs make it easy to monitor who is using the data and how often. This opens up possibilities for charging based on usage or offering premium tiers.\n\n\n\nWorkflow of Using APIs to Acquire Data\nMost APIs are user-friendly because the rules and protocols of data acquisition have already been set up by data providers. Although every API may differ significantly from the other in many ways. Standard workflow still applies to the usage of APIs.\nWe are going to use the “Music to Scrape” website (https://music-to-scrape.org/) to walk through the usage of APIs.\n\nRead User Manuals\nAPIs are structured differently for different functions and needs. In practice, every API may have its own predefined functions and specific requirements for inputs and outputs. Therefore, it is rather important to carefully read through API user manuals before starting using it. Additionally, data providers often provide detailed user manuals and sample program for their APIs. As a user, it would be useful to go through these materials (if available).\n\n\nObtaining API Key\nIn many cases, the access to APIs is limited - you need to get an API key so that you can use the API. We can think API key as the log-in credentials for programs. Data providers can use API keys to track the download and the usage of data and prevent unwarranted access to their data.\nFor the simple website we are working on, there is no need to obtain an API key. In the workshop, we are going to apply for a free API key from the Federal Reserve Economic Data (FRED).\n\n\nUsing APIs\nBefore proceed with programming, we always start by reading API manuals carefully (https://api.music-to-scrape.org/docs). We are going to follow the example provided by the website to extract top-tracts for Week 42 of 2023.\nThe httr package in R is used for working with APIs. It provides tools that let your R program send requests to websites or servers and receive data in return. Note that, not all APIs use httr. You need to follow closely to the user manual of the API that you are using.\n\n# Load the required library\nlibrary(httr)\nlibrary(dplyr)\n\n# Specify the URL of the API\napi_url &lt;- \"https://api.music-to-scrape.org\"\n\n# Remember, the API documentation is available at https://api.music-to-scrape.org/docs\n\n# We are going to extract top ranked songs played in Week 42 of 2023\n\n# Set up the url to get response from (Closely follow the API documentation)\nresponse &lt;- GET(paste0(api_url, '/charts/top-tracks?week=42&year=2023'))\n\n# Check if the request was successful\nif (response$status_code == 200) {\n    # If successful (code == 200), parse the responses  into lists and then combine into data\n    # Parse the JSON response\n    data &lt;- content(response, \"parsed\")\n    \n    # Compile data in a table and select columns\n    song_data &lt;- data$chart %&gt;% bind_rows() %&gt;% select(name, artist)\n    \n} else {\n    # Error message for unsuccessful request\n    cat(\"Failed to retrieve data. Status code:\", response$status_code, \"\\n\")\n    song_data &lt;- NULL\n}\n\n# View the resulting song data\nprint(song_data)",
    "crumbs": [
      "Finding and Storing Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Collected Structured and Unstructured Data</span>"
    ]
  },
  {
    "objectID": "data_management/data_collection.html#website-scraping",
    "href": "data_management/data_collection.html#website-scraping",
    "title": "8  Collected Structured and Unstructured Data",
    "section": "8.4 Website Scraping",
    "text": "8.4 Website Scraping\nIt may not always be feasible to obtain data from well-structured databases or APIs. In fact, plenty of useful data could be from unstructured sources. These data could be from newspapers, surveys,\nSometimes, we may be interested in the data and information on websites and would like to obtain these data in a batch. Website scraping is the technique that allows us to do so.\n\nWebsite Basics\nWebsites are one of the most important sources for us to obtain data that are not available from structured databases. For example, we may be interested in It may not be difficult for us to manually copy and paste inforamtion and data from websites if we are dealing with a small number of data points\nHTML (Hypertext Markup Language) is the programming language used for building up a website. It defines how each website is displayed and presented on website browsers. Basic knowledge of HTML would help us understand website structures and master basic techniques for website scraping. Note that the technical details of HTML is not required for this subject.\nAlthough websites appear very different from each other - some websites are full of fancy animation and decorations, while others are plain and simple, they are constructed using the same set of building blocks, HTML elements. HTML use a standard set of tags to identify each HTML element on a website (sections, divisions, headings of different levels, paragraphs, image blocks etc.). As these tags are standardized across different websites, we are able to extract the contents\nBelow is a simple example of HTML file, and we are going to briefly elaborate on the building blocks of websites.\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n  &lt;head&gt;\n    &lt;title&gt;My First Page&lt;/title&gt;\n    &lt;style&gt;\n      body {\n        font-family: Arial, sans-serif;\n        line-height: 1.6;\n        margin: 20px;\n        background-color: #f9f9f9;\n      }\n\n      h1 {\n        color: darkblue;\n        text-align: center;\n      }\n\n      section {\n        margin-bottom: 30px;\n        padding: 20px;\n        background-color: #ffffff;\n        border: 1px solid #ddd;\n        border-radius: 8px;\n      }\n    &lt;/style&gt;\n  &lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;h1&gt;Welcome to My Website!&lt;/h1&gt;\n\n    &lt;section id=\"section1\"&gt;\n      &lt;h2&gt;About This Site&lt;/h2&gt;\n      &lt;div&gt;\n        &lt;p&gt;This is a paragraph of text.&lt;/p&gt;\n        &lt;p&gt;Because I wanted to learn HTML and share my interests.&lt;/p&gt;\n      &lt;/div&gt;\n    &lt;/section&gt;\n\n    &lt;section id=\"section2\"&gt;\n      &lt;h2&gt;Gallery&lt;/h2&gt;\n      &lt;div&gt;\n        &lt;h3&gt;What You'll Find Here&lt;/h3&gt;\n        &lt;p&gt;Pictures, stories, and ideas I enjoy.&lt;/p&gt;\n        &lt;img src=\"photo.jpg\" alt=\"A photo of a sunset\"&gt;\n      &lt;/div&gt;\n    &lt;/section&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n\nHTML Tags and Elements\nHTML documents are simply semi-structured data. HTML documents use “tags” to wrap and identify contents. These tags are the fundamental building blocks of a webpage’s structure. Tags (often come in pairs) tell the browser what each element is and how it should be organized.\n\n\n\n\n\n\nNoteExample\n\n\n\nThis is a simple HTML document:\n&lt;h1&gt;Title&lt;/h1&gt;\n&lt;p&gt;This is a paragraph.&lt;/p&gt;\n&lt;img src=\"cat.jpg\" alt=\"A cat\"&gt;\n\n&lt;h1&gt; &lt;\\h1&gt;: wrap the title and specify it as the header (level 1).\n&lt;p&gt; &lt;\\p&gt;: wrap the contents of a paragraph.\n&lt;img&gt; specifies an image to be inserted.\n\n\n\nBelow is a more complete list of commonly seen HTML tags:\n\n&lt;!DOCTYPE html&gt; is the start for an HTML file, specifying that the format of the document is in HTML.\n&lt;head&gt; and &lt;/head&gt; are used to define the title of a website.\n&lt;style&gt; and &lt;/style&gt; are inside the heading of an HTML file to specify the appearance of each part (using Cascade Style Sheets (CSS)).\n&lt;body&gt; and &lt;/body&gt; specify the body of the HTML document.\n&lt;section&gt; and &lt;/section&gt; are used to define sections of a website.\n&lt;div&gt; and &lt;/div&gt; are used to define divisions, which are often used to group related contents in a website.\n&lt;h1&gt; (or &lt;h2&gt;, &lt;h3&gt;,…, &lt;h6&gt;) are used to generate structures for websites (similar to the heading structure in Microsoft Word).\n&lt;p&gt; and &lt;/p&gt; contain paragraghs.\n&lt;img&gt; are used to insert images into a website.\n\n\n\n\nWebsite Scraping Workflow\nWe are going to work on a website, https://music-to-scrape.org, to learn basic website scraping techniques and walk through the basic procedures. This practice, however, is by no means a sophisticated way of web scraping nor can it be directly used on other websites.\n\nUnderstand the Website Structure\nAs a preparation for website scraping, we are going to first get a basic sense of the HTML document of the website.\n\n\n\nHTML code as shown in Chrome\n\n\n\n\nLocate the Contents\nWe need to locate the contents that we want to scrape in the HTML document. An HTML document is basically semi-structured data with tags labeling contents of a website. Hence, what we need to do is to locate the contents and identify the tags labeling the contents.\nFor most mainstream browsers (e.g., Google Chrome, Firefox, and Microsoft Edge), we can view the HTML code of websites under the “developer mode” (or similar).\nAdditionally, we can use the inspection function to locate the contents of our interest in HTML file. For example, if we are interested in obtaining the names of song writers/singers under “Top 15 Weekly Tracks” from https://music-to-scrape.org, we can select the song name and use right click in Google Chrome to inspect the HTML file:\n\n\n\nUse “Inspect” to map from website contents to HTML contents\n\n\nThe name appears in the HTML file:\n\n\n\nUse “Inspect” to map from website contents to HTML contents\n\n\nThe key to scrape information and data from websites is to precisely locate relevant contents using HTML tags. If we would like to obtain the name information, we need to first find a way to uniquely locate the contents based on CSS selectors.\n\nThe contents are under the section &lt;section name=\"weekly_15\"&gt;\nWe can locate “Gabriel Yared” uniquely by following the hierarchy of the HTML document: &lt;a&gt;, &lt;h5&gt;, and lastly &lt;b&gt;.\n\n\n\nScrape the Contents\nAfter locating the contents and identifying the tags, we use the following program to scrape the information we need from the website.\n\n# Load the necessary libraries\nlibrary(rvest)\nlibrary(dplyr)\n\n# Specify the URL of the website\nurl &lt;- \"https://music-to-scrape.org\"\n\n# Read the HTML code of the website into R\npage &lt;- read_html(url)\n\n# Extract the desired information using the tags (here, names from the weekly top 15)\nnames &lt;- page %&gt;%\n    html_nodes(\"section[name='weekly_15']\") %&gt;% #Locate to the section\n    html_elements('a') %&gt;% # Get to the elements to be scraped through the HTML hierarchy\n    html_element('h5') %&gt;% \n    html_element('b') %&gt;% \n    html_text() #Grab the text from the selected contents\n\n\n\n\n\n\n\nTiphtml_element() and html_elements()\n\n\n\nThese two arguments are in singular and plural forms, respectively:\n\nhtml_element() selects the first match of the HTML tag and stops there.\nhtml_elements() selects all matched tags through of the HTML document.\n\nCombining the two would help you accurately locate and extract information you want. In the example website that we are working on:\n\n&lt;a&gt; is a label corresponding to each block showing a top song on the website. If we want to get all the top song blocks selected, we need to use html_elements() to select all of them.\n\nYou may have a try by changing it to html_element().\n\n&lt;h5&gt; and &lt;b&gt; under each block tagged by &lt;a&gt; only appear once - it does not matter in this case whether we use singular or plural. In some other cases, it may be necessary to use singular or plural form.\n\n\n\n\n\n15 min\n\nModify the code above to address the following questions:\n\nThere is one line of code selecting HTML tags redundant. Find and remove it from the program.\nCollect both the names of artists and musics in the weekly top 15 list and make the output as a table.\nObtain usernames of recent active users on the website.\n\n\n\nSolution\n\n\n\nhtml_element('h5') is redundant. This is because under each node &lt;a&gt;, we are able to uniquely locate the name using the tag &lt;b&gt;. Hence, there is no need to specify another layer &lt;h5&gt; in between.\n\n\n\n# Specify the URL of the website\nurl &lt;- \"https://music-to-scrape.org\"\n\n# Read the HTML code of the website into R\npage &lt;- read_html(url)\n\n# Locate the information to be scraped using HTML tags\n\n# Extract the information from HTML document\ndata &lt;- page %&gt;%\n    html_nodes(\"section[name='weekly_15']\") %&gt;% #Locate to the section\n    html_elements('a') %&gt;% # Get to the elements to be scraped through the HTML hierarchy\n    html_elements('b,p') %&gt;% \n    html_text() #Grab the text from the selected contents\n\n# Convert into tibble\nsong_data &lt;- tibble(\n  artists = data[seq(1, length(data), 2)],\n  songs = data[seq(2, length(data), 2)]\n)\n\nprint(song_data)\n\n\n\n\n\n\n\nTipseq() in R\n\n\n\nseq() is a useful function in R to generate a sequence. Here, we are using seq() to generate a list of indices to select elements from a list.\nThe function has three arguments: seq(from = , to = , by = )\n\nfrom = specifies the start\nto = specifies the end\nby = specifies the step size (i.e. the gap between two consecutive numbers)\n\nseq(1, length(data), 2) will generate a list as (1, 3, 5, …) in this case until sequence reach to the length of the list.\n\n\n\nWe need to use “Inspect” function on your browser to locate the contents in the HTML file and modify the program accordingly. Note that you need to change the sections to be scraped in this exercise.\n\n\n# Specify the URL of the website\nurl &lt;- \"https://music-to-scrape.org\"\n\n# Read the HTML code of the website into R\npage &lt;- read_html(url)\n\n# Locate the information to be scraped using HTML tags\n\n# Extract the information from HTML document\nactive_users &lt;- page %&gt;%\n    html_nodes(\"section[name='recent_users']\") %&gt;% #Locate to the section\n    html_elements('a') %&gt;% # Get to the elements to be scraped through the HTML hierarchy\n    html_elements('h5') %&gt;% \n    html_text() #Grab the text from the selected contents\n\nprint(active_users)\n\n\n\n\n\nPros and Cons of Web Scraping\nCollecting data through website scraping has pros and cons.\nPros:\n\nAbility to obtain real-time data, no need to wait for databases.\nFull flexibility in data collection.\n\nCons:\n\nLegal and ethical issues regarding website scraping.\nMany websites have installed protection mechanisms (e.g., bot detectors and CAPTCHA)\nUnstable performance due to website maintenance, restructuring, and limited internet access.\nInconsistent data quality, requires additional work to clean up collected data.\n\n\n\nMore Advanced Web Scraping\nAs you will notice along the way, getting information directly from HTML contents becomes less feasible nowadays. Here are the reasons:\n\nDynamic and Interactive Web Pages In the early web, HTML pages were mostly static: the content was directly embedded in the HTML source (the same as the simple website that we just worked on) Today, many websites use JavaScript frameworks (e.g., React, Angular, Vue) to dynamically load and render content in the browser. As a result, if you fetch the raw HTML, you often see only the skeleton structure (&lt;div&gt; placeholders), while the actual text, images, or tables are injected later by scripts.\nAnti-scraping Measures and Legal Restrictions Many websites deliberately make HTML scraping difficult by using obfuscation, CAPTCHAs, or rate-limiting. Some also encrypt content or require user interaction (scrolling, clicking) to load additional data.\n\nAs a result, you have to sharpen your tools to get adapted to the changing world. To get into the web scraping space, we need to have solid knowledge about HTML and Javascript. Additionally, we need to be familiar with more advanced web scraping frameworks (e.g., selenium).\n\n\nEthics of Web Scraping\n\nRespect for Website Terms and Conditions\n\nMany websites describe the usage of website data in their terms and conditions. Always check these documents before conducting website scraping to avoid getting into trouble.\n\nAvoiding Hindering Normal Website Operations\n\nExcessive website scraping can strain website servers and result in slowed performance or even downtime. Ethical scraping practices include implementing rate limits, setting up program sleep time, and avoiding high-frequency visits in peak times.\n\nBe Aware of Sensitive and Private Data\n\nCertain data scraped from websites, even though they are public, could contain sensitive information. Make sure your website scraping practice and data usage are in compliance with relevant data privacy protection laws.\n\nUse the Data Ethically -Using scraped data for good purposes. Do not use scraped data for spamming, unauthorized data resale, or activities infringing on intellectual property rights. Be careful about your legal responsibility and ligitation risk when using scraped data for commercial purposes.",
    "crumbs": [
      "Finding and Storing Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Collected Structured and Unstructured Data</span>"
    ]
  },
  {
    "objectID": "advanced_analytics/data_variation.html",
    "href": "advanced_analytics/data_variation.html",
    "title": "9  Variation in Cross-Sectional, Times-Series, and Panel Data",
    "section": "",
    "text": "9.1 The Business Challenge\nFinancial markets generate an enormous amount of data every second: prices, trading volumes, financial reports, press releases, news headlines, analyst reports, and even tweets. Behind every movement in the stock market lies information waiting to be uncovered.\nIn today’s financial market, data-driven investment and trading have become the main stream. Quantitative trading (“quant trading”) is built on the notion that market behavior can be modeled, measured, and predicted using data. Different from trading based on human judgment, quant trading sticks to a way of trading financial assets, such as stocks, currencies, or commodities, using mathematical models and evidence from data analysis, instead of relying purely on human intuition or gut feeling.\nInstead of asking:\nQuant traders think about:\nIn this chapter, we will step into the role and act as a quantitative trader, analyzing stock prices and experimenting with simple trading strategies. To prepare for this role, we first need to understand how data describes the variation across entities and time. Next, we will work on stock price data to get familiar with how to deal with dates and time stamps effectively. Finally, we will create a mini trading strategy in panel data using past stock price performance.\nBefore diving into stock prices and financials, it’s essential to understand the type of data we are working with. Different types of data allow us to answer different questions:\nWhen dealing with data in the financial market, recognizing the structure of your data is crucial for selecting the right models and strategies.",
    "crumbs": [
      "Extracting Deeper Insights from Data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Variation in Cross-Sectional, Times-Series, and Panel Data</span>"
    ]
  },
  {
    "objectID": "advanced_analytics/data_variation.html#the-business-challenge",
    "href": "advanced_analytics/data_variation.html#the-business-challenge",
    "title": "9  Variation in Cross-Sectional, Times-Series, and Panel Data",
    "section": "",
    "text": "“Do I think this stock will go up tomorrow?”\n\n\n\n“What does the data tell me about the likelihood this stock will go up tomorrow?”\n\n\n\n\nCross-sectional data tells us about differences across entities at one point in time.\nTime-series data helps us observe how something evolves over time.\nPanel data combines both, letting us see how multiple entities change across time.",
    "crumbs": [
      "Extracting Deeper Insights from Data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Variation in Cross-Sectional, Times-Series, and Panel Data</span>"
    ]
  },
  {
    "objectID": "advanced_analytics/data_variation.html#cross-sectional-data",
    "href": "advanced_analytics/data_variation.html#cross-sectional-data",
    "title": "9  Variation in Cross-Sectional, Times-Series, and Panel Data",
    "section": "9.2 Cross-sectional Data",
    "text": "9.2 Cross-sectional Data\n\n\n\n\n\n\nNoteDefinition: Cross-Sectional Data\n\n\n\nData consists of observations collected at a single point in time across multiple subjects or entities (e.g., firms, stocks, households, individuals, cities, countries). This type of data provides a “snapshot” of a population at that moment, showing patterns and differences among various entities\n\n\nYou can consider cross-sectional data as taking a “photo” or “snapshot.” Imagine pausing the market at a time point, and writing down the stock price and return of every constituent firm in the S&P 500. That dataset would let us compare firms against each other at that moment: Which companies outperformed, which lagged behind, and whether certain firm characteristics were linked to higher or lower returns at the time point.\n\n\n10 min\n\nIn this exercise, we are going to get a sense of cross-sectional data by looking at stock trading data of S&P 500 firms as of 31/12/2024:\n\n\n\n\n\n\nTipS&P 500\n\n\n\n\n\nThe Standard and Poor’s 500, or simply the S&P 500, is a stock market index tracking the stock performance of 500 leading companies listed on stock exchanges in the United States. It is one of the most commonly followed equity indices and includes approximately 80% of the total market capitalization of U.S. public companies, with an aggregate market cap of more than $57.401 trillion as of August 29, 2025. (Source: Wikipedia)\nNote that the list of firms belonging to the S&P 500 index is not constant. Firms may be added to or dropped from the list due to changes in market capitalization (i.e., the total value of the shares) or merger and aquisition (M&A). In addition, the index may have more than 500 firms because it may include two share classes of its component companies.\n\n\n\n\nLoad the data, sp500_xs.csv, that captures stock prices of S&P 500 firms on 31/12/2024. Take a glimpse of the data. In the dataset, prc is stock price, ret refers to daily stock return, shrout standards for the number of shares outstanding, and vol is the trading volume (the number of shares being traded during that day).\n\n\n# Load packages for data wrangling and visualization\nlibrary(dplyr)\nlibrary(tibble)\nlibrary(lubridate)\nlibrary(ggplot2)\n# quantmod is used to get stock price data from Yahoo Finance\nlibrary(quantmod)\n\n\n# Read data into R\nsp500_xs &lt;- YOUR_CODE(\"../data/sp500_xs.csv\") \n\n# Take a glimpse \nYOUR_CODE\n\n\nCalculate total market capitalization (i.e., value of all shares, calculated as stock price * number of shares). Which are the top 5 companies in terms of total market cap? Do you know these firms?\n\n\ntop5 &lt;- sp500_xs %&gt;%\n    YOUR_CODE(mktcap = YOUR_CODE) %&gt;%\n    arrange(YOUR_CODE) %&gt;%\n    YOUR_CODE(5)\n\ntop5\n\n\n\nSolution\n\n\n\n\n\n\n# Read data into R\nsp500_xs &lt;- read_csv(\"../data/sp500_xs.csv\") \n\nRows: 503 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): tic\ndbl (5): stock_id, prc, ret, shrout, vol\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Take a glimpse \nsp500_xs %&gt;% glimpse()\n\nRows: 503\nColumns: 6\n$ stock_id &lt;dbl&gt; 10104, 10107, 10138, 10145, 10516, 10696, 11308, 11403, 11404…\n$ tic      &lt;chr&gt; \"ORCL\", \"MSFT\", \"TROW\", \"HON\", \"ADM\", \"FI\", \"KO\", \"CDNS\", \"ED…\n$ prc      &lt;dbl&gt; 111.70, 397.58, 108.45, 202.26, 55.58, 141.87, 59.49, 288.46,…\n$ ret      &lt;dbl&gt; 0.063265, 0.057281, 0.007057, -0.035525, -0.230407, 0.067977,…\n$ shrout   &lt;dbl&gt; 2748922, 7430436, 223938, 652182, 533381, 600186, 4323414, 27…\n$ vol      &lt;dbl&gt; 1711372, 5256086, 319141, 645959, 1805810, 533327, 2938936, 3…\n\n\n\n\n\n\ntop5 &lt;- sp500_xs %&gt;%\n    mutate(mktcap = prc * shrout) %&gt;%\n    arrange(desc(mktcap)) %&gt;%\n    head(5)\n\ntop5\n\n# A tibble: 5 × 7\n  stock_id tic     prc     ret   shrout      vol      mktcap\n     &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;\n1    10107 MSFT   398.  0.0573  7430436  5256086 2954192671.\n2    14593 AAPL   184. -0.0422 15441881 11805597 2847482702.\n3    84788 AMZN   155.  0.0215 10387381  9505252 1612121531.\n4    86580 NVDA   615.  0.242   2464000  9656674 1516025329.\n5    13407 META   390.  0.102   2200049  3428921  858327139.",
    "crumbs": [
      "Extracting Deeper Insights from Data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Variation in Cross-Sectional, Times-Series, and Panel Data</span>"
    ]
  },
  {
    "objectID": "advanced_analytics/data_variation.html#time-series-data",
    "href": "advanced_analytics/data_variation.html#time-series-data",
    "title": "9  Variation in Cross-Sectional, Times-Series, and Panel Data",
    "section": "9.3 Time-Series Data",
    "text": "9.3 Time-Series Data\n\n\n\n\n\n\nNoteDefinition: Time-Series Data\n\n\n\nTime-series data is a sequence of observations of a single subject or entity collected over time at successive (usually equally spaced) intervals. This type of data provides a trajectory for a single subject or entity over time.\n\n\nTime-series data varies in an orthogonal dimension compared with cross-sectional data. Instead of taking a snapshot of all stocks on the market, time-series data records the same set of information of a single stock over time. For instance, you may collect daily stock prices of Apple Inc. (ticker: AAPL) over time and evaluate the ups and downs.\n\n9.3.1 Data Wrangling for Timestamps\nMany datasets (especially in finance, business, and analytics) are tied to dates and times (e.g., stock prices, sales transactions, website visits, or macroeconomic indicators).\nHowever, dates and times are often a tricky component in programming:\n\nThey can appear in texts, numbers, or a mix of the two (e.g., December 31, 2024 vs. 12/31/2024)\nThe same date/time can be expressed in many different ways depending on convention or users’ habits (e.g., 12/31/2024 vs. 31/12/2024 vs. 2024-12-31; 5:00 PM vs. 17:00)\nDates and times can be added/subtracted, but the numeral system for time is different (12/24/60 as bases)\nTime zones and daylight saving times\n\nWhen we work on financial market data, date and times are extremely important. When we analyze stock prices and financial performance, we would like to have data ordered by date and time. Sometimes, we even need to merge datasets by dates and date ranges.\nlubridate is the package in R that offers powerful functions and tools to deal with time stamps and periods. We will be actively using the tools from lubridate throughout this chapter. You may refer to R4DS for more comprehensive reading of dealing with time stamps and using lubridate.\n\n\n9.3.2 Obtain Stock Price Data Using quantmod\nIn R, there is one powerful package that allows us to easily help us acquire financial market data, quantmod. getSymbols() is a powerful function in quantmod that uses the API to fetch financial market data from various sources (e.g., Yahoo Finance).\n\n\n\n\n\n\nNoteWhat is quantmod?\n\n\n\nquantmod stands for Quantitative Financial Modelling Framework. It is mainly used for downloading, charting, and modeling financial time series data such as stock prices, exchange rates, or interest rates.\nquantmod offers a wide range of data sources, including Yahoo Finance and Federal Reserve Economic Data (you may try to use quantmod to obtain data from FRED).\n\n\nWe are going to use getSymbols() from quantmod to get a sense of time-series data by looking at Apple Inc. (AAPL)’s year-to-date stock prices.\nLoad required packages and use getSymbols() to obtain AAPL’s stock prices from 01/01/2025 to now. Check the class of the output. Is the output in the format of tibble?\n\n# Obtain AAPL's stock prices\ngetSymbols(\"AAPL\",\n    src = \"yahoo\", \n    from = as.Date(\"2025-01-01\")\n    # to = # If to = is not specified, it fetchs data up till the most recent\n)\n\n[1] \"AAPL\"\n\nhead(AAPL)\n\n           AAPL.Open AAPL.High AAPL.Low AAPL.Close AAPL.Volume AAPL.Adjusted\n2025-01-02    248.93    249.10   241.82     243.85    55740700      242.9874\n2025-01-03    243.36    244.18   241.89     243.36    40244100      242.4992\n2025-01-06    244.31    247.33   243.20     245.00    45045600      244.1333\n2025-01-07    242.98    245.55   241.35     242.21    40856000      241.3532\n2025-01-08    241.92    243.71   240.05     242.70    37628900      241.8415\n2025-01-10    240.01    240.16   233.00     236.85    61710900      236.0122\n\nclass(AAPL)\n\n[1] \"xts\" \"zoo\"\n\n\n\n\n\n\n\n\nNoteas.Date()\n\n\n\nas.Date() is a base function in R. You can use it without loading lubridate. It converts dates stored in strings into &lt;date&gt; format. Inside the function, you need to specify the original format of dates in strings.\nBy default, as.Date() tackles dates in the format of “yyyy-mm-dd”. If the original date strings are not in this format (e.g., dd/mm/yyyy), you need to specify format = \"%d/%m/%Y in the function. %d, %m, and %Y are regular expressions that standard for a specific way expressing time concepts (see R4DS for more details).\n\n\n\n\n\n\n\n\nTip(Optional) xts objects\n\n\n\n\n\nxts stands for eXtensible Time Series.\n\nIt is a special type of data structure in R built on top of a matrix or data frame, but with dates (or times) as the row index.\nThis means, instead of just rows numbered 1, 2, 3…, each row is labeled by a date (or datetime)\n\nIt is designed and optimized for time-series data like stock prices, exchange rates, or economic indicators. It makes it very easy to deal with dates and times:\n\nSubset by time (AAPL[\"2022-01\"] gives all January 2022 data)\nMerge or align time series of different data categories and frequency\nAllow efficient processing of time-series data and save memory\n\nHowever, as an object specializing in time-series data, xts may not be generalizable to other tasks and cannot be directly processed by our tidyverse tools.\nWe can convert xts objects into tibbles so that we are able to apply our data wrangling tools\n\n\n\nConvert the output in the xts class to tibble. Look at date, is it a date variable?\n\n# Convert xts objects to tibble\naapl_tbl &lt;- as_tibble(AAPL, rownames = \"date\") %&gt;%\n  rename_with(~ sub(\"AAPL\\\\.\", \"\", .x))   # remove \"AAPL.\" prefix\nhead(aapl_tbl)\n\n# A tibble: 6 × 7\n  date        Open  High   Low Close   Volume Adjusted\n  &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 2025-01-02  249.  249.  242.  244. 55740700     243.\n2 2025-01-03  243.  244.  242.  243. 40244100     242.\n3 2025-01-06  244.  247.  243.  245  45045600     244.\n4 2025-01-07  243.  246.  241.  242. 40856000     241.\n5 2025-01-08  242.  244.  240.  243. 37628900     242.\n6 2025-01-10  240.  240.  233   237. 61710900     236.\n\n\n\n# Check the type\nclass(aapl_tbl$date)\n\n[1] \"character\"\n\n# Convert into dates\naapl_tbl &lt;- aapl_tbl %&gt;%\n    mutate(date = as.Date(date))\n\n# Check the type again\nclass(aapl_tbl$date)\n\n[1] \"Date\"\n\n\nVisualize the price trends using ggplot:\n\nggplot(aapl_tbl, aes(x = date, y = Close)) +\n  geom_line(color = \"blue\") +\n  scale_x_date(date_breaks = \"1 months\", date_labels = \"%b-%Y\") +\n  labs(title = \"AAPL Stock Price (2025)\",\n       x = \"Date\", y = \"Adjusted Close Price ($)\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIt is important that the variable, date, is converted to the “Date” format. If not, ggplot() and other functions in R may not be able to correctly recognize and process dates.\n\nTry to plot the price trend without converting the date variable\n\n\n\nYou may have seen stock prices being plotted into “candlesticks.” The candlestick chart is way to visualize stock price movements by showing (1) open prices, (2) close prices, (3) highest prices, and (4) lowest prices together in one plot. The candlestick chart offers a neat way to convey rich information in prices.\nLet’s use ggplot() to generate a candlestick chart for the period between 08/09/2025 and 12/09/2025:\n\ncandle_sticks &lt;- ggplot(aapl_tbl %&gt;%\n    filter(date&gt;=as.Date(\"2025-09-08\") & date&lt;=as.Date(\"September 12, 2025\", format = \"%B %d, %Y\"))\n    , aes(x = date, open = Open, high = High, low = Low, close = Close)) +\n  tidyquant::geom_candlestick() +\n  labs(title = \"AAPL Candlestick Chart\",\n       y = \"Price\", x = \"Date\")\n\n\n\n\n\n\n\nTipCandlestick Chart\n\n\n\n\n\nCandlestick chart is widely used to represent stock price information. For each trading day (or week, month, quarter, …), the chart use a thin bar to represent open and closing prices and long sticks to show the highest and the lowest prices of a day.\nIn addition, it uses colors to distinguish gains and losses. In the chart below, red bars and sticks represent losses, whereas blue bars and sticks represent gains.\n\n\n\nRead the candlestick chart of AAPL between 08/09/2025 and 12/09/2025. Given that Apple Inc. announced its new iPhone, Airpods, and Apple Watches on 10/09/2025, how do you think Apple’s investors perceive the new product?\n\ncandle_sticks\n\n\n\n\n\n\n\n\n\n\n9.3.3 From Prices to Returns\nA stock (or share) represents a small piece of ownership in a company. If you buy one share of Apple, you literally own a tiny slice of Apple. As a shareholder, you benefit when the company does well because the value of your share can go up, and you may also receive dividends, which are cash payments companies sometimes give to their owners.\nStock prices simply how much one share costs in the market. For example, if Apple is trading at $234.07 as of 12/09/2025, that means you would pay $234.07 to own one share or sell a share of Apple at $234.07. Stock prices change constantly throughout the day as investors react to news, company announcements, or broader economic conditions.\nWhen we look at stocks, we usually care less about the exact price levels but more about how that price changes over time. That is where the concept of a return comes in. A stock’s return tells us the percentage gain or loss over a given period. For example, without considering any dividends, if Apple’s share price rises from $234.07 to $250, the return is about 6.8% ((250.00 - 234.07)/234.07). In simple terms, returns show you how much your money has grown (or shrunk) as a percentage, making them directly comparable across investments in different companies and time periods.\nAssume there is no dividends, stock splits, and repurchases, we can use the stock prices at two different time points (\\(t1\\) and \\(t2\\)) to calculate the returns accrued in the period between the two time points:\n\\(Return_{t1, t2} = (Price_{t2} - Price_{t1})/Price_{t1}\\)\nIn practice, investors are interested in returns of fixed time periods, such as daily returns, monthly returns, quarterly returns, and annual returns. We use the differences between the closing price at the end of the period and the closing price right before the period to calculate such returns.\nHence, if we would like to calculate daily returns for AAPL on day \\(t\\), we are going to first calculate the changes in closing prices from day \\(t-1\\) to \\(t\\) and then scaled by the closing price of day \\(t-1\\) (the base period). To do so, we need to use the lag() operation:\n\n# Calculate returns from prices \naapl_return &lt;- aapl_tbl %&gt;%\n    arrange(date) %&gt;% # Sort in ascending order to avoid mistakes\n    transmute(date, Price = Close,\n              Return = (Close - lag(Close))/lag(Close)) \n\n\n\n\n\n\n\nTiptransmute()\n\n\n\ntransmute() combines mute() and select(). We can both select columns and do operations over columns ithin transmute().\n\n\n\n\n9.3.4 Cumulative Returns\nAs an investor, you may be interested in how much money you are making by buying and holding a stock. We can achieve this goal by either calculating price changes or compounding daily returns. Compounding of returns are more commonly used in portfolio management as it may be difficult to get “prices” for each portfolio. To prepare us to analyze portfolio returns, we are going to get familiar with compounding operations in R:\n\naapl_cumret &lt;- aapl_return %&gt;%\n    mutate(Return = if_else(is.na(Return), 0, Return)) %&gt;% # Fill missing as 0\n    mutate(Cumret = cumprod(1 + coalesce(Return, 0)) -1)\n\n\n\n\n\n\n\nNoteHow to calculate cumulative (compounded) returns?\n\n\n\nIn the code chunk above, we use cumprod(1 + coalesce(Return, 0)) - 1 to calculate cumulative returns. If we express it in mathematically, we are simply calculating:\n\\(Cumret_t = (1 + Return_1)\\times(1 + Return_2) \\times ... \\times (1 + Return_{t-1}) \\times (1 + Return_t)\\)\n\nReturn is the stock return for each day\ncoalesce(Return, 0) means that we replace missing values in Return as 0 (otherwise, everything after a missing value will be missing)\nTo do compounding, we need to add 1 to returns and then multiply these returns up to a given date\ncumprod is used to calculate cumulative product of all daily returns up to a given date\n\n\n\nWith Cumret, we can visualize the ups and downs of our wealth. Suppose we invest $100 into AAPL at the beginning of 2025, the plot below shows how the $100 evolves throughout the year:\n\n# Do some transformation to Cumret so that it reflects investment value\nggplot(aapl_cumret, aes(x = date, y = 100 * (Cumret + 1))) +\n    geom_line() +\n    scale_x_date(date_breaks = \"1 months\", date_labels = \"%b-%Y\") +\n    labs(title = \"Value of Investment in AAPL\",\n       y = \"Value\", x = \"Date\") +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor a single stock, looking at prices directly is actually easier than doing the compounding return above. However, when it comes to an investment portfolio of multiple stocks, our approach using compounding returns becomes much better by unifying all stocks using returns and avoiding dealing with prices of multiple stocks.",
    "crumbs": [
      "Extracting Deeper Insights from Data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Variation in Cross-Sectional, Times-Series, and Panel Data</span>"
    ]
  },
  {
    "objectID": "advanced_analytics/data_variation.html#panel-data",
    "href": "advanced_analytics/data_variation.html#panel-data",
    "title": "9  Variation in Cross-Sectional, Times-Series, and Panel Data",
    "section": "9.4 Panel Data",
    "text": "9.4 Panel Data\n\n\n\n\n\n\nNoteDefinition: Cross-Sectional Data\n\n\n\nPanel data is a combination of cross-sectional and time-series data. Instead of only looking across firms at one point in time (cross-sectional), or following a single firm across time (time-series), panel data records many firms across many points in time.\n\n\nMost of time, the data available to us comes as panel data. Panel data has two (orthogonal) dimensions, entity and time. Below is an example of panel data where rows represents each entity and columns represents time points.\n\npanel &lt;- read_csv('../data/panel_example.csv')\n\nRows: 10 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (6): id, Year1, Year2, Year3, Year4, Year5\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npanel\n\n# A tibble: 10 × 6\n      id  Year1 Year2  Year3  Year4  Year5\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1     1 0.520  0.600 0.584  0.562  0.554 \n 2     2 0.215  0.252 0.0458 0.626  0.396 \n 3     3 0.0319 0.880 0.363  0.274  0.707 \n 4     4 0.817  0.550 0.852  0.869  0.594 \n 5     5 0.743  0.110 0.895  0.0314 0.205 \n 6     6 0.439  0.554 0.749  0.581  0.0274\n 7     7 0.748  0.902 0.501  0.612  0.640 \n 8     8 0.372  0.886 0.942  0.819  0.245 \n 9     9 0.482  0.120 0.560  0.492  0.267 \n10    10 0.542  0.807 0.781  0.296  0.720 \n\n\nIn practice, such data is often stored into the “long form” rather than the “short form”:\n\n# Make the data as long table\npanel_long &lt;- panel %&gt;%\n  pivot_longer(\n    cols = starts_with(\"Year\"),   # columns to reshape\n    names_to = \"Year\",            # new column for year labels\n    values_to = \"Value\"           # new column for values\n  )\n\npanel_long\n\n# A tibble: 50 × 3\n      id Year   Value\n   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n 1     1 Year1 0.520 \n 2     1 Year2 0.600 \n 3     1 Year3 0.584 \n 4     1 Year4 0.562 \n 5     1 Year5 0.554 \n 6     2 Year1 0.215 \n 7     2 Year2 0.252 \n 8     2 Year3 0.0458\n 9     2 Year4 0.626 \n10     2 Year5 0.396 \n# ℹ 40 more rows\n\n\nThe strength of panel data is that it lets us study both differences across firms and changes within firms over time in the same framework. This opens the door to much more powerful analysis. For example, we could ask: “Do firms with higher profitability earn higher future returns?” By using panel data, we can track the same firm’s financial ratios and returns over time while also comparing across many firms at once. This dual perspective helps us avoid misleading results that might come from only looking at a single snapshot or only tracking one company.\nPanel data is especially important because it allows us compare one observation with other observations at the same time point or with itself in the past. Such features of panel data help us draw more reliable conclusions and inferences from data.\n\n\n\n\n\n\nTip(Optional) Fixed Effects\n\n\n\n\n\nPanel data allows us to use fixed effects to isolate the effect of individual entities or time. Most of us may have used the notion of fixed effects consciously or unconsciously. For example, when we compare entities with other comparable entities at the same time point, we are actually controlling for time fixed effects. When we compare someone with its past, we are accounting for individual entity fixed effects.\nFixed effects refers to the average effect either at the entity level or at the time level. Understanding the concept of fixed effects may help avoid fallacys in drawing conclusions.\nTime fixed effects\n\nAdjust for events that affect all entities\nExample: during the 2008 financial crisis or the COVID-19 pandemic, stock prices fell for almost every firm\nWe should not attribute the decline of stock prices to firm-specific reasons if this is a market-wide phenomenon\nWe can figure which firms experience declines of relatively smaller magnitude than others or compare a firm with the average of all entities at the same time point\n\nFirm Fixed Effects\n\nAccount for entity-level time-invariant effects\nCertain industries are innately more profitable than others (e.g., high tech vs. cafe)\nIf we observe that AAPL makes net profit margin of 25.26%, while a local cafe makes 10%, it does not necessarily mean that the local cafe is doing poorly.\nWe need to compare “Apple” to “Apple”!\n\n\n\n\n\n9.4.1 Construct an Investment Portfolio\n\nDo not put all your eggs in one basket!\n\nIn practice, quantitative hedge funds and traders rarely put all their money into just one stock. Instead, they spread their investments across many different stocks, creating what we call a portfolio. By looking at these portfolios, we can see two kinds of variation: cross-sectional variation, which tells us how different firms perform on the same day, and time-series variation, which tracks how a single firm’s performance changes from day to day. The job of a quant trader is to analyze both types of variation and find patterns that can guide trading decisions. In this lecture, we are going to step into their shoes and explore real data to see how this works.\nWe have obtained a list of stocks that ranks top 10 in terms of market cap in our earlier exercises. Now, we will use getSymbols() to retrieve their stock prices and combine all these data as a panel. Note that you are not required (but highly recommended) to fully understand the block of code below.\n\n# Get the List of Tickers\nticker_list &lt;- top5$tic\n\n# Get prices for all stocks on the list\ngetSymbols(ticker_list, from = \"2025-01-01\", auto.assign = TRUE)\n\n[1] \"MSFT\" \"AAPL\" \"AMZN\" \"NVDA\" \"META\"\n\n# Grab all output objects into a list\nprices_list &lt;- mget(ticker_list, inherits = TRUE)\n\n# Convert a list of xts objects into a tibble\nall_stocks &lt;- imap_dfr(\n  prices_list,\n  ~ {\n    xt  &lt;- .x\n    tic &lt;- .y\n    df  &lt;- as_tibble(data.frame(date = index(xt), coredata(xt)))\n    # Strip any \"AAPL.\" / \"MSFT.\" etc. prefix from all price columns\n    names(df) &lt;- sub(\"^.*\\\\.\", \"\", names(df))\n    df %&gt;%\n      mutate(ticker = tic) %&gt;%\n      select(date, ticker, Open, High, Low, Close, Volume, Adjusted)\n  }\n)\n\n\n\n9.4.2 Visualize Returns of Individual Stocks\nNow, we are going to visualize the ups and downs of the five stocks in our portfolio since 01/01/2025. As you can see from the data, the price levels of stocks vary significantly across different firms. In order to make them comparable, a better option is to calculate cumulative returns and make a plot with five stocks sharing the same starting point. To do this task, we can replicate what we have done for AAPL to other stocks:\nFor each stock (i.e., group_by(ticker)), calculate daily returns and then compound returns up to each date:\n\n# Calculate cumulative returns\nall_stocks &lt;- all_stocks %&gt;%\n  group_by(ticker) %&gt;%\n  arrange(date, .by_group = TRUE) %&gt;%\n  mutate(\n    Return   = Close / lag(Close) - 1,\n    Cumret = cumprod(1 + coalesce(Return, 0)) - 1\n  ) %&gt;%\n  ungroup()\n\nSimilar to what we have done for AAPL, assume that we invest 100 dollars into each stock, plot the value of investment for each stock since 01/01/2025:\n\nggplot(all_stocks, aes(x = date, y = 100 * (Cumret + 1), color = ticker)) +\n    geom_line(size = 0.75) +\n    labs(\n        title = \"Value of Investment since 2025-01-01\",\n        x = \"Date\",\n        y = \"Value of Investment\",\n        color = \"Ticker\"\n    ) +\n    scale_x_date(date_breaks = \"1 months\", date_labels = \"%b-%Y\") +\n    theme_minimal(base_size = 12)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\n9.4.3 Calculate Equal-Weighted Portfolio Returns\nQuant trading always invests in multiple stocks and construct a portfolio to manage all investment targets. When constructing portfolios, a fund can invest in constituent stocks equally. For example, if you have five stocks in your portfolio, you put 20% of your wealth in each of them. Such portfolios are called as equal-weighted portfolios.\nIn some cases, we do not want to evenly split our money among serveral stocks. We may want to invest a big more on certain stocks based on our assessment or statistical analyses. One common practice of quant investment is to construct value-weighted portfolios based on market capitalization. Also, you may place larger weights on Nvidia (NVDA) and Microsoft (MSFT) because you expect that the positive (or even overly positive) sentiments for AI may push stock prices of these stocks even higher.\nHere, we can use dplyr to easily summarize portfolio_returns at the portfolio level and calculate returns for such equal-weighted portfolio:\n\n# Equal weighted portfolio\n\npf_ew &lt;- all_stocks %&gt;%\n  group_by(date) %&gt;%\n  summarise(\n    ew_ret = mean(Return, na.rm = TRUE)   # equal weight = simple average\n  ) %&gt;%\n  arrange(date) %&gt;%\n  mutate(\n    ew_cumret = cumprod(1 + coalesce(ew_ret, 0)) - 1\n  )\n\n# Plot returns of equal-weighted portfolio over time\nggplot(pf_ew, aes(x = date, y = ew_cumret)) +\n    geom_line(color = \"darkblue\", size = 1) +\n    labs(\n        title = \"Equal-Weighted Portfolio Cumulative Return\",\n        x = \"Date\", y = \"Cumulative Return\"\n    ) + \n    scale_x_date(date_breaks = \"1 months\", date_labels = \"%b-%Y\") +\n    theme_minimal(base_size = 12) +\n    scale_y_continuous(labels = scales::percent)\n\n\n\n\n\n\n\n\n\n\n10 min\n\nMake use of the variables in sp500_xs to construct value-weighted portfolio:\n\nAssign weights using market capitalization as of 2024-12-31. Note that you do not want to use information or data that is not available when you construct any porfolio (i.e., no foresight).\nAggregate the five stocks into a value-weighted portfolio and visualize the return of the portfolio.\n\n\n\nSolution\n\n\n\nall_stocks_vw &lt;- all_stocks %&gt;% \n    left_join(top5 %&gt;% select(ticker = tic, mktcap), \n              by = 'ticker')\n\n\npf_vw &lt;- all_stocks_vw %&gt;%\n    group_by(date) %&gt;%\n    summarise(\n        vw_ret       = weighted.mean(Return, mktcap, na.rm = TRUE),\n        .groups = \"drop\"\n  ) %&gt;%\n    arrange(date) %&gt;%\n    mutate(\n    vw_cumret = cumprod(1 + coalesce(vw_ret, 0)) - 1\n  )\n\n\n# Plot returns of equal-weighted portfolio over time\nggplot() +\n    geom_line(data = pf_ew, aes(x = date, y = ew_cumret, color = \"Equal-Weighted\"), linewidth = 0.75) +\n    geom_line(data = pf_vw, aes(x = date, y = vw_cumret, color = \"Value-Weighted\"), linewidth = 0.75) +\n    labs(\n        title = \"Portfolio Cumulative Return\",\n        x = \"Date\", y = \"Cumulative Return (%)\",\n        colar = \"Portfolio Types\"\n    ) +\n    theme_minimal(base_size = 12) +\n    scale_y_continuous(labels = scales::percent)",
    "crumbs": [
      "Extracting Deeper Insights from Data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Variation in Cross-Sectional, Times-Series, and Panel Data</span>"
    ]
  },
  {
    "objectID": "advanced_analytics/data_variation.html#conclusion",
    "href": "advanced_analytics/data_variation.html#conclusion",
    "title": "9  Variation in Cross-Sectional, Times-Series, and Panel Data",
    "section": "9.5 Conclusion",
    "text": "9.5 Conclusion\nIn this lecture, you learned how data structure shapes the questions you can ask and the tools you should use:\n\nCross-sectional data compares entities at one time (e.g., S&P 500 firms on 31/12/2024).\nTime-series data tracks one entity over time (e.g., AAPL daily prices in 2025).\nPanel data combines both, letting you study differences across firms and changes within firms over time.\n\nKey takeaways\n\nAlways identify your unit of observation (entity, time) and understand the variation of your data along different dimensions before proceeding with data analyses.\nData wrangling skills for dates and times\nSimple quant trading workflows (more in the workshop)",
    "crumbs": [
      "Extracting Deeper Insights from Data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Variation in Cross-Sectional, Times-Series, and Panel Data</span>"
    ]
  },
  {
    "objectID": "tutorials/what_is_business_analytics.html",
    "href": "tutorials/what_is_business_analytics.html",
    "title": "Tutorial 1: Business Analytics Skills for Inventory Management",
    "section": "",
    "text": "The Business Challenge\nDavid Jones sells a wide range of seasonal products, including fashion items that see strong swings in demand around holidays like Christmas.\nInventory decisions are especially challenging for seasonal products because:\nRight now, David Jones is using a fixed restocking rule:\nBut with the holiday season approaching, this rigid approach might not be enough.\nYou’ve been asked to review the data and advise the company on how to improve its inventory decisions.\nYour task is to:",
    "crumbs": [
      "Tutorials",
      "Tutorial 1: Business Analytics Skills for Inventory Management"
    ]
  },
  {
    "objectID": "tutorials/what_is_business_analytics.html#the-business-challenge",
    "href": "tutorials/what_is_business_analytics.html#the-business-challenge",
    "title": "Tutorial 1: Business Analytics Skills for Inventory Management",
    "section": "",
    "text": "Demand can spike unpredictably\nPromotions and discounts influence customer behaviour\nRegional preferences can differ\nSuppliers need advance notice to adjust deliveries\n\n\n\nEvery week, 100 units are delivered to each region, no matter what.\n\n\n\n\nSpot patterns in demand\nEvaluate the effectiveness of the current restocking rule\nRecommend improvements to ensure shelves are stocked — but not overstocked",
    "crumbs": [
      "Tutorials",
      "Tutorial 1: Business Analytics Skills for Inventory Management"
    ]
  },
  {
    "objectID": "tutorials/what_is_business_analytics.html#prepare-these-exercises-before-class",
    "href": "tutorials/what_is_business_analytics.html#prepare-these-exercises-before-class",
    "title": "Tutorial 1: Business Analytics Skills for Inventory Management",
    "section": "Prepare these Exercises before Class",
    "text": "Prepare these Exercises before Class\nPrepare these exercises before coming to class. Plan to spend 30 minutes on these exercises.\n\nExercise 1: What is Inventory Management?\nWatch the video “Inventory Management in 11 minutes” and a write 2–3 sentence summary of what you learned. Try to connect what you hear in the video to the challenge David Jones is facing: what makes inventory management tricky, and why is it so important?\n\n\nExercise 2: Approaches to Inventory Planning\nRead the article, “A New Approach to Production and Inventory Planning,” and answer the following questions in 2–3 sentences:\n\nWhat is one key challenge businesses face in managing inventory today?\nHow could this apply to a retailer like David Jones, especially during busy seasons like Christmas?\n\n\n\nExercise 3: Identifying a research question\nWrite a research question that could help David Jones improve its inventory decisions.\n\n\n\n\n\n\nTip\n\n\n\nTry to make your research question clear, specific, and something that could be answered with data.\n\n\n\n\nExercise 4: Identifying Data\nIf you were working as an analyst at David Jones, what types of data would you want to collect to help manage inventory effectively? List at least three kinds of data.\n\n\nExercise 5: Skill Choices\nWhat skills do you think does a business data analyst needs to solve this problem? Justify your answer.",
    "crumbs": [
      "Tutorials",
      "Tutorial 1: Business Analytics Skills for Inventory Management"
    ]
  },
  {
    "objectID": "tutorials/what_is_business_analytics.html#in-class-exercises",
    "href": "tutorials/what_is_business_analytics.html#in-class-exercises",
    "title": "Tutorial 1: Business Analytics Skills for Inventory Management",
    "section": "In-Class Exercises",
    "text": "In-Class Exercises\nYou will discuss these exercises in class with your peers in small groups and with your tutor. These exercises build from the exercises you have prepared above, you will get the most value from the class if you have completed those above before coming to class.\n\nExercise 6: Defining a Problem and Crafting Questions\n\nShare your draft research questions with your group.\nDiscuss which questions are clear? Which are answerable with data? Which could help David Jones make a better decision?\nWhich of the proposed questions is the “best”. Explain why.\n\n\n\n\n\n\n\nTipConversation prompt\n\n\n\nAre your group’s questions descriptive (what happened), predictive (what will happen), or causal (why did it happen)? How does that shape the analysis?\n\n\n\n\nExercise 7: Exploring Last Year’s Data\nThe team now wants to use last year’s data to improve inventory decisions for the upcoming holiday season.\nAs the new group of analysts in the company, you’ve been asked to review what happened and make recommendations for this year’s strategy.\nThe data you have to work from are provided below:\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek Commencing\nRegion\nStarting Inventory\nUnits Delivered\nUnits Sold\nEnding Inventory\nPrice (AUD)\nPromotion (Y/N)\n\n\n\n\n25 Nov\nSydney\n500\n100\n100\n500\n50\nN\n\n\n25 Nov\nMelbourne\n500\n100\n150\n450\n50\nN\n\n\n2 Dec\nSydney\n500\n100\n180\n420\n45\nY\n\n\n2 Dec\nMelbourne\n450\n100\n220\n330\n45\nY\n\n\n9 Dec\nSydney\n420\n100\n150\n370\n45\nY\n\n\n9 Dec\nMelbourne\n330\n100\n100\n330\n45\nY\n\n\n16 Dec\nSydney\n370\n100\n400\n70\n40\nY (Christmas)\n\n\n16 Dec\nMelbourne\n330\n100\n400\n30\n40\nY (Christmas)\n\n\n23 Dec\nSydney\n70\n100\n120\n50\n50\nN\n\n\n23 Dec\nMelbourne\n30\n100\n150\n-20 (backorder)\n50\nN\n\n\n\nFollow the three-step approach below to draw conclusions about what happened in the previous year.\n\nStep 1: Observe\n\nWhat patterns do you notice in sales over time?\nHow do price changes and the Christmas promotion relate to changes in sales?\nWhat differences stand out between Sydney and Melbourne?\n\n\n\nStep 2: Analyse\n\nHow well is the fixed restocking rule working in each region?\nWhen did inventory levels become risky?\nWere there signs earlier in the data that could have helped predict this?\n\n\n\nStep 3: Predict & Explain\n\nIf nothing changes, what do you expect to happen next week?\nWhat factors might be driving the differences between the two regions?\nWhat other data would help you explain or predict these patterns more confidently?\n\n\n\n\nExercise 8: Planning for This Year’s Holiday Season\nDavid Jones’ supplier is asking you to confirm delivery quantities now, so they can prepare for the season ahead.\n\nHow many units should you order for Sydney and Melbourne for the first week of the holiday season?\nJustify your decision based on last year’s patterns and your reasoning. Consider factors like demand trends, regional differences, price effects, and promotion timing.\nSuppose the supplier has limited capacity and can only supply 200 units total for the first week. How would you allocate this stock between Sydney and Melbourne? What are the risks of your choice?\n\n\n\nExercise 9: Presenting to Management\nImagine you are presenting your recommendation on what to order now to David Jones’ management team.\nWrite 2–3 sentences summarising:\n\nWhat you saw in the data from last year\nWhat you recommend\nWhy this matters for the business\n\n\n\nExercise 10: Reflection\n\nWhich skills of a business data analyst did you use today?\nWhich steps of the business analytics workflow did you go through?\nDid you adopt descriptive, predictive or causal perspectives in your analyses?\nHow did your perspectives differ from other groups and how did this change the approach and results?",
    "crumbs": [
      "Tutorials",
      "Tutorial 1: Business Analytics Skills for Inventory Management"
    ]
  },
  {
    "objectID": "tutorials/how_to_do_business_analytics.html",
    "href": "tutorials/how_to_do_business_analytics.html",
    "title": "Tutorial 2: Core Skills of a Business Scientist & Introduction to Posit Cloud",
    "section": "",
    "text": "The Business Challenge\nUber is facing a new challenge in cities like Sydney and Melbourne. Some drivers have found a way to game the surge pricing system by temporarily logging off in groups to reduce supply. This triggers a surge in prices—and when they log back in, they benefit from higher fares.\nWhile this behavior doesn’t technically break any rules, it raises serious concerns. Should Uber treat it as clever strategy or as manipulation? Should it intervene—and if so, how? Cracking down too hard could upset drivers and hurt morale. Doing nothing might damage rider trust and public perception.\nUber’s analytics and operations teams need to investigate what’s really going on, what’s driving this behavior, and whether (and how) to respond in a way that balances fairness, incentives, and platform health.",
    "crumbs": [
      "Tutorials",
      "Tutorial 2: Core Skills of a Business Scientist & Introduction to Posit Cloud"
    ]
  },
  {
    "objectID": "tutorials/how_to_do_business_analytics.html#prepare-these-exercises-before-class",
    "href": "tutorials/how_to_do_business_analytics.html#prepare-these-exercises-before-class",
    "title": "Tutorial 2: Core Skills of a Business Scientist & Introduction to Posit Cloud",
    "section": "Prepare these Exercises before Class",
    "text": "Prepare these Exercises before Class\nPrepare these exercises before coming to class. Plan to spend 30 minutes on these exercises.\n\nExercise 1: Identifying the key concepts\nWatch this short news story: “Uber drivers creating artificial surge prices” (ABC News, 29 July 2025).\nIn your own words, define the following key terms or ideas mentioned in the article:\n\nSurge pricing\nAlgorithmic pricing\nPlatform manipulation\nDriver incentives\n\n\n\n\n\n\n\nTip\n\n\n\nYou don’t need to provide formal or technical definitions. Focus on explaining each concept in your own words, based on how it was used in the video.\n\n\nWrite your answer here\n\n\nExercise 2: Explaining Context and Identifying a Research Question\nUber’s pricing system is designed to respond to real-time supply and demand. But as the video shows, driver behavior can complicate this—sometimes in ways the algorithm wasn’t built to anticipate.\nImagine you’re part of a team investigating the situation in Sydney and Melbourne.\nIn 3–4 sentences, explain how you would define the problem for Uber. Then, write one clear research question that your team could use to guide further analysis.\n\n\n\n\n\n\nTip\n\n\n\nTo help you think through this, you might want to consider the following:\n\nWhat’s really going wrong here?\nWhy does it matter to the business?\nWhat kind of evidence or data would help clarify the situation?\n\n\n\nWrite your answer here\n\n\nExercise 3: How to answer the question?\nYour manager turns to you and says:\n\n“Where should we start?”\n\nIn 3–5 bullet points, outline how you would approach the problem as a business scientist.\n\n\n\n\n\n\nTip\n\n\n\nTo help you think through this, you may want to integrate answers from the following questions:\n\nWhat business or consumer theories might help you?\nWhat data would you want?\nWhat kind of methods might be useful?\n\n\n\nWrite your answer here",
    "crumbs": [
      "Tutorials",
      "Tutorial 2: Core Skills of a Business Scientist & Introduction to Posit Cloud"
    ]
  },
  {
    "objectID": "tutorials/how_to_do_business_analytics.html#in-class-exercises",
    "href": "tutorials/how_to_do_business_analytics.html#in-class-exercises",
    "title": "Tutorial 2: Core Skills of a Business Scientist & Introduction to Posit Cloud",
    "section": "In-Class Exercises",
    "text": "In-Class Exercises\nYou will discuss these exercises in class with your peers in small groups and with your tutor. The first few exercises build from the exercises you have prepared above, you will get the most value from the class if you have completed those above before coming to class. The latter exercises are designed to help you get more comfortable using PositCloud.\n\nExercise 4: Defining a Problem and Crafting Questions\n\nShare your business context descriptions and research questions (from Question 2) with your group.\nAs a group, discuss:\n\n\nWhich context descriptions are clear and well-framed?\nWhich research questions are answerable using data?\nWhich questions are most likely to help Uber make a better decision?\n\n\nChoose the best research question your group came up with. Write down the question and explain why you chose it. What makes it clear, useful, or powerful?\n\n\n\n\n\n\n\nTipConversation prompt\n\n\n\nAre your group’s questions descriptive (what happened), predictive (what will happen), or causal (why did it happen)? How does that shape the analysis?\n\n\nWrite your answer here\n\n\nExercise 5: Sketching Out the Way Forward\n\nShare your bullet point responses to Question 3 (“Where should we start?”) with your group.\nAs a group, discuss:\n\n\nWhich responses feel realistic and well thought-through?\nWhich ones show a good understanding of how business context and data interact?\nWhich approaches would be easiest to communicate to a stakeholder who doesn’t know the data?\n\n\nChoose the group member’s response that you think offers the strongest starting point. Write it down and explain why—what makes it practical, well-reasoned, or insightful?\n\nWrite your answer here\n\n\nExercise 6: Logging into PositCloud\n\n\n\n\n\n\nCautionContent Pivot\n\n\n\nIn this week’s Workshop we are pivoting midway through to give you more time to get comfortable with the computing environment for this class, PositCloud.\nWe know this is a sudden pivot in content. In future weeks, we don’t anticipate sudden jumps in content in the middle of a session.\n\n\nLog in to Posit Cloud and launch your project.\n\nGo to https://posit.cloud and log in using your university credentials.\nNavigate to the “CMCE 10002 - 2025 Semester 2” Space on the left sidebar.\nFind the project titled “Tutorial 2”.\nClick “Make a Copy”.\nWait for the RStudio session to finish loading — you should see the Console, Files, and Environment panes.\n\n\n\nExercise 7: Make Some Edits to Your Quarto Document\n\nAt the top of the document, in the YAML header, change the author field to your full name.\nEdit the sentence below sentence so:\n\nOne word is bold by wrapping it in double asterisks: **like this**\nAnother word italic by wrapping it in single asterisks: *like this*\n\nAdd a new R code chunk that computes the answer to 2+2 and returns the answer:\nAdd another R code chunk that stores the answer to 2+2 a variable called my_answer and prints it.\nRender the document and see the changes you have made.\n\nEDIT THIS: The quick brown fox jumps over the lazy dog\n\n\nExercise 8: Write Your Own Code\n\nAdd new code chunk to store a vector, called my_vector that contains each of the numbers 1 through 5. You can produce that vector using c(1,2,3,4,5)\nCalculate the mean of my_vector by applying the mean() function.\nAdd a comment above your code that computes the mean to describe what it does. You can add a comment using the hashtag to start a line inside an R chunk like this # This is a comment\nNow use the Console to search for help on the function that returns the median value in a numeric vector, ?median.",
    "crumbs": [
      "Tutorials",
      "Tutorial 2: Core Skills of a Business Scientist & Introduction to Posit Cloud"
    ]
  },
  {
    "objectID": "tutorials/data_visualisation.html",
    "href": "tutorials/data_visualisation.html",
    "title": "Tutorial 3: Data Visualisation for Business Intelligence",
    "section": "",
    "text": "The Business Challenge",
    "crumbs": [
      "Tutorials",
      "Tutorial 3: Data Visualisation for Business Intelligence"
    ]
  },
  {
    "objectID": "tutorials/data_visualisation.html#the-business-challenge",
    "href": "tutorials/data_visualisation.html#the-business-challenge",
    "title": "Tutorial 3: Data Visualisation for Business Intelligence",
    "section": "",
    "text": "The Topic: Understanding Melbourne’s Housing Market\nMelbourne’s real estate market is one of the most dynamic in Australia. Property prices are influenced by several factors, including location, dwelling type, and proximity to the Central Business District (CBD). Buyers, sellers, and policymakers constantly analyze housing trends to make informed decisions. Understanding these trends is crucial in a $2 trillion+ housing market, where even small fluctuations in pricing expectations can have significant financial impacts.\nA key question we will explore is: How do property prices vary by region, dwelling type, and location? By analyzing real estate data, we can identify patterns that help explain the drivers of Melbourne’s housing prices and their implications for different stakeholders in the market.\n\n\nThe Data: The Melbourne Housing Market Dataset\nWe will use a dataset containing real estate sales data from Melbourne. The dataset provides insights into housing trends and includes key variables that influence property prices.\nThe dataset captures the property location within broader metropolitan regions (regionname), allowing us to compare trends across different areas. It also includes the type of dwelling, classified as House (h), Townhouse (t), or Apartment (u), enabling us to assess price differences based on housing type. The sale price (price) provides a measure of market value, while the distance to the CBD (distance) helps analyze the impact of proximity to the city center on property prices.\nSimilar datasets are widely used by real estate analysts, banks, and property investors to assess housing trends, predict future movements, and guide investment decisions. Through this tutorial, we’ll work with real-world data to uncover patterns and develop a deeper understanding of Melbourne’s property market. Let’s dive in!\n\n\nWhere we’re headed\nJust a few lines of R code transform numbers to data visualizations.\nFrom this:\n\n\n# A tibble: 10 × 6\n   type      address          postcode   price distance regionname    \n   &lt;chr&gt;     &lt;chr&gt;               &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;         \n 1 House     85 Turner St         3067 1480000      2.5 Northern Metro\n 2 House     25 Bloomburg St      3067 1035000      2.5 Northern Metro\n 3 House     5 Charles St         3067 1465000      2.5 Northern Metro\n 4 House     40 Federation La     3067  850000      2.5 Northern Metro\n 5 House     55a Park St          3067 1600000      2.5 Northern Metro\n 6 House     129 Charles St       3067  941000      2.5 Northern Metro\n 7 House     124 Yarra St         3067 1876000      2.5 Northern Metro\n 8 House     98 Charles St        3067 1636000      2.5 Northern Metro\n 9 House     217 Langridge St     3067 1000000      2.5 Northern Metro\n10 Townhouse 18a Mollison St      3067  745000      2.5 Northern Metro\n\n\nto this:",
    "crumbs": [
      "Tutorials",
      "Tutorial 3: Data Visualisation for Business Intelligence"
    ]
  },
  {
    "objectID": "tutorials/data_visualisation.html#loading-the-data",
    "href": "tutorials/data_visualisation.html#loading-the-data",
    "title": "Tutorial 3: Data Visualisation for Business Intelligence",
    "section": "Loading the Data",
    "text": "Loading the Data\n\nR packages for today\n\nlibrary(tidyverse)  # for plotting, includes ggplot\nlibrary(patchwork)  # for combining multiple plots into subfigures\nlibrary(scales)     # for formatting axis scales\nlibrary(ggokabeito) # color blind friendly color palette -- this course's default\nlibrary(ggthemes)   # some extra styling & themes to explore\n\n\n\nLoading the Data in R\n\nhousing &lt;-\n    read_csv(\"data/melbourne_housing.csv\")",
    "crumbs": [
      "Tutorials",
      "Tutorial 3: Data Visualisation for Business Intelligence"
    ]
  },
  {
    "objectID": "tutorials/data_visualisation.html#prepare-these-exercises-before-class",
    "href": "tutorials/data_visualisation.html#prepare-these-exercises-before-class",
    "title": "Tutorial 3: Data Visualisation for Business Intelligence",
    "section": "Prepare these Exercises before Class",
    "text": "Prepare these Exercises before Class\nPrepare these exercises before coming to class. Plan to spend 45 minutes on these exercises.\n\n\n\n\n\n\nImportantSwitch on the eval flag when you want to evaluate code!\n\n\n\nIn the R code chunks below we have provided starter code for you to work from. We have set the key eval to the value false so that they are not run because they have syntax such as YOUR_VALUE_HERE which would generate errors.\nSwitch the eval value to true when you want the R code within a chunk to be run when you compile your document.\n\n\n\nExercise 1: Plotting Number of Listings by Region\nBy the end of this exercise, your plot should look similar to this one:\n\n\n\n\n\n\n\n\n\n(a). If you were a real estate investor or city planner, why would knowing the number of listings by region be valuable? What decisions could be influenced by this visualization?\n(b). Use the starter code below to create a first version of the bar plot. Replace YOUR_VARIABLE_NAME with the appropriate variable name from the dataset.\n\n\n\n\n\n\nTipWhat is a Bar Plot?\n\n\n\n\n\nA bar plot is a graphical representation of categorical data, where each category is represented by a bar whose height reflects its frequency or proportion. It is commonly used to compare counts across different groups, making it easy to identify the most and least common categories in a dataset. In business and research, bar plots help visualize trends, distributions, and key differences in data at a glance.\n\n\n\n\nggplot(housing, \n       aes(x = YOUR_VARIABLE_NAME)\n       ) +\n    geom_bar()\n\n(c). Extend the plot above by adding labels to the x-axis, y-axis, and title. Use the labs() function to add these labels. The code below also rotates the axis labels by 25 degrees to improve readability.\n\nggplot(housing, \n       aes(x = YOUR_VARIABLE_NAME)\n       ) +\n    geom_bar() +\n    labs(x = \"YOUR_X_AXIS_LABEL\", \n         y = \"YOUR_Y_AXIS_LABEL\", \n         title = \"YOUR_TITLE\"\n         ) +\n    theme(\n        axis.text.x = element_text(angle = 25, hjust = 1),\n    )\n\n(d). What patterns do you notice in the number of listings? Are certain regions overrepresented or underrepresented? How might this affect housing supply and pricing?\n\n\nExercise 2: Visualizing the distribution of Prices by Dwelling Type\nBy the end of this exercise, your plot should look similar to this one:\n\n\n\n\n\n\n\n\n\n(a). Why is a histogram useful for understanding price distributions? What insights can we gain from plotting price distributions separately for each dwelling type?\n\n\n\n\n\n\nTipWhat is a Histogram?\n\n\n\n\n\nA histogram is a visualization that represents the distribution of numerical data by dividing values into bins and counting the number of observations in each bin. Unlike a bar plot, which displays categorical data, a histogram is used for continuous variables like price, age, or distance.\nHistograms help identify patterns in data, such as skewness, outliers, and central tendencies, making them a crucial tool for understanding how values are distributed in a dataset.\n\n\n\n(b). Use the starter code below to create a basic histogram of house prices. Replace YOUR_VARIABLE_NAME with the correct variable.\n\nggplot(housing, \n       aes(x = YOUR_VARIABLE_NAME)\n       ) +\n    geom_histogram(bins = 50)\n\n(c). Extend the plot above by adding labels to the x-axis, y-axis, and title using the labs() function.\n\nggplot(housing, \n       aes(x = YOUR_VARIABLE_NAME)\n       ) +\n    geom_histogram(bins = 50) +\n    labs(x = \"YOUR_X_AXIS_LABEL\", \n         y = \"YOUR_Y_AXIS_LABEL\", \n         title = \"YOUR_TITLE\"\n         )\n\n(d). Modify the x-axis so that prices are displayed in dollar format by adding scale_x_continuous(labels = scales::dollar). We have rotated the axis labels to make the numbers easier to read.\n\nggplot(housing, \n       aes(x = price)\n       ) +\n    geom_histogram(bins = 50) +\n    YOUR_CODE_HERE +\n    labs(x = \"YOUR_X_AXIS_LABEL\", \n         y = \"YOUR_Y_AXIS_LABEL\", \n         title = \"YOUR_TITLE\"\n         ) +\n    theme(\n        axis.text.x = element_text(angle = 55, hjust = 1),\n    )\n\n(e). Add fill = type inside aes() to color the histogram by dwelling type and then create subplots by dwelling type.\n\nggplot(housing, \n       aes(x = price, \n           YOUR_CODE_HERE\n           )\n       ) +\n    geom_histogram(bins = 50) +\n    YOUR_CODE_HERE +\n    labs(x = \"YOUR_X_AXIS_LABEL\", \n         y = \"YOUR_Y_AXIS_LABEL\", \n         title = \"YOUR_TITLE\"\n         ) +\n    theme(\n        axis.text.x = element_text(angle = 55, hjust = 1),\n    ) +\n    YOUR_CODE_HERE\n\n(f). What does the histogram tell you about price differences between dwelling types?\n\n\nExercise 3: The Price - Distance Relationship\nBy the end of this exercise, your plot should look similar to this one:\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n(a). Why might we expect a relationship between price and distance to the CBD?\n(b). Use the starter code below to build a scatter plot of price vs. distance, replacing YOUR_VARIABLE_NAME with the correct column names.\n\n\n\n\n\n\nTipWhat is a Scatter Plot?\n\n\n\n\n\nA scatter plot is used to visualize the relationship between two continuous variables by plotting individual data points. Each dot represents one observation, helping identify patterns, clusters, and trends. In this case, it allows us to explore how house prices change with distance.\n\n\n\n\nggplot(housing, \n       aes(x = YOUR_VARIABLE_NAME, y = YOUR_VARIABLE_NAME)\n       ) +\n    geom_point()\n\n(c). Since we have a large number of data points, many overlap, making it difficult to distinguish individual observations. Modify the geom_point() function to reduce over-plotting by adjusting the alpha value. Experiment with different values (e.g., 0.1, 0.05, 0.01) and describe how each affects the visualization.\n\nggplot(housing, \n       aes(x = YOUR_VARIABLE_NAME, y = YOUR_VARIABLE_NAME)\n       ) +\n    geom_point(alpha = YOUR_NUMBER)\n\n(d). We can better understand the relationship by adding a statistical transformation to the plot. Complete the geom_smooth() function call to overlay a straight line over the data. The se = FALSE argument removes the confidence interval around the line.\n\nggplot(housing, \n       aes(x = YOUR_VARIABLE_NAME, \n           y = YOUR_VARIABLE_NAME)\n       ) +\n    geom_point(alpha = YOUR_NUMBER) +\n    geom_smooth(method = YOUR_METHOD, \n                se = FALSE)\n\n(e). Finally, add labels to the x-axis, y-axis, and title using the labs() function. Adjust the x- and y-axis scales as you deem appropriate.\n\nggplot(housing, \n       aes(x = YOUR_VARIABLE_NAME, y = YOUR_VARIABLE_NAME)\n       ) +\n    geom_point(alpha = YOUR_NUMBER) +\n    geom_smooth(method = YOUR_METHOD, \n                se = FALSE) +\n    scale_y_continuous(YOUR_CODE) +\n    labs(x = \"YOUR_LABEL\", \n         y = \"YOUR_LABEL\", \n         title = \"YOUR_TITLE\"\n         )\n\n(f). What does the scatter plot reveal about the relationship between house prices and distance to the CBD? How does the line help interpret this relationship?",
    "crumbs": [
      "Tutorials",
      "Tutorial 3: Data Visualisation for Business Intelligence"
    ]
  },
  {
    "objectID": "tutorials/data_visualisation.html#in-class-exercises",
    "href": "tutorials/data_visualisation.html#in-class-exercises",
    "title": "Tutorial 3: Data Visualisation for Business Intelligence",
    "section": "In-Class Exercises",
    "text": "In-Class Exercises\nYou will discuss these exercises in class with your peers in small groups and with your tutor. These exercises build from the exercises you have prepared above, you will get the most value from the class if you have completed those above before coming to class.\n\nExercise 4: Improving the Listings by Region Plot\n(a). Together with your peers, propose three changes you want to make to the plot in Exercise 1. Discuss the rationale behind each change and how it might improve the visualization.\n(b). Work with your tutor to create an agreed upon list of changes to make to the plot. What steps do you need to take to make these improvements?\n(c). Implement the changes suggested in the R code.\n\n\nExercise 5: Improving the Price Distribution Plot\n(a). Together with your peers, propose three changes you want to make to the plot in Exercise 2. Discuss the rationale behind each change and how it might improve the visualization.\n(b). Work with your tutor to create an agreed upon list of changes to make to the plot. What steps do you need to take to make these improvements?\n(c). Implement the changes suggested in the R code.\n\n\nExercise 6: Improving the Price-Distance Relationship Plot\n(a). Together with your peers, propose three changes you want to make to the plot in Exercise 3. Discuss the rationale behind each change and how it might improve the visualization.\n(b). Work with your tutor to create an agreed upon list of changes to make to the plot. What steps do you need to take to make these improvements?\n(c). Implement the changes suggested in R.\n\n\nExercise 7: Putting the plots together\n(a). Use the code below to combine the plots for the three exercises above. Replace PLOT_ONE, PLOT_TWO, and PLOT_THREE with the plots you created in Exercises 5, 6, and 7.\n\nPLOT_ONE /    \n    (PLOT_TWO | PLOT_THREE) + \n    plot_layout(guides = \"collect\") & \n    theme(legend.position = \"bottom\")\n\n(b). Save this combined plot as a PDF file using the ggsave() function. You can specify the file name and dimensions using the filename and width and height arguments.\n\nggsave(\"YOUR_FILE_NAME\", width = 12, height = 8)\n\n\n\nExercise 8: Synthesizing the Findings\nIf you were presenting these results in a business meeting, how would you explain the key takeaways? Structure your summary to include:\n\nWhat you analyzed (data, variables)\nWhat you found (key insights from the visualizations)\nWhat decisions could be made based on this information?",
    "crumbs": [
      "Tutorials",
      "Tutorial 3: Data Visualisation for Business Intelligence"
    ]
  },
  {
    "objectID": "tutorials/data_wrangling.html",
    "href": "tutorials/data_wrangling.html",
    "title": "Tutorial 4: Data Wrangling for Business Analytics",
    "section": "",
    "text": "The Business Challenge",
    "crumbs": [
      "Tutorials",
      "Tutorial 4: Data Wrangling for Business Analytics"
    ]
  },
  {
    "objectID": "tutorials/data_wrangling.html#the-business-challenge",
    "href": "tutorials/data_wrangling.html#the-business-challenge",
    "title": "Tutorial 4: Data Wrangling for Business Analytics",
    "section": "",
    "text": "The Topic: Understanding firm strategy in the Australian economy\nIn the lecture, we examined the level of firms’ profits and the efficiency with which firms use investors’ capital to produce these profits (i.e., returns as a measure of profitability). However, we have yet to examine variation in ‘how’ firms produce profits. Broadly speaking, firms can pursue one of two strategies. Firms can employ a high-margin, low-volume strategy, or a low-margin, high-volume strategy. The former rely on pricing power or operational efficiencies that drive down costs in order to produce more profit per unit of good or services sold. The latter rely on scaling up operations and distribution in order to sell at very high volumes goods or services that provide much less profit per unit.\nWe calculate a firm’s profit margin as follows:\n\\[\n\\mathrm{Profit\\ Margin} = \\frac{\\mathrm{EBIT}}{\\mathrm{Sales}}\n\\]\nFirms that effectively employ a high-margin, low-volume strategy have high profit margins (i.e., these firms generate more profit from each dollar of sales).\nWe calculate a firm’s asset turnover as follows:\n\\[\n\\mathrm{Asset\\ Turnover} = \\frac{\\mathrm{Sales}}{\\mathrm{Assets}}\n\\]\nFirms that effectively employ a low-margin, high-volume strategy have high asset turnover (i.e., these firms generate more sales from a given asset base).\nIn the following exercises we will explore research questions such as the following:\n\nHow does firm strategy vary by firm size and industry?\nDo we observe companies that successfully maintain high margins and high volumes, and if so, which types of companies fall into this category?\nDo we observe the inverse-i.e., companies that suffer from low margins and low volume?\nAnd, perhaps most importantly, how does firm strategy relate to profitability?\n\n\n\nThe Data: Financial Statement Observations from Yahoo Finance\nTo answer these questions, we draw on audited 2024 financial statements for the 200 largest ASX-listed firms (as measured by assets). This data set should be familiar to you from the lecture.\nFinancial statements provide a standardised snapshot of a company’s performance and financial health. In our data set, the key variables are:\n\nCompany name (conml) – the name of the firm as listed on the ASX\n\nIndustry (industry) – the industry classification, allowing comparison across sectors such as mining, banking, or healthcare\n\nProfit (ebit) – earnings before interest and taxes, representing core business profit after expenses (from the income statement)\n\nAssets (at) – the total resources the company owns that have economic value (from the balance sheet)\n\nInvested Capital (invested_capital) – the total capital invested in the company by both shareholders and debt holders to fund operations and growth. It generally includes:\n\nEquity (shareequity) – the funds contributed by shareholders plus retained earnings\n\nInterest-bearing debt (dlc, dltt, dvp) – loans and borrowings the company uses for financing\n\n\nBy working with these statements and variables, we can replicate the kind of assessments that professional analysts and fund managers use to guide billion-dollar investment decisions.\nAs you will see, just a few lines of R code can wrangle raw financial statements information into orderly data that we can effectively visualize.\nFrom this:\n\n\n# A tibble: 10 × 30\n   gvkey  curcd fyear   fyr datadate     at  capx shareequity   dlc   dltt   dvp\n   &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1 013312 USD    2024     6 6/30/20… 102362 9273       49120  2084  18634     NA\n 2 210216 AUD    2024     6 6/30/20…  45550 2288       17352  4228  12740     NA\n 3 223003 USD    2024     6 6/30/20…  38022  849       19401   944  11239     NA\n 4 212650 AUD    2024     6 6/30/20…  36694  104       11678  1590  18596     NA\n 5 100894 AUD    2024     6 6/30/20…  33936 2548        5570  2311  14411     NA\n 6 212427 USD    2024     6 6/30/20…  30060 2834       19531   192   5208     NA\n 7 101601 AUD    2024     6 6/30/20…  27309  923        8585  1165  10113     NA\n 8 226744 AUD    2024     6 6/30/20…  20894  754.       5275.  606. 10332.    NA\n 9 220244 AUD    2024     6 6/30/20…  20564 2761         294   600   5991     NA\n10 017525 AUD    2024     6 6/30/20…  20454  608        9489    68   3310     NA\n# ℹ 19 more variables: ebit &lt;dbl&gt;, netprofit &lt;dbl&gt;, pstk &lt;dbl&gt;, sale &lt;dbl&gt;,\n#   epsexcon &lt;dbl&gt;, nicon &lt;dbl&gt;, conm &lt;chr&gt;, fic &lt;chr&gt;, conml &lt;chr&gt;,\n#   ggroup &lt;dbl&gt;, gind &lt;dbl&gt;, gsector &lt;dbl&gt;, gsubind &lt;dbl&gt;, sector &lt;chr&gt;,\n#   indgroup &lt;chr&gt;, industry &lt;chr&gt;, subind &lt;chr&gt;, debt &lt;dbl&gt;,\n#   invested_capital &lt;dbl&gt;\n\n\nTo this:",
    "crumbs": [
      "Tutorials",
      "Tutorial 4: Data Wrangling for Business Analytics"
    ]
  },
  {
    "objectID": "tutorials/data_wrangling.html#loading-the-data",
    "href": "tutorials/data_wrangling.html#loading-the-data",
    "title": "Tutorial 4: Data Wrangling for Business Analytics",
    "section": "Loading the Data",
    "text": "Loading the Data\n\nR packages for today\n\nlibrary(tidyverse)     # collection of packages for data manipulation and visualization\nlibrary(scales)        # for formatting and transforming axis scales\nlibrary(ggokabeito)    # color blind friendly color palette — this course's default\nlibrary(ggthemes)      # additional ggplot themes for polished, publication-ready plots\nlibrary(patchwork)     # for combining multiple ggplots into one figure\nlibrary(stringr)       # for working with strings using consistent functions\nlibrary(RColorBrewer) # color palette\n\n\n\nLoading the Data in R\n\nasx_200_2024 &lt;-\n    read_csv(\"data/asx_200_2024.csv\")\n\n\n\nExercise 1: Visualizing the distributions of profit margin and asset turnover\nOur objective is to create the following plots:\n\n\n\n\n\n\n\n\n\n(a). Before creating these plots yourself, let’s think about what we can learn from them. Based on the figure above, what is the profit margin of the typical firm in our sample? How about asset turnover? Given what you know about the types of firm strategy, how do you expect a firm’s position in profit margin distribution to relate to its position in the asset turnover distribution?\n(b). Let’s get started on building these plots. First, we need to create two new variables-profit_margin, and asset_turnover-and store these in a new data frame (which we will use for the rest of this week’s exercises). As we did in the lecture, we will also create our measure of firm profitability-roic-so that we can study returns in subsequent exercises. Use the starter code below to create these variables. Replace FUNCTION_NAME and YOUR_VARIABLE_NAME with the appropriate function and variable names.\n\nfirm_strategy &lt;- \n    asx_200_2024 |&gt;\n    # Drop observations where we would have to divide by zero when calculating our new variables\n    FUNCTION_NAME(invested_capital &gt; 0 & at &gt; 0 & sale &gt; 0) |&gt;\n    # Create new variables\n    FUNCTION_NAME(\n        roic = YOUR_VARIABLE_NAME / YOUR_VARIABLE_NAME,\n        profit_margin = YOUR_VARIABLE_NAME / YOUR_VARIABLE_NAME,\n        asset_turnover = YOUR_VARIABLE_NAME / YOUR_VARIABLE_NAME\n        )\n\n(c). Next, we want to check that we have correctly created these new variables, profit_margin, and asset_turnover. To do so, create a short, narrow table that focuses on these variables and their components for a small subset of firms in our sample (in this case, the first 10 observations). Using this table, we can then visually check that each variable is calculated as per the formula above. Use the starter code below to create and display such a table. Replace YOUR_DATAFRAME_NAME and YOUR_FUNCTION_NAME with the appropriate data frame and function names.\n\nYOUR_DATAFRAME_NAME |&gt;\n    YOUR_FUNCTION_NAME(conml, ebit, at, sale, profit_margin, asset_turnover) |&gt;\n    YOUR_FUNCTION_NAME(10) \n\nYour table should look like this:\n\n\n# A tibble: 10 × 6\n   conml                    ebit     at   sale profit_margin asset_turnover\n   &lt;chr&gt;                   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;          &lt;dbl&gt;\n 1 BHP Group Ltd          22771  102362 55658         0.409           0.544\n 2 Telstra Group Limited   3712   45550 22928         0.162           0.503\n 3 CSL Ltd                 3896   38022 14690         0.265           0.386\n 4 Transurban Group        1132   36694  4119         0.275           0.112\n 5 Woolworths Group Ltd    3100   33936 67922         0.0456          2.00 \n 6 Fortescue Ltd           8520   30060 18220         0.468           0.606\n 7 Wesfarmers Ltd          3849   27309 44189         0.0871          1.62 \n 8 Ramsay Health Care Ltd   939.  20894 16660.        0.0563          0.797\n 9 Qantas Airways Ltd      2198   20564 20114         0.109           0.978\n10 Origin Energy Ltd        952   20454 16138         0.0590          0.789\n\n\n(d). Now, we can plot profit_margin, and asset_turnover and see how they behave for our sample of firms. To do so, we will first create a basic histogram that visualizes the distribution of profit_margin. Use the starter code below to produce this plot. Replace YOUR_DATAFRAME_NAME, YOUR_FUNCTION_NAME and YOUR_VARIABLE with the appropriate data frame, function, and variable names.\n\nYOUR_DATAFRAME_NAME |&gt;\n    YOUR_FUNCTION_NAME(aes(x = YOUR_VARIABLE)) + \n    YOUR_FUNCTION_NAME(fill = \"darkorange\", color = \"black\", \n                       alpha = 0.7, bins = 100) +\n    labs(title = \"Distribution of Profit Margin for Sample of Large ASX Companies in 2024\",  \n         x = YOUR_LABEL,  \n         y = YOUR_LABEL\n         ) +  \n    theme_minimal() \n\nYour plot should look like this:\n\n\n\n\n\n\n\n\n\n(e). Oops! Examining this histogram plot, what issue do you notice with the data that complicates plotting the distribution of profit_margin? How do you suggest we address this issue in the data, and what do we gain and lose by doing so?\n(f). Now, let’s move on to our other new variable. Let’s create a basic histogram that visualizes the distribution of asset_turnover. Use the starter code below to produce this plot. Replace YOUR_DATAFRAME_NAME, YOUR_FUNCTION_NAME and YOUR_VARIABLE with the appropriate data frame, function, and variable names.\n\nYOUR_DATAFRAME_NAME |&gt;\n    YOUR_FUNCTION_NAME(aes(x = YOUR_VARIABLE)) + \n    YOUR_FUNCTION_NAME(fill = \"skyblue\", color = \"black\", \n                       alpha = 0.7, bins = 100\n                       ) +\n    labs(title = \"Distribution of Asset Turnover for Sample of Large ASX Companies in 2024\", \n         x = YOUR_LABEL,  \n         y = YOUR_LABEL\n         ) +  \n    theme_minimal()\n\nYour plot should look like this:\n\n\n\n\n\n\n\n\n\n(g). This looks better! Examining the plot above, explain why we don’t have the issue that complicated plotting the distribution of profit margin.\n\n\nExercise 2: Firm Strategy and The Profit Margin - Asset Turnover Relationship\nOur objective is to create the following plot:\n\n\n\n\n\n\n\n\n\n(a). In exercise 1, you used histograms to show the distributions of profit_margin and asset_turnover. Use the starter code below to identify the five firms in the right tail of each of these distributions-i.e., those firms that have the highest profit margins, and those firms that have the highest asset turnover.\nTo help, we have included the table that your code should output for profit_margin:\n\n\n# A tibble: 5 × 8\n  conml        industry   ebit     at   sale profit_margin asset_turnover   roic\n  &lt;chr&gt;        &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;          &lt;dbl&gt;  &lt;dbl&gt;\n1 Aft Corpora… Buildin…   28.8   585. 3.04e1         0.945         0.0520 0.0494\n2 APA Group    Gas Uti… 1850   19563  3.04e3         0.609         0.155  0.114 \n3 Fortescue L… Metals … 8520   30060  1.82e4         0.468         0.606  0.342 \n4 Perseus Min… Metals …  462.   1986. 1.03e3         0.451         0.517  0.259 \n5 Emerald Res… Metals …  159.    712. 3.71e2         0.428         0.521  0.262 \n\n\n\n# Replicate the profit margin table\nfirm_strategy |&gt; \n    # slice_max is a concise alternative to using arrange() then head()\n    slice_max(YOUR_VARIABLE_NAME, n = 5) |&gt; \n    select(YOUR_VARIABLE_NAME, ..., YOUR_VARIABLE_NAME)\n\n\n# Produce the equivalent table for asset turnover\nfirm_strategy |&gt; \n    # slice_max is a concise alternative to using arrange() then head()\n    slice_max(YOUR_VARIABLE_NAME, n = 5) |&gt; \n    select(YOUR_VARIABLE_NAME, ..., YOUR_VARIABLE_NAME)\n\n(b). Do we observe differences in industries or firm size across these tables? Are the same or different firms present across these tables? How do profit margin and asset turnover appear to be related and what does this reveal about firm strategy?\n(c). Next, we want to create a new variable that classifies each firm in our sample as one of the following strategy types:\n\nIF above median profit margin AND below median asset turnover THEN, strategy type = ‘High margin’\nIF below median profit margin AND above median asset turnover THEN, strategy type = ‘High volume’\n\nSome firms may fail to meet the conditions for either of these categories. Classify these firms as follows:\n\nIF below median profit margin AND below median asset turnover THEN, strategy type = ‘Sinking ship’\nIF above median profit margin AND above median asset turnover THEN, strategy type = ‘Superstar’\n\nUse the starter code below to create the variable strategy_type and so classify each firm’s strategy type. Replace YOUR_DATAFRAME, YOUR_FUNCTION_NAME and YOUR_VARIABLE_NAME as appropriate.\n\nYOUR_DATAFRAME &lt;- \n    firm_strategy |&gt;\n    YOUR_FUNCTION_NAME(\n        strategy_type = case_when(\n            # Case 1: High Margin\n            profit_margin &gt;= median(profit_margin, na.rm = TRUE) &\n            asset_turnover &lt; median(asset_turnover, na.rm = TRUE) ~ \"High margin\",\n            # Case 2: High Volume      \n            YOUR_VARIABLE_NAME &lt; median(YOUR_VARIABLE_NAME, na.rm = TRUE) &\n            YOUR_VARIABLE_NAME &gt;= median(YOUR_VARIABLE_NAME, na.rm = TRUE) ~ \"High volume\",\n            # Case 3: Sinking Ship\n            YOUR_VARIABLE_NAME &lt; median(YOUR_VARIABLE_NAME, na.rm = TRUE) &\n            YOUR_VARIABLE_NAME &lt; median(YOUR_VARIABLE_NAME, na.rm = TRUE) ~ \"Sinking ship\",\n            # Case 4: Superstar\n            YOUR_VARIABLE_NAME &gt;= median(YOUR_VARIABLE_NAME, na.rm = TRUE) &\n            YOUR_VARIABLE_NAME &gt;= median(YOUR_VARIABLE_NAME, na.rm = TRUE) ~ \"Superstar\",\n            # Default : catch any unmatched cases\n            .default = NA_character_  \n    )\n  )\n\n(d). To shed light on how the above ‘works’, run the code below and explain what each part of the code ‘does’ to the output. What must be the case for a firm’s strategy type to be classified as ‘High margin’? How is a firm’s strategy type classified if this is not the case? What value does strategy type take if only one of these conditions holds?\n\nfirm_strategy |&gt;\n    mutate(\n        strategy_type = case_when(\n            # Case 1: \n            profit_margin &gt;= median(profit_margin, na.rm = TRUE) &\n                asset_turnover &lt; median(asset_turnover, na.rm = TRUE) ~ \"High margin\",\n            # Case 2:\n            is.na(profit_margin) | is.na(asset_turnover) ~ NA_character_,\n            # Other cases -- the default:\n            .default = \"Other\"\n    )\n  ) |&gt; \n  select(conml, profit_margin, asset_turnover, strategy_type) |&gt;\n  head(10)\n\n(e). Now, we can examine the relationship between profit margin, asset turnover, and firm strategy. Use the starter code below to create a basic scatterplot of profit margin vs asset turnover. Replace the placeholder code (e.g., YOUR_DATAFRAME) as appropriate.\n\nYOUR_DATAFRAME |&gt;\n    # To improve our visualization, we need to focus on representative \n    # firms and drop outliers (see exercises 1 & 4 for more)\n    filter(profit_margin &gt; -2) |&gt; \n    YOUR_FUNCTION_NAME() + \n    YOUR_FUNCTION_NAME(\n        aes(y = YOUR_VARIABLE_NAME, x = YOUR_VARIABLE_NAME), \n            alpha = 0.3\n        ) +\n        # Add a vertical line to show median value\n    geom_vline(xintercept = median(YOUR_DATAFRAME$YOUR_VARIABLE_NAME, na.rm = TRUE), \n               linetype = \"dashed\", color = \"blue\", size = 1\n        ) +\n        # Add a horizontal line to show median value\n    geom_hline(yintercept = median(YOUR_DATAFRAME$YOUR_VARIABLE_NAME, na.rm = TRUE), \n               linetype = \"dashed\", color = \"blue\", size = 1\n        ) + \n    theme_minimal() + \n    labs(title = \"Variation in Firm Strategy Across A Sample of Large ASX Companies in 2024\", \n         x = YOUR_LABEL, \n         y = YOUR_LABEL\n         ) + \n    guides(color = guide_legend(title = NULL))\n\nYou plot should look like this:\n\n\n\n\n\n\n\n\n\n(f). Nice! We can learn a lot from this plot, but to start let’s return to an earlier question we considered: How do profit margin and asset turnover appear to be related and what does this reveal about firm strategy? Explain why we have few firms in the top right? And why we have few firms in the bottom left?\n\n\nExercise 3: Firm Strategy: Industry, Size, and Profitability\nOur objective is to create the following plots:\n\n\n\n\n\n\n\n\n\n(a). Examine the plot below that captures how the relationship between profit margin and asset turnover varies with industry (as the code shows, this plot focuses on the ‘big’ industries that we covered in the lecture material). Explain how this plot is consistent with your findings in the last part of the previous question.\n\nbig_industries &lt;- c(\"Metals & Mining\", \"Specialty Retail\", \"Construction & Engineering\", \n                    \"Hotels, Restaurants & Leisure\", \"Oil, Gas & Consumable Fuels\", \n                    \"Commercial Services & Supplies\", \"Food Products\")\n\nscatter_ind &lt;- \n    firm_strategy |&gt; \n    filter(profit_margin &gt; -2 & industry %in% big_industries) |&gt; \n    ggplot() + \n    geom_point(aes(x = asset_turnover, y = profit_margin, fill = industry), \n               alpha = 0.7, shape = 21, size = 3\n               ) + # shape = 21 allows filling\n    geom_vline(xintercept = median(firm_strategy$asset_turnover, na.rm = TRUE), \n               linetype = \"dashed\", color = \"blue\", size = 1\n               ) + \n    geom_hline(yintercept = median(firm_strategy$profit_margin, na.rm = TRUE), \n               linetype = \"dashed\", color = \"blue\", size = 1\n               ) + \n    scale_fill_okabe_ito() +\n    theme_minimal() + \n    labs(title = \"Firm Strategy\", \n         x = \"Asset Turnover\", \n         y = \"Profit Margin\") + \n    guides(color = guide_legend(title = NULL), \n           fill = guide_legend(title = \"Industry\"))\n\nscatter_ind\n\n\n\n\n\n\n\n\n(b). Use the starter code provided below to produce a scatter plot that captures how the relationship between profit margin and asset turnover varies with firm size (at). How does this plot show differences in firm size? Explain what this plot reveals about the relationship between firm size and strategy.\n\nscatter_size &lt;- \n    YOUR_DATAFRAME |&gt; \n    filter(profit_margin &gt; -2) |&gt; \n    ggplot() + \n    YOUR_FUNCTION_NAME(aes(x = asset_turnover, y = profit_margin, \n                           size = YOUR_VARIABLE_NAME\n                           ),  \n                       alpha = 0.7, color = \"black\"\n                       ) +\n    geom_vline(xintercept = median(firm_strategy$asset_turnover, na.rm = TRUE), \n               linetype = \"dashed\", color = \"blue\", size = 1\n               ) + \n    geom_hline(yintercept = median(firm_strategy$profit_margin, na.rm = TRUE), \n               linetype = \"dashed\", color = \"blue\", size = 1\n               ) + \n    theme_minimal() + \n    labs(title = \"Firm Strategy\", \n         x = \"Asset Turnover\", \n         y = \"Profit Margin\"\n         ) + \n    guides(color = guide_legend(title = NULL), \n           size = guide_legend(title = \"YOUR_VARIABLE_NAME\")\n           )\n\nscatter_size\n\n(c). Use the starter code below to produce a scatter plot that captures how the relationship between profit margin and asset turnover varies with firm profitability (roic). Explain what this plot reveals about the relationship between firm strategy and profitability. What seems to ‘hurt’ firm performance more: weak margins, or weak volume?\n\nscatter_return &lt;- \n    YOUR_DATAFRAME |&gt; \n    filter(profit_margin &gt; -2) |&gt; \n    ggplot() + \n    YOUR_FUNCTION_NAME(aes(x = asset_turnover, y = profit_margin, \n                           fill = cut(YOUR_VARIABLE_NAME, \n                                      breaks = 3, labels = c(\"Low\", \"Medium\", \"High\")\n                                      )\n                           ), \n                       alpha = 0.7, shape = 21, color = \"black\", size = 4\n                       ) + \n    geom_vline(xintercept = median(firm_strategy$asset_turnover, na.rm = TRUE), \n               linetype = \"dashed\", color = \"blue\", size = 1\n               ) + \n    geom_hline(yintercept = median(firm_strategy$profit_margin, na.rm = TRUE), \n               linetype = \"dashed\", color = \"blue\", size = 1\n               ) + \n    scale_fill_okabe_ito() +\n    theme_minimal() + \n    labs(title = \"Firm Strategy: Large Australian Public Companies (2024)\", \n         x = \"Asset Turnover\", \n         y = \"Profit Margin\"\n         ) + \n    guides(color = guide_legend(title = NULL), \n           fill = guide_legend(title = \"ROIC\")\n           )\n\nscatter_return\n\n(d). Write a short chunk of code that integrates the three plots you created above into a single figure. Use the patchwork package to do this.\n\n# Write your answer here\n\n\n\nExercise 4: Identifying and removing outliers to improve the profit margin histogram\nIn exercise 1, we created the following plot:\n\n\n\n\n\n\n\n\n\n(a). We have two extreme observations in the left tail of our distribution. Create a short, narrow table that isolates these observations and shows only the sub-set of variables that relate to profit margin and asset turnover. Using this table, explains why these firms have such extreme (negative) profit margins. Use the starter code below to create and display such a table.\n\nYOUR_DATAFRAME |&gt;\n    YOUR_FUNCTION_NAME(YOUR_VARIABLE_NAME) |&gt;\n    YOUR_FUNCTION_NAME(2) |&gt;\n    YOUR_FUNCTION_NAME(conml, industry, ebit, at, sale, profit_margin, asset_turnover)\n\n(b). These extreme observations badly drag out our distribution’s left tail, and so compress our visualization of the rest of the distribution (i.e., the vast majority of firms in our sample are pushed into a single column even though there is much variation in these firms’ profit margins). Use the starter code below to remove these outliers from our sample and then create a basic histogram that visualizes the distribution of profit for our ‘pruned’ sample.\n\nYOUR_DATAFRAME |&gt;\n    YOUR_FUNCTION_NAME(YOUR_VARIABLE_NAME &gt; YOUR_LOWER_BOUND_VALUE) |&gt;\n    YOUR_FUNCTION_NAME(aes(x = YOUR_VARIABLE_NAME)) + \n    YOUR_FUNCTION_NAME(fill = \"darkorange\", color = \"black\", \n                       alpha = 0.7, bins = 100\n                       ) +  \n    labs(title = \"Distribution of Profit Margin for Sample of Large ASX Companies in 2024\", \n         x = YOUR_LABEL, \n         y = YOUR_LABEL\n         ) + \n    theme_minimal() \n\n(c). Hmm! Explain whether you think the plot above effectively visualizes the distribution of profit margin for large Australian public companies. Do we still have extreme observations in our left tail? How do these affect what we can learn about the majority of firms, which exhibit profit margins &gt;= 0?\n(d). Use the starter code below to further ‘prune’ our sample and so again visualize the distribution of profit margin for large Australian public companies. Do we now get a better sense of our how profit margin is distributed across large Australian companies?\n\nYOUR_DATAFRAME |&gt;\n    YOUR_FUNCTION_NAME(YOUR_VARIABLE_NAME &gt; YOUR_HIGHER_LOWER_BOUND_VALUE) |&gt;\n    YOUR_FUNCTION_NAME(aes(x = YOUR_VARIABLE_NAME)) + \n    YOUR_FUNCTION_NAME(fill = \"darkorange\", color = \"black\", \n                       alpha = 0.7, bins = 100\n                       ) +  \n    labs(title = \"Distribution of Profit Margin for Sample of Large ASX Companies in 2024\", \n         x = YOUR_LABEL, \n         y = YOUR_LABEL\n         ) + \n    theme_minimal() \n\n(e). Using the plot produced above, what would you consider to be the typical large Australian company’s profit margin? What would you consider a high-margin firm to be? Explain why it is not possible for the distribution of profit margin to have a very long right tail (i.e., firms that enjoy extremely large profit margins)?\n\n\nExercise 5: Unpacking the Profit Margin-Asset Turnover Scatterplot\nIn exercise 2, we created the following plot:\n\n\n\n\n\n\n\n\n\n(a). Refer to the above plot. Firms that employ a ‘high margin’ strategy are located in which quadrant? Firms that employ which strategy are located in the bottom-right quadrant?\n(b). One way to ‘check’ your answers to the above question is to use color to identify each firm’s strategy type. Use the starter code below and explain what the plot reveals.\n\nYOUR_DATAFRAME |&gt; \n    YOUR_FUNCTION_NAME(profit_margin &gt; -2) |&gt; \n    YOUR_FUNCTION_NAME() + \n    YOUR_FUNCTION_NAME(aes(x = YOUR_VARIABLE_NAME, y = YOUR_VARIABLE_NAME, \n                           fill = YOUR_VARIABLE_NAME\n                           ), \n                       alpha = 0.7, shape = 21, size = 3, color = \"black\"\n                      ) + \n    geom_vline(xintercept = median(YOUR_DATAFRAME$YOUR_VARIABLE_NAME, na.rm = TRUE), \n               linetype = \"dashed\", color = \"blue\", size = 1\n               ) + \n    geom_hline(yintercept = median(YOUR_DATAFRAME$YOUR_VARIABLE_NAME, na.rm = TRUE), \n               linetype = \"dashed\", color = \"blue\", size = 1\n               ) + \n    scale_fill_okabe_ito() +\n    theme_minimal() + \n    labs(title = \"Variation in Firm Strategy Across A Sample of Large ASX Companies in 2024\", \n             x = YOUR_LABEL, \n             y = YOUR_LABEL\n         ) + \n    guides(color = guide_legend(title = NULL), \n           fill = guide_legend(title = YOUR_LABEL))\n\n(c). Run the first block of code provided below. Which firm from the plot above does this code identify? Run the second block of code provided below. And, which firm from the plot above does this code identify?\n\nfirm_strategy |&gt; \n    filter(strategy_type == 'High volume') |&gt;\n    arrange(profit_margin) |&gt;\n    select(conml, industry, ebit, at, sale, profit_margin, asset_turnover, roic) |&gt;\n    head(1)\n\n\nfirm_strategy |&gt; \n      filter(strategy_type == 'High margin') |&gt;\n      arrange(asset_turnover) |&gt;\n      select(conml, industry, ebit, at, sale, profit_margin, asset_turnover, roic) |&gt;\n      head(1)\n\n(d). The plots above show that some strategy types are more common than others among our sample of large ASX companies in 2024. Use the starter code below that provides the count of observations for each strategy type in our sample. Which strategy type is more common, high margin or high volume?\n\nYOUR_DATAFRAME |&gt;\n    YOUR_FUNCTION_NAME(profit_margin &gt; -2) |&gt;\n    group_by(YOUR_VARIABLE_NAME) |&gt;\n    YOUR_FUNCTION_NAME(obs = n())\n\n(e). Let’s dig down into one of these firm strategy types, High margin. Write and run a chunk of code that counts the number of firms from each industry that employ a high-margin strategy. Write your code such that it outputs a table that shows the five industries with the highest counts. Which industry accounts for the most high-margin firms in our sample?\n\n# Write your answer here\n\n(f). Now, write and run a chunk of code that produces the equivalent table but for firms that employ a high-volume strategy. Compare this table to the one you produced in the previous question.\n\n# Write your answer here\n\n(g). In the lecture, we examined firms from the ASX’s ‘big industries’ (i.e., the seven industries that account from more than half of all large listed firms in Australia). Let’s gain some insight into how strategy type varies within each of these big industries. Use the starter code below to produce a stacked bar plot that shows the share of each industry’s firms that employ a given strategy type. What are the main insights that this plot reveals? Which industries have the most variation in strategy type? The least?\n\nYOUR_DATAFRAME |&gt;  \n    # Use %in% for filtering industries\n    YOUR_FUNCTION(profit_margin &gt; -2 & industry %in% big_industries) |&gt;\n    # Group by both industry and strategy_type\n    group_by(YOUR_VARIABLE_NAME, YOUR_VARIABLE_NAME) |&gt;  \n    # Count the number of occurrences for each strategy_type within each industry\n    YOUR_FUNCTION_NAME(count = n(), .groups = 'drop') |&gt;  \n    # Map 'industry' to x-axis and 'strategy_type' to fill\n    ggplot(aes(x = YOUR_VARIABLE_NAME, y = YOUR_VARIABLE_NAME, fill = YOUR_VARIABLE_NAME)) +  \n    geom_bar(stat = \"identity\") +\n    theme_minimal() +  \n    # Rotate and align x-axis labels\n    theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1),  \n          # Remove the legend title\n          legend.title = element_blank()\n          ) +\n    # Wrap x-axis labels at width 10 characters\n    scale_x_discrete(labels = function(x) str_wrap(x, width = 10)) +  \n    # Use color palette for filling\n    scale_fill_okabe_ito() +  \n    labs(#No label for x-axis\n         x = NULL,\n         y = \"No. of Firms\",\n         title = \"Strategy Type Across Large Australia Comapnies in 2024: The 'Big' Industries\"\n         )  \n\nYour plot should look like this:",
    "crumbs": [
      "Tutorials",
      "Tutorial 4: Data Wrangling for Business Analytics"
    ]
  },
  {
    "objectID": "tutorials/data_tidy.html",
    "href": "tutorials/data_tidy.html",
    "title": "Tutorial 5: Shaping and Joining Data for Business Analytics",
    "section": "",
    "text": "The Business Challenge",
    "crumbs": [
      "Tutorials",
      "Tutorial 5: Shaping and Joining Data for Business Analytics"
    ]
  },
  {
    "objectID": "tutorials/data_tidy.html#the-business-challenge",
    "href": "tutorials/data_tidy.html#the-business-challenge",
    "title": "Tutorial 5: Shaping and Joining Data for Business Analytics",
    "section": "",
    "text": "The Topic: How well do Australian firms’ earnings explain what investors are willing to pay for these companies?\nShare prices of Australia’s biggest companies go up and down every day. These changes reflect how investors feel about the company’s future earnings — based on news, forecasts, and the broader economy.\nTwo key numbers help us understand this:\nEarnings per share (EPS), tells us how much profit a company earns for each share:\n\\[\n\\text{EPS} = \\frac{\\text{Profit}}{\\text{Shares Outstanding}}\n\\]\nThe Price-to-Earnings (P/E) Ratio shows how much investors are willing to pay for $1 of earnings:\n\\[\n\\text{P/E} = \\frac{\\text{Share Price}}{\\text{EPS}}\n\\]\nTo tie these measures back to our earlier material on firm’s Earnings Before Interest and Taxes (EBIT), we can define firm’s earnings that are ultimately paid out to shareholders as follows:\n\\[\n\\text{Earnings} = \\text{EBIT} - \\text{Interest} - \\text{Taxes}\n\\]\nWe’ll use real data from major Australian companies listed on the ASX to practise cleaning, combining, and analysing financial variables like EPS and the P/E ratio. In particular we will:\n\nTidy messy financial data\nJoin multiple datasets together\n\n\n\n\nThe Data: Share Prices and Earnings\nWe’ll be using real data on Australian companies from Yahoo Finance. It includes key financial info:\n\ngvkey: a unique company ID we can use to match data across files\nprice: each company’s share price at the end of the year\neps: earnings per share – how much profit each share earns\npe: price-to-earnings ratio – how much investors are paying per dollar of earnings\n\nWe also have a separate file with industry details:\n\ngsubind: a code showing what industry a company is in\nsubind: the name of that industry (like “Gold” or “Construction & Engineering”)\n\nTogether, these files let us compare company valuations and see how they differ across industries.\n\n\nThe Method: Shaping and Combining Data with tidyr and dplyr\nData often needs cleaning before it’s ready to analyse. It might be too wide (with different years in separate columns), too long, or split across multiple files.\nIn this tutorial, we’ll use two tidyverse tools to fix that:\n\ntidyr: to reshape messy data using functions like pivot_longer()\ndplyr: to join different datasets together and spot what’s missing\n\nThese tools will help us:\n\nMake messy data easier to work with\nCombine company data with industry labels",
    "crumbs": [
      "Tutorials",
      "Tutorial 5: Shaping and Joining Data for Business Analytics"
    ]
  },
  {
    "objectID": "tutorials/data_tidy.html#getting-set-up-and-loading-the-data",
    "href": "tutorials/data_tidy.html#getting-set-up-and-loading-the-data",
    "title": "Tutorial 5: Shaping and Joining Data for Business Analytics",
    "section": "Getting set up and loading the data",
    "text": "Getting set up and loading the data\n\nR packages for today\n\nlibrary(tidyverse)     # collection of packages for data manipulation and visualization",
    "crumbs": [
      "Tutorials",
      "Tutorial 5: Shaping and Joining Data for Business Analytics"
    ]
  },
  {
    "objectID": "tutorials/data_tidy.html#prepare-these-exercises-before-class",
    "href": "tutorials/data_tidy.html#prepare-these-exercises-before-class",
    "title": "Tutorial 5: Shaping and Joining Data for Business Analytics",
    "section": "Prepare these Exercises before Class",
    "text": "Prepare these Exercises before Class\nPrepare these exercises before coming to class. Plan to spend 45 minutes on these exercises.\n\nExercise 1: Identifying Issues with Un-tidy Data\nWe’ll start with a deliberately messy version of the ASX stock price data. Run the following code to load and implement some preliminary manipulations to this dataset.\n\nasx_prices_messy &lt;- \n    read_csv(\"data/asx_prices_messy.csv\")\n\n\nasx_prices_messy |&gt; \n    select(gvkey, conm, price_2023, price_2024, \n           eps_2023, eps_2024) |&gt;\n    head(10)\n\n# A tibble: 10 × 6\n   gvkey  conm                      price_2023 price_2024 eps_2023 eps_2024\n   &lt;chr&gt;  &lt;chr&gt;                          &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 327267 1414 DEGREES LTD               0.04       0.067  -0.0091  -0.0107\n 2 349318 29METALS LIMITED               0.375     NA      -0.799   NA     \n 3 284698 3D ENERGI LIMITED              0.056      0.07    0.0129  -0.0075\n 4 317959 3P LEARNING LTD                1.27       1.02    0.023   -0.208 \n 5 339106 4DMEDICAL LTD                  0.45       0.675  -0.105   -0.0971\n 6 314650 4DS MEMORY LTD                 0.135      0.084  -0.0038  -0.0031\n 7 324960 5E ADVANCED MATERIALS INC      2.26       0.54   -0.7     -1.18  \n 8 325746 5G NETWORKS LIMITED            0.125      0.148  -0.0579  -0.0836\n 9 253429 88 ENERGY LTD                  0.005     NA      -0.0006  NA     \n10 323980 8COMMON LTD                    0.075      0.041  -0.0147  -0.0114\n\n\n(a). In what ways is this data frame not tidy? Be specific: which tidy‑data principles are violated?\n(b). Suppose we want the average share price across all firms for the period 2023–2024. With this messy layout you will need to compute sum and counts across columns (i.e., sum prices for 2023, for 2024; then, count non-missing observations in these columns for 2023, 2024, etc). Use the starter code below to compute the average share price across all firms for the period 2023-2024.\n\nasx_prices_messy |&gt;\n    summarise(\n        # Sum the prices from the 2023 year\n        sum_prices_2023   = sum(YOUR_CODE_HERE, na.rm = TRUE),\n        # Sum the prices from the 2024 year\n        sum_prices_2024   = sum(YOUR_CODE_HERE, na.rm = TRUE),\n        # Count the number of firms with non-missing\n        # prices in 2023\n        count_firms_2023 = sum(!is.na(price_2023)),\n        # Count the number of firms with non-missing\n        # prices in 2024\n        count_firms_2024 = sum(!is.na(price_2024))\n    ) |&gt;\n    mutate(\n        # Compute the total sum of prices across both years\n        total_sum   = YOUR_CODE_HERE + YOUR_CODE_HERE,\n        # Compute the total count of firms across both years\n        total_count = count_firms_2023 + count_firms_2024\n    ) |&gt;\n    mutate(\n        # Manually compute the average\n        avg_price_23_24 = YOUR_CODE_HERE / YOUR_CODE_HERE\n    ) |&gt; \n    select(avg_price_23_24)\n\n(c) Explain why the approach we took in (b) doesn’t scale well as more years of data are added.\n(d) Examining relationships between variables in a messy data frame can be especially tricky. Suppose we want to see whether share price and earnings per share are correlated in 2024. With this messy structure we must hard-code the year’s columns:\n\nasx_prices_messy |&gt;\n    filter(eps_2024 &gt; -5) |&gt;\n    ggplot(aes(x = eps_2024, y = price_2024)) +\n    geom_point(alpha = 0.4) +\n    geom_smooth(method = \"lm\", se = TRUE, color = \"blue\") +\n    labs(\n        title = \"Share Price vs EPS (2024)\",\n        x = \"Earnings per Share (EPS, 2024)\",\n        y = \"Share Price (2024)\"\n    ) +\n    theme_minimal()\n\n\n\n\n\n\n\n\nBased on this plot, what relationship do you see in 2024? Why does this make sense economically (what are investors “buying” when they invest)?\n(e). Now imagine you also wanted to include 2023 in the same analysis. You do not need to implement code to add these data points to the plot above — just explain the steps you’d need to take.\n\nWhich additional columns would you need to reference in the data?\nHow would your plotting code change?\nWhat problems do you foresee if you wanted to extend this further to 2019–2024?\n\n\n\nExercise 2: Combining Datasets\nIn this exercise, we’ll combine two or more data frames using joins. Sometimes, joining adds columns. Other times, it can add rows when duplicate keys exist.\nLet’s walk through both situations.\nTo start, let’s load the necessary data for this exercise - for two of these datasets, the code below also does some ‘light’ wrangling (similar to what we did to our datasets in last week’s tutorial):\n\nfirm_codes &lt;- \n    read_csv(\"data/asx_200_2024.csv\") |&gt; \n    select(gvkey, conm, gsubind)\n\nsubindustry_names &lt;-\n    read_csv(\"data/GICS_subindustry.csv\")\n\npe_data &lt;- \n    read_csv(\"data/pe.csv\") |&gt; \n    select(gvkey, fyear, pe) |&gt;\n    filter(!is.na(pe)) |&gt; \n    arrange(gvkey, fyear)\n\n(a). We start with a data frame that lists each firm’s subindustry code (gsubind), using the largest 200 firms on the ASX in 2024:\n\nfirm_codes |&gt; \n    head(10)\n\n# A tibble: 10 × 3\n   gvkey  conm                    gsubind\n   &lt;chr&gt;  &lt;chr&gt;                     &lt;dbl&gt;\n 1 013312 BHP GROUP LTD          15104020\n 2 210216 TELSTRA GROUP LIMITED  50101020\n 3 223003 CSL LTD                35201010\n 4 212650 TRANSURBAN GROUP       20305020\n 5 100894 WOOLWORTHS GROUP LTD   30101030\n 6 212427 FORTESCUE LTD          15104050\n 7 101601 WESFARMERS LTD         25503030\n 8 226744 RAMSAY HEALTH CARE LTD 35102020\n 9 220244 QANTAS AIRWAYS LTD     20302010\n10 017525 ORIGIN ENERGY LTD      55101010\n\n\nWe also have a lookup table that tells us the name of each subindustry:\n\nsubindustry_names |&gt; \n    head(10)\n\n# A tibble: 10 × 2\n    gsubind subind                              \n      &lt;dbl&gt; &lt;chr&gt;                               \n 1 10101010 Oil & Gas Drilling                  \n 2 10101020 Oil & Gas Equipment & Services      \n 3 10102010 Integrated Oil & Gas                \n 4 10102020 Oil & Gas Exploration & Production  \n 5 10102030 Oil & Gas Refining & Marketing      \n 6 10102040 Oil & Gas Storage & Transportation  \n 7 10102050 Coal & Consumable Fuels             \n 8 15101010 Commodity Chemicals                 \n 9 15101020 Diversified Chemicals               \n10 15101030 Fertilizers & Agricultural Chemicals\n\n\nUse a left join to add the subindustry name to each firm in the firm_codes dataset using the variable gsubind as the join key. The code below gets you started:\n\nfirm_info &lt;- \n    YOUR_DATASET_NAME |&gt;\n    left_join(YOUR_DATASET_NAME, by = \"YOUR_VARIABLE_NAME\")\n\nfirm_info |&gt; \n    head(10)\n\n(b). What kind of join did we just perform? To help answer this question, consider the following:\n\nWhat kind of thing are we adding: columns or rows?\nDid each gsubind only appear once in the dataset subindustry_names?\nDoes this kind of join have a name? If so, what is it?\n\n(c).\nNow let’s bring in PE ratio data, which has multiple years per firm:\n\npe_data |&gt; \n    head(10)\n\n# A tibble: 10 × 3\n   gvkey  fyear    pe\n   &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1 005302  2008  41.7\n 2 010991  2004  27.2\n 3 010991  2005  41.8\n 4 010991  2006  45.0\n 5 010991  2007  29.7\n 6 010991  2008 135. \n 7 010991  2021 979. \n 8 010991  2023  57.0\n 9 013312  2005  21.3\n10 013312  2006  14.8\n\n\nUse a left join to add the P/E data to each firm in the firm_info dataset using the variable gvkey as the join key. Use the starter code below to implement this join:\n\nfirm_pe_data &lt;- \n    YOUR_DATASET_NAME |&gt;\n    YOUR_JOIN_TYPE(YOUR_DATASET_NAME, by = \"YOUR_VARIABLE_NAME\")\n\nfirm_pe_data |&gt;\n  head(10) |&gt;\n  select(pe, everything())\n\n(d). What kind of join did we just perform? To help answer this question, consider the following:\n\nWhat kind of thing are we adding: columns or rows?\nHow many rows are in the dataset firm_pe_data? How about firm_info?\nDoes each gvkey in the original table (firm_info) now appear once or multiple times in the ‘joined’ data frame (firm_pe_data)? Why?\nDoes this kind of join have a name? If so, what is it?",
    "crumbs": [
      "Tutorials",
      "Tutorial 5: Shaping and Joining Data for Business Analytics"
    ]
  },
  {
    "objectID": "tutorials/data_tidy.html#in-class-exercises",
    "href": "tutorials/data_tidy.html#in-class-exercises",
    "title": "Tutorial 5: Shaping and Joining Data for Business Analytics",
    "section": "In-Class Exercises",
    "text": "In-Class Exercises\nYou will discuss these exercises in class with your peers in small groups and with your tutor. These exercises build from the exercises you have prepared above, you will get the most value from the class if you have completed those above before coming to class.\n\nExercise 3: Tidying and Unlocking Scalable Insights\n(a). We’ll now tidy the messy stock price dataset from Exercise 1 so we can more easily analyse and visualise it. Use the starter code below to turn your wide-format data back into a tidy format using pivot_longer().\n\nasx_prices_tidy &lt;- \n    asx_prices_messy |&gt;\n    pivot_longer(\n        cols = YOUR_CODE_HERE,\n        names_to = c(\".value\", \"fyear\"),\n        names_sep = YOUR_CODE_HERE,\n        values_drop_na = TRUE\n    ) |&gt;\n    mutate(fyear = as.numeric(fyear))\n\nasx_prices_tidy |&gt; \n    head(10)\n\n(b). Why is this structure better for analysis? Why might calculations or visualisations now be easier?\n(c). Let’s re-do a simple tasks from Exercise 1, that was quite complicated to implement — and see how much easier it is now that our data is tidy. What’s the average share price for all firms across 2023 and 2024?\n\nasx_prices_tidy |&gt;\n    YOUR_CODE(fyear %in% c(YOUR_YEARS)) |&gt;\n    YOUR_CODE(avg_price = mean(YOUR_VARIABLE, na.rm = TRUE))\n\n(d). Let’s re-do our plot from Exercise 1 too. Plot price vs EPS over the full sample (2019–2024).\n(e). Suppose we wanted to modify the plot in (d) to only include the years 2023 and 2024. Modify the code you just wrote to make this change by adding one new line of code.\n(f). What kind of relationship do you observe in the plot in (d)? What might explain firms that lie above or below the fitted line? For EPS &lt; 0, does our line-of-best-fit ‘make sense’?\n\n\nExercise 4: What’s Missing? Understanding Joins by Seeing What They Drop\nIn Exercise 2, we used left_join() to combine data from different tables — like firm details, subindustry names, and P/E ratios. That helped us build one data frame with all the information in one place. Now, we’ll go a step further and explore what different kinds of joins keep and drop. This can help us spot missing or underrepresented data — and understand our dataset more deeply.\n(a). Let’s start by counting how many firms in the ASX200 belong to each subindustry. Use the starter code below to return the three most common industries:\n\nfirm_info |&gt; \n    YOUR_CODE(subind) |&gt; \n    YOUR_CODE(desc(n)) |&gt; \n    YOUR_CODE(3)\n\nAnd, now the three least common:\n\nfirm_info |&gt; \n    YOUR_CODE(subind) |&gt; \n    YOUR_CODE((n)) |&gt; \n    YOUR_CODE(3)\n\n(b). Are there any subindustries that don’t appear in the ASX200 sample-and so are even less common those that we identify in (a)?\nWe can use anti_join() to find subindustries listed in subindustry_names but missing from our firms data. How many subindustries are not included in the ASX200?\n\nYOUR_CODE |&gt; \n    YOUR_CODE(firm_info, by = \"subind\") |&gt; \n    nrow()\n\n(c). What kinds of industries are missing? Why might they be underrepresented in the ASX200?\n(d). An inner_join() keeps only the firms that have a matching subindustry in the lookup table. Recreate the join you did in Exercise 2(a), but this time use inner_join() instead of left_join().\n\nfirm_info_inner &lt;- \n    firm_codes |&gt;\n    YOUR_CODE(subindustry_names, by = \"gsubind\")\n\n(e) Are there any differences in the data frame in (d) to the one you constructed in 2 (a)? Explain your answer.\n(f). Over this exercise and Exercise 2, you’ve used three different types of joins:\n\nleft_join()\nanti_join()\ninner_join()\n\nEach one gives you a slightly different view of how your data frames relate to each other.\nIn your own words, answer the following:\n\nWhat does each join keep and what does it drop?\nWhich join is most useful when you want to preserve all rows from one table, even if they don’t match?\nWhich join is best for finding mismatches between two tables?\nWhich join only keeps perfect matches between the two tables?",
    "crumbs": [
      "Tutorials",
      "Tutorial 5: Shaping and Joining Data for Business Analytics"
    ]
  },
  {
    "objectID": "tutorials/data_storage.html",
    "href": "tutorials/data_storage.html",
    "title": "Tutorial 6: Storing & Retrieving Data",
    "section": "",
    "text": "The Business Challenge: Identifying Shill Bidding\nOnline auctions are big business. Platforms like eBay, Yahoo! Auctions, and specialist marketplaces connect millions of buyers and sellers every day. But with opportunity comes risk: not every bidder is playing fair.\nOne form of fraud is called shill bidding. This happens when sellers (or their associates) secretly bid on their own items. The goal isn’t to win the auction, but to drive up the price and trick genuine buyers into paying more than they otherwise would. Shill bidders might:\nFor an auction platform, shill bidding creates serious problems:\nYour task in this exercise is to step into the role of an analyst at an online auction company. You’ve been asked to examine bidding data and look for patterns that might indicate suspicious activity. To do this, you’ll need to:",
    "crumbs": [
      "Tutorials",
      "Tutorial 6: Storing & Retrieving Data"
    ]
  },
  {
    "objectID": "tutorials/data_storage.html#the-business-challenge-identifying-shill-bidding",
    "href": "tutorials/data_storage.html#the-business-challenge-identifying-shill-bidding",
    "title": "Tutorial 6: Storing & Retrieving Data",
    "section": "",
    "text": "Place bids just high enough to outbid real customers, forcing them to keep bidding.\nUse multiple accounts or devices to hide their identity.\nBid early and often to create the illusion of demand.\n\n\n\nIt damages buyer trust, which can reduce participation and long-term revenue.\nIt raises legal and reputational risks if fraud is not detected.\nIt can distort the competitive landscape for honest sellers.\n\n\n\nExplore the auctions data provided in multiple formats (CSV, JSON, database).\nUse wrangling and visualization tools to spot differences between ordinary buyers and suspicious bidders.\nInterpret these patterns in business terms — would a manager at the auction platform be worried?",
    "crumbs": [
      "Tutorials",
      "Tutorial 6: Storing & Retrieving Data"
    ]
  },
  {
    "objectID": "tutorials/data_storage.html#the-data-auction-bids-and-devices",
    "href": "tutorials/data_storage.html#the-data-auction-bids-and-devices",
    "title": "Tutorial 6: Storing & Retrieving Data",
    "section": "The Data: Auction Bids and Devices",
    "text": "The Data: Auction Bids and Devices\nWe’ll be working with synthetic data that simulates an online auction platform. It comes in a few different formats:\n\nCSV / SQLite: structured data on auctions, bidders, bid amounts, and timestamps.\nJSON: extra details about each bidder’s activity, such as the devices or sessions they used.\n\nTogether, these files let us see not just who is bidding and how much, but also how they are bidding — which can be a clue for spotting suspicious behavior.",
    "crumbs": [
      "Tutorials",
      "Tutorial 6: Storing & Retrieving Data"
    ]
  },
  {
    "objectID": "tutorials/data_storage.html#the-method-wrangling-data-across-backends",
    "href": "tutorials/data_storage.html#the-method-wrangling-data-across-backends",
    "title": "Tutorial 6: Storing & Retrieving Data",
    "section": "The Method: Wrangling Data Across Backends",
    "text": "The Method: Wrangling Data Across Backends\nIn this tutorial, the focus isn’t just on wrangling data — it’s on seeing how the same tidyverse tools work across different backends. We’ll be using:\n\ndplyr: to filter, group, summarise, and join data.\ntidyr: to reshape nested or messy data into tidy tables.\n\nYou’ll apply these verbs on top of different data sources:\n\nFlat files (CSV)\nDatabases (SQLite)\nNested structures (JSON)\n\nThe key idea is that once you know the tidyverse workflow, it carries over no matter where the data lives.\n\nlibrary(tidyverse) # Core tidyverse for wrangling and plotting\nlibrary(patchwork) # Combine multiple plots\nlibrary(jsonlite) # For reading JSON files\nlibrary(DBI) # For working with databases\nlibrary(duckdb) # for working with duckdb databases",
    "crumbs": [
      "Tutorials",
      "Tutorial 6: Storing & Retrieving Data"
    ]
  },
  {
    "objectID": "tutorials/data_storage.html#prepare-these-exercises-before-class",
    "href": "tutorials/data_storage.html#prepare-these-exercises-before-class",
    "title": "Tutorial 6: Storing & Retrieving Data",
    "section": "Prepare these Exercises before Class",
    "text": "Prepare these Exercises before Class\nPrepare these exercises before coming to class. Plan to spend 30 minutes on these exercises.\n\nExercise 1: Auction results baseline.\n(a). Read the CSV file located at data/auction_results.csv and examine its structure by completing the code below:\n\n# Load the data\nauction_results &lt;-\n    YOUR_CODE(\"data/auction_results.csv\")\n\n# Examine the structure\nYOUR_CODE(auction_results)\n\n(b). What does one row represent in the dataset? Which columns look most informative for understanding auction outcomes?\n(c). Produce 3 plots using the starter code below:\n\nA histogram of final_price\nA histogram of num_bids\nA scatterplot of the relationship between final_price and num_bids.\n\n\np1 &lt;-\n    auction_results |&gt;\n    ggplot() +\n    geom_histogram(aes(x = YOUR_CODE)) +\n    theme_minimal()\n\np2 &lt;-\n    auction_results |&gt;\n    ggplot() +\n    YOUR_CODE(aes(x = YOUR_CODE), bins = YOUR_CODE) +\n    theme_minimal()\n\np3 &lt;-\n    auction_results |&gt;\n    ggplot(aes(x = YOUR_CODE, y = YOUR_CODE)) +\n    YOUR_CODE() +\n    geom_smooth(method = \"lm\") +\n    theme_minimal()\n\np1 / p2 / p3\n\n(d). Provide a short explanation of your findings from (c). Are there any patterns that are suggestive of shill bidding?\n(e). Next, let’s explore how certain auction outcomes vary across sellers. Complete the starter code below to produce summary statistics by seller:\n\nseller_profile &lt;- \n    auction_results |&gt;\n    group_by(seller_id) |&gt;\n    summarise(\n        n_auctions    = YOUR_CODE(),\n        median_price  = YOUR_CODE(final_price, na.rm = TRUE),\n        median_bids   = YOUR_CODE(num_bids, na.rm = TRUE),\n        share_missing_winner = YOUR_CODE(is.na(YOUR_CODE)) / n_auctions,\n        .groups = \"drop\"\n    ) |&gt;\n    arrange(YOUR_CODE(n_auctions))\n\n(f). Based on the seller profiles in (e), are there any patterns suggestive of shill bidding? What further information would you need to confirm any suspicion?\n\n# Write your answer here\n\n\n\nExercise 2: Digging Deeper into User Behaviour\nIn Exercise 1, we built a market baseline from auction outcomes: prices, bid counts, and seller summaries. That showed us where to look, but not how those outcomes happened. Next, we’ll bring in behavioral telemetry from a JSON file (devices/sessions). Using dplyr/tidyr verbs, we’ll flatten the nested JSON and link it to our auctions so we can ask: who was active, on which devices, and how activity clustered around particular sellers.\n(a). An example record in the JSON file data/bids.jsonl is as follows:\n{\n  \"auction_id\": \"A00001\",\n  \"bid_time\": \"2024-11-28T23:27:00Z\",\n  \"bidder\": {\n    \"id\": \"B00153\",\n    \"rating\": 83,\n    \"country\": \"AU\"\n  },\n  \"amount\": 21.43,\n  \"device\": {\n    \"os\": \"iOS\",\n    \"browser\": \"Firefox\"\n  },\n  \"auto_bid_flag\": false,\n  \"ip_address\": \"203.0.113.189\"\n}\nExplain the structure of this record in words (what’s at the top level vs. what’s nested, and what fields each nested object contains).\n(b). Now let’s load the data from the file data/bids.jsonl and examine the first two rows. Complete the starter code below and identify any nested columns:\n\n\n\n\n\n\nTipStreaming JSON has a different structure.\n\n\n\n\n\nThis JSON file has a different structure than we saw in the reading and in the lecture. It’s called streaming JSON (or sometimes “JSON Lines”) — one complete JSON record per line, with no surrounding [] array and no commas between records.\nThink of it like a log file: one complete JSON record per line. (Regular JSON is often one big list: [{...}, {...}, ...].)\nWhy it’s used: handles large files smoothly — you can read it line by line without loading everything.\nHow to load in R: use stream_in() (not read_json()`).\nCommon gotcha: don’t add brackets or commas between lines; each line must be valid JSON on its own.\n\n\n\n\nbids_json &lt;- stream_in(file(\"YOUR_FILE_NAME\"))\n\nhead(YOUR_DATA_SET, 2)\n\n(c) Complete the code below to unnest the JSON data:\n\nbids_json &lt;-\n    bids_json  |&gt; \n    unnest_wider(YOUR_VARIABLE, names_sep = \"_\") |&gt; \n    YOUR_FUNCTION(YOUR_VARIABLE, names_sep = \"_\") \n\n(d). One piece of evidence pointing towards shill bidding is if multiple bidders are using the same IP address. Explain why this is the case, then complete the code below to isolate those instances.\n\n\n\n\n\n\nTip\n\n\n\n\n\nWhat’s an IP address (and why we care)?\n\nThink of an IP address like a street address for a device on the internet. It helps websites know where to send data.\nOn most shared Wi-Fi (homes, offices, libraries), many people/devices can appear under the same public IP. So “same IP” ≠ “same person.”\nIPs can change over time (dynamic addresses) and can be hidden/changed with VPNs — so they’re imperfect identity signals.\n\n\n\n\n\nsame_ip &lt;-\n    bids_json |&gt;\n    distinct(auction_id, ip_address, bidder_id) |&gt;\n    count(YOUR_VARIABLE, YOUR_VARIABLE) |&gt;\n    filter(YOUR_CONDITION) |&gt;\n    arrange(desc(n)) \n\n(e). Another possible sign of shill bidding could be auto-bidding. Explain when auto-bids could be signs of shill-bids versus legitimate behaviour. Complete the code below to measure the share of auto-bids per auction.\n\n\n\n\n\n\nTip\n\n\n\n\n\nWhat is auto-bidding (proxy bidding)?\n\nIt’s a feature where you set a maximum price you’re willing to pay and the system bids for you automatically in small steps to keep you on top—without you watching the auction.\nWhy people use it: convenience and to avoid overbidding in the moment.\nWhat it looks like in the data: a series of quick, incremental bids appearing from the same bidder as others place bids.\n\nImportant: auto-bidding is normal. It’s only review-worthy if you also see other flags (e.g., shared IPs, unusual timing patterns, repeated ties to the same seller).\n\n\n\n\nauto_bids &lt;-\n    bids_json |&gt;\n    group_by(auction_id) |&gt;\n    summarise(\n        num_auto_bids = sum(YOUR_VARIABLE, na.rm = TRUE),\n        total_bids    = YOUR_FUNCTION(),\n        share_auto    = YOUR_VARIABLE / YOUR_VARIABLE\n    ) |&gt;\n    arrange(desc(num_auto_bids))\n\n(f) Let’s combine the data on shared IP addresses and auto-bids into one data-frame. Complete the code below, and then identify any auctions that might have seen shill bidding. Explain your answer.\n\n# aggregate IP data to the auction level for easy join\nsame_ip_auctions &lt;-\n    same_ip |&gt;\n    group_by(auction_id) |&gt;\n    summarise(\n        n_shared_ips = n(),           # how many IPs in this auction were used by 2+ bidders\n        max_shared_n = max(n),        # largest number of bidders sharing a single IP\n        .groups = \"drop\"\n    )\n\nshill_signals &lt;- \n    YOUR_DATA |&gt;\n    YOUR_FUNCTION(YOUR_DATA, by = \"auction_id\") |&gt; \n    arrange(desc(n_shared_ips), desc(share_auto))\n\n\n# Write your answer here",
    "crumbs": [
      "Tutorials",
      "Tutorial 6: Storing & Retrieving Data"
    ]
  },
  {
    "objectID": "tutorials/data_storage.html#in-class-exercises",
    "href": "tutorials/data_storage.html#in-class-exercises",
    "title": "Tutorial 6: Storing & Retrieving Data",
    "section": "In-Class Exercises",
    "text": "In-Class Exercises\nYou will discuss these exercises in class with your peers in small groups and with your tutor. These exercises build from the exercises you have prepared above, you will get the most value from the class if you have completed those above before coming to class.\n\nExercise 3: Working with a Database\nOur next data source is a database so we can join tables at scale (bids, auctions, items, users, orders) and study relationships: who bids on whom, how concentrated that behaviour is, and whether it translates into wins. Using the same dplyr verbs on a database backend, we’ll push work to the database and only collect() results when needed.\n\n\n\n\n\n\nWarningQuarto gotcha: close your DuckDB connection before you Render\n\n\n\n\n\nQuarto renders your document in a fresh R session. If you opened a DuckDB connection in the Console (or earlier chunk) and leave it open, that session may lock the .duckdb file, causing errors like “database is locked” or “unable to open database file.”\nYou can fix this by following one of these routes:\n\nManage the connection inside the document and close it at the end\n\n\nlibrary(DBI); \nlibrary(duckdb)\ncon &lt;- dbConnect(duckdb(), dbdir = \"data/auctions.duckdb\")\n\n# ... your analysis ...\n\ndbDisconnect(con, shutdown = TRUE)\n\nIf you connected in the Console, run:\n\ndbDisconnect(con, shutdown = TRUE)\n\nbefore clicking Render.\n(If you still see a lock—especially on Windows—restart R and render again.)\n\n\n\n(a). Explain how a database differs from the CSV and JSON data types we have seen so far.\n(b). Connect to the database stored in data/auctions.duckdb and list the tables present in it by completing the code below.\n\ncon &lt;- dbConnect(duckdb(), dbdir = \"YOUR_FILENAME\")\n\ndbListTables(YOUR_CODE)\n\n(c) Find what information is present in each table by completing the code below.\n\ntbl(con, \"bids\")       |&gt; glimpse()\ntbl(con, \"YOUR_TABLE\") |&gt; glimpse()\ntbl(con, \"YOUR_TABLE\") |&gt; YOUR_FUNCTION()\nYOUR_FUNCTION(con, \"YOUR_TABLE\") |&gt; YOUR_FUNCTION()\nYOUR_FUNCTION(YOUR_OBJECT, \"YOUR_TABLE\") |&gt; YOUR_FUNCTION()\n\n(d) For each bidder, count their total number of bids and the number of auctions they have particpated in by completing the code below.\n\nbidder_activity &lt;- \n    tbl(con, \"bids\") |&gt;\n    group_by(YOUR_VARIABLE) |&gt;\n    YOUR_VARIABLE(\n        total_bids = YOUR_FUNCTION(),\n        auctions_participated = YOUR_FUNCTION(YOUR_VARIABLE),\n        .groups = \"drop\"\n    ) |&gt;\n    collect()\n\n(e) A colleague suggests that you look at links between buyers and sellers. We will want to see how often does the same bidder bid against the same seller, and across how many auctions. Why might these metrics useful for identifying possible shill bidding behaviour and could they be suggestive of something more innocent?\n(f). We’ll go a slightly different route and instead compute the share of one bidder’s bids that go to their top seller. Is there an advantage to this versus your colleagues suggestion in (e)?\n(g) Complete the code below to compute the share of one bidder’s bids that go to their top seller. Note that there are many small steps to achieve this goal. Think through why each is necessary\n\n# Step 1: rename to make easier to track\nbids &lt;- \n    tbl(con, \"bids\") |&gt;\n    rename(bidder_id = user_id)\n\n# Step 2: Link bids to the seller who's \n#         auction they are participating in\n#\n# Note: here we are selecting the min. number of columns\n#       we need to keep\nbids_items &lt;- \n    bids |&gt;\n    left_join(tbl(con, \"auctions\") |&gt; \n                  select(auction_id, item_id),\n                by = \"auction_id\") |&gt;\n    left_join(tbl(con, \"items\")    |&gt; \n                  select(item_id, seller_id),\n                by = \"item_id\")\n\n# Step 3: How many bids does a bidder make in total?\ntotal_bids_per_bidder &lt;- \n    bids_items |&gt;\n    YOUR_FUNCTION(bidder_id, name = \"total_bids\")\n\n# Step 4: How many bids does a bidder make to each seller?\nbidder_seller_bids &lt;- \n    bids_items |&gt;\n    YOUR_FUNCTION(YOUR_VARIABLE, YOUR_VARIABLE, name = \"bids_to_seller\")\n\n# Find the highest concentration a bidder bids go to one seller\nconcentration &lt;- \n    YOUR_DATASET |&gt;\n    YOUR_FUNCTION(YOUR_DATASET, \n              by = \"bidder_id\") |&gt;\n    mutate(seller_share = YOUR_VARIABLE / YOUR_VARIABLE) |&gt;\n    group_by(bidder_id) |&gt; \n    slice_max(seller_share, n = 1) |&gt; \n    collect()\n\n(h) Do any of these bid concentration ratios appear “too high”? Identify a set of bidders whose behaviour you want to explore more closely. Explain your decision making strategy here.\n(i) Why might possible shill bidder’s share of bids to one seller be an intermediate value, like 20 - 30 percent, rather than over 50%?\n(j) Concentrating one’s bidding to a given seller is likely not enough. People who shill bid do not want to win the auction. Calculate how often the bidders win the auctions they participate in.\n\n# Step 1: winners per auction\nwinners &lt;- \n    tbl(con, \"orders\") |&gt;\n    select(auction_id, winner_id)\n\n# Step 2: distinct auctions each bidder joined\nbidder_auctions &lt;- \n    bids_items |&gt;\n    YOUR_FUNCTION(bidder_id, auction_id)\n\n# Step 3: join winners and compute win_rate\nwin_rates &lt;- \n    bidder_auctions |&gt;\n    YOUR_FUNCTION(winners, by = \"auction_id\") |&gt;\n    mutate(won = bidder_id == winner_id) |&gt;\n    group_by(bidder_id) |&gt;\n    summarise(\n        auctions_participated = YOUR_FUNCTION(),\n        auctions_won          = YOUR_FUNCTION(YOUR_VARIABLE, na.rm = TRUE),\n        .groups = \"drop\"\n    ) |&gt;\n    mutate(win_rate = YOUR_VARIABLE / YOUR_VARIABLE) |&gt; \n    collect()\n\n(k). Join the win_rate data to the concentration data and save the combined dataset to a file data/shill_metrics.csv.\n\nshill_metrics &lt;-\n    YOUR_DATASET |&gt; \n    YOUR_FUNCTION(YOUR_DATASET, by = \"bidder_id\")\n\nYOUR_FUNCTION(YOUR_DATASET, \"data/shill_metrics.csv\")\n\n(l). Create a scatter plot that shows the relationship between win_rate and seller_share. Are there any regions in this plot that are indicative of shill bidding?\n(m) (Optional, if time). Propose an alternative way to use the win_rate and concentration data, along with any other data you have access to in the exercise to identify possible shill bidders. Explain why your method works. You can explain your method intuitively, you do not need to code a solution.",
    "crumbs": [
      "Tutorials",
      "Tutorial 6: Storing & Retrieving Data"
    ]
  },
  {
    "objectID": "tutorials/data_collection.html",
    "href": "tutorials/data_collection.html",
    "title": "Tutorial 7: Obtaining Macroeconomic Data through FRED",
    "section": "",
    "text": "The Business Challenge\nThe macroeconomy is the stage on which every business decision plays out. No business, whether a global tech giant, a local café, or even a freelance consultant, operates in a vacuum. Shifts in GDP growth, spikes in inflation, surges in unemployment, or sudden changes in interest rates ripple through the economy and ultimately shape outcomes for each microeconomic entity.\nOver the past few decades, we have seen how powerful these forces can be. The 2008 global financial crisis reshaped credit markets and consumer demand for years. The COVID-19 pandemic disrupted supply chains, accelerated digital adoption, and left lasting changes in labor markets. Even today, firms must navigate the challenges of inflationary pressures, shifting interest rate policies, and geopolitical uncertainty.\nFor data analysts, we cannot answer micro-level business questions without accounting for the macroeconomic backdrop. In this session, you will step into the role of a macro-aware business analyst. Specifically, you will:\nTo carry out these tasks, we need high-quality and reliable macroeconomic data. The Federal Reserve Economic Data (FRED), managed by the Federal Reserve Bank of St. Louis, is one of the most widely used platforms by academics, policymakers, and industry analysts alike. It offers thousands of time series on the U.S. and global economy.\nWe will learn two ways to interact with FRED:\nBy the end of this workshop, you’ll not only gain hands-on experience with macroeconomic data, but also develop an appreciation for how the “big picture” shapes the smaller decisions businesses face every day.",
    "crumbs": [
      "Tutorials",
      "Tutorial 7: Obtaining Macroeconomic Data through FRED"
    ]
  },
  {
    "objectID": "tutorials/data_collection.html#the-business-challenge",
    "href": "tutorials/data_collection.html#the-business-challenge",
    "title": "Tutorial 7: Obtaining Macroeconomic Data through FRED",
    "section": "",
    "text": "Obtain real macroeconomic data on key indicators like GDP, unemployment, and inflation.\nVisualize and explore trends across different time periods, spotting patterns between macroeconomic indicators.\n\n\n\n\nDirectly through the FRED website, which allows quick searches and visualization.\nProgrammatically through the FRED API, which lets us pull large datasets directly into our analysis tools and integrate them into reproducible workflows.",
    "crumbs": [
      "Tutorials",
      "Tutorial 7: Obtaining Macroeconomic Data through FRED"
    ]
  },
  {
    "objectID": "tutorials/data_collection.html#prepare-these-exercises-before-class",
    "href": "tutorials/data_collection.html#prepare-these-exercises-before-class",
    "title": "Tutorial 7: Obtaining Macroeconomic Data through FRED",
    "section": "Prepare these Exercises before Class",
    "text": "Prepare these Exercises before Class\nPrepare these exercises before coming to class. Plan to spend 30 minutes on these exercises.\n\nExercise 1: Visualize and Download Macroeconomic Data Directly from the FRED Website\nFRED offers an easily accessible web-based platform for users to visualize and download macroeconomic data directly from their website (https://fred.stlouisfed.org/).\nSearch in FRED and find the pages of the following data:\n\nReal Gross Domestic Product (GDP)\nCore Consumer Price Index (CPI)\nUnemployment Rate\n\nRead through each page and think about the following questions (recap of Macroeconomics):\n\nWhat does “core CPI” mean here? How does it differ from normal CPI?\nWhat is the relation between CPI and inflation?\n\n\n\nExercise 2: Preparing for Obtaining Data through APIs\nIn addition to the web-based interactive platform, FRED offers APIs to those who would like to acquire data through programs and in large batches (see the page for details: https://fred.stlouisfed.org/docs/api/fred/).\nTo prepare for our practices during the workshop, make sure you have applied for the API key for FRED before the workshop (VERY IMPORTANT!). Below are the detailed steps\n\nVisit the FRED’s API webpage and click the link to “API Keys.”\n\n\n\nRequest your own API key:\n\n\n\nYou will see a pop-up window asking for signing in. Click “create new account” at the top and you will get to the following page:\n\n\n\nUse your Uni email address to register. For the question “In what context do you use FRED?”, choose “Student.”\nOnce you finish your registration, you will be automatically directed to the API request page. Click “+ Request API Key”.\n\n\n\nIn the next page, briefly describe why you are applying for the API access.\n\n\n\nYou will now see your API key on the page. MAKE SURE that you copy and store the key safely. Do not share your API key with others, as doing so may block your access to the FRED API.\nIf you forget your API key, you can retrieve it from FRED using your registered email and log-in credentials.\n\n\n\nExercise 3: Set Up FRED API and Acquire Unemployment Data\n\n\n\n\n\n\nNoteTasks\n\n\n\n\nSkim the user manual of the FRED API (https://fred.stlouisfed.org/docs/api/fred/) and the package fredr (https://cran.r-project.org/web/packages/fredr/vignettes/fredr.html).\nSet up FRED API in your program.\nUse fredr to search for the ID of unemployment rate data.\nDownload unemployment rate data from 1990 to 2024 using the FRED API.\n\n\n\n\n(a) Set Up the FRED API\nAfter obtaining the API key for FRED, let’s connect to the API and download the data we need.\n\nMake sure that you have all required packages loaded. fredr is the package we need for getting access to the FRED API.\n\n\n# Install the packages if needed:\n# install.packages(c(\"fredr\", \"dplyr\", \"ggplot2\", \"lubridate\"))\n\n# Load packages\nlibrary(fredr) # Package for Using FRED API\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(lubridate)\n\n\nWe need to tell the program your API key (i.e., the log-in credentials for your R program) using the function fredr_set_key().\n\n\n# Assign your API key to a variable \"api_key\"\napi_key &lt;- \"YOUR_API_KEY\" \n# Set API Key \nfredr_set_key(api_key)\n\n\nWe are now connected to the API and ready for data acquisition.\n\n\n\n(b) Extract data using fredr()\nfredr() is the function we are going to use to extract data from FRED.\nThis function comes with several arguments:\n\ndata &lt;- fredr(\n  series_id = \"SERIES ID\", # Replace with the Series ID in FRED\n  observation_start = , # The start date of extracted data\n  observation_end   = # The end date of extracted data\n)\n\n\nseries_id is the ID of the data series you would like to retrieve from FRED.\nobservation_start specifies the starting date of the data. Note that this value needs to be in the date format. We can use as.Date(YYYY-MM-DD) to generate dates. If this value is not specified, the function will retrieve the data back to the earliest available period.\nobservation_end specifies the starting date of the data. If this value is not specified, the function will retrieve the data until the most recent period.\n\n\n\n\n\n\n\nTipWhat is as.Date()\n\n\n\n\n\nas.Date() is a base function in R that generates or converts variables to date variables. Dates and time stamps are unique data formats that require additional work (more about this topic in the next chapter).\n\nBy default, as.Date() converts strings in “YYYY-MM-DD” into corresponding dates and store the value as the date format. For example, as.Date(\"2025-09-10\") delivers a date value corresponding to September 10, 2025.\nas.Date() can also deal with other date expressed in alternative ways as long as you specify the date style accordingly. For example, as.Date(\"10/09/2025\", format = \"%d/%m/%Y\") and as.Date(\"Sep 10, 2025\", format = \"%b %d, %Y\") also give you the same date value in R.\n\nIn fredr(), the values for observation_start = and observation_end = must be in the date format. You need to make sure to input date values for these two arguments.\n\n\n\n\n\n\n\n\n\nNoteGet Series IDs\n\n\n\nEach series of data in FRED has a unique ID. There are two ways to find the IDs:\n\nYou can see the series ID on its pages on the FRED’s website\nUse the embedded search function in fredr to get the IDs\n\nfredr_series_search_text() is the function for you to search by key words and find series IDs for the data you need.\nSimply put the key words (with quote marks) in the function to do the search\nFilter the output and keep only titles and corresponding IDs\n\n\n\nfredr_series_search_text(\"TEXT HERE\")\n# Select and display the results with two columns: Series ID and title of data\nfredr_series_search_text(\"TEXT HERE\")[YOUR_CODE]\n\n\n\n\n\n(c) Retrieve Data\nOnce you get the ID for the unemployment rate data, download the unemployment data from 1990 to 2024. Note that fredr() plays nicely with the tidyverse and you can directly apply data wrangling skills you have learned so far.\n\nunrate &lt;- YOUR_CODE(\n  series_id = \"YOUR_CODE\",\n  observation_start = as.Date(\"1990-01-01\"),\n  observation_end   = as.Date(\"2024-12-31\")\n) %&gt;% \n  YOURCODE(date, unrate = value) \n  # Keep the date column and the unemployment rate (rename as \"unrate\")\n\n\n\n(d) Visualize Inflation and Unemployment Data\nInflation has become a major concern in the macroeconomy. Since 2020, large-scale monetary expansions and government stimulus have driven inflation sharply higher, making it a central topic of discussion. In this exercise, you’ll get to use the FRED API to pull real economic data and calculate inflation yourself.\n(d1) Retrieve the Core Consumer Price Index (all items excluding food and energy) during 1990 and 2024 from FRED.\n(d2) Inflation is often measured as the changes in consumer price index (CPI). Here, we are going to calculate YoY changes in core CPI.\n\n\n\n\n\n\nTipCalculate YoY (year-over-year) Changes\n\n\n\n\n\nWhen calculating changes in macroeconomic data (as well as other data evolving over time), we tend to focus on YoY changes. The major reason is that many economic indicators are strongly affected by seasons. Comparing July to June (MoM) or Q4 to Q3 (QoQ) may reflect seasonal swings rather than true economic trends. If we focus on YoY changes, such seasonal variations will be cancelled out.\nThe tidyverse makes it easy for us to calculate YoY changes in a variable. As the CPI data is monthly, to calculate YoY changes, we simply need to calculate the changes in the value from month t-12 to t.\nTo do this in R, we need to - Sort the data by date in the ascending order - Use lag() to “look backwards in the data”. lag() lets you grab the value in previous rows within the same column so you can compare it to the current one. - lag(x, 1): value from 1 row earlier - lag(x, 3): value from 1 row earlier (useful if your data is monthly and you want last quarter’s value) - lag(x, 12): value from 12 rows earlier (useful if your data is monthly and you want last year’s value) - You may need additional data ahead of your desired sample period so that the first few observations are non-missing. - For example, if you want to calculate inflation rate as of 1990-01-01, you need to keep your CPI data back to 1989-01-01 so that you can calculate YoY changes for 1990-01-01. - If you only have CPI data starting from 1990-01-01, the inflation rate for 1990-01 to 1990-12 will be missing, as their is no denominator for the calculation of YoY changes/growth.\n\n\n\nHere is the code and you need to fill in necessary information\n\n# Calculate inflation\ninflation &lt;- YOUR_CODE(\n  series_id = \"CPILFESL\",\n  observation_start = as.Date(\"1989-01-01\"), # One year ahead for lag operation\n  observation_end = as.Date(\"2024-12-31\")) |&gt;\n  YOUR_CODE(date, cpi = value) |&gt;\n  arrange(date) |&gt;\n  YOUR_CODE(inflation = (cpi / lag(cpi, 12) - 1) * 100) |&gt;\n  filter(!is.na(inflation)) # Remove missing values\n\n(d3) Plot unemployment rates and inflation from 1990 to 2024 in one single graph.\n\np1 &lt;- ggplot() +\n  # Inflation line\n  YOUR_CODE(\n    data = inflation,\n    aes(x = date, y = inflation, color = \"Inflation (%)\"),\n    linewidth = 1\n  ) +\n  # Unemployment line\n  YOUR_CODE(\n    data = unrate,\n    aes(x = date, y = unrate, color = \"Unemployment Rate (%)\"),\n    linewidth = 1\n  ) +\n  labs(\n    title = \"Inflation and Unemployment Rate (1990 - 2024)\",\n    x = NULL, y = \"Percent\", color = NULL\n  ) +\n  scale_x_date(date_breaks = \"2 years\", date_labels = \"%Y\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  theme_minimal(base_size = 12) +\n  theme(legend.position = \"top\")\n\np1\n\n\n\n(e) Recession Periods\nEconomic recessions indicate that the whole economy is in a bad shape. The National Bureau of Economic Research defines economic recessions as a period when there is a significant decline in economic activities (https://www.nber.org/research/business-cycle-dating). According to their thresholds for the depth, diffusion, and duration of economic recessions, there are certain periods defined as recession periods. FRED integrates NBER’s recession period data into its database and allows us to extract the data using the API.\n(e1) Use the FRED API to get access to the U.S. recession data (series_id =\"USREC\").\n\n# US recessions data: 1 = during recession, 0 = otherwise\nusrec &lt;- fredr(series_id = \"USREC\", \n               YOUR_CODE = YOUR_CODE(\"1990-01-01\"),\n               YOUR_CODE = YOUR_CODE(\"2024-12-31\")) |&gt;\n  YOUR_CODE(date, rec = value) |&gt;\n  arrange(date)\n\n(e2) (Challenge yourself) The data from FRED is at the month level, showing whether a month is in recession or not. Use data wrangling tools you have learned from this subject to convert the monthly recession data into a data at the recession level (i.e., each row represents one recession). For each recession, we need two columns: start showing the start month of a recession, and end showing the end month of a recession.\n\n\n\n\n\n\nTipTry before seeing this part\n\n\n\n\n\nThe code below is an application of data wrangling tools as well as logical operations (True and False, as well as the operations for logical values). No pressure if you cannot write this chunk independently - make sure you are able to understand each line.\n\nrecession_period &lt;- usrec |&gt;\n  mutate(\n    rec = (rec == 1), \n    # Convert \"rec\" into a logical variable (true/false) for next line.\n    start_flag = rec & !lag(rec, default = FALSE) \n    # Mark the start month of recession\n  ) |&gt;\n  filter(rec) |&gt;\n  group_by(grp = cumsum(start_flag)) |&gt;\n  summarise(\n    start = min(date),\n    end   = max(date),\n    .groups = \"drop\" \n  ) |&gt; select(start, end)\n\n\n\n\n(e3) In the plot generated from (d3), add shaded areas to indicate recession periods.\n\np1 +   geom_rect(\n    data = recession_period,\n    aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf),\n    fill = \"grey75\", alpha = 0.25, inherit.aes = FALSE\n  )\n\n(e4) Is there any patterns for unemployment rates and inflation during the recession periods? How do you explain the pattern(s)?",
    "crumbs": [
      "Tutorials",
      "Tutorial 7: Obtaining Macroeconomic Data through FRED"
    ]
  },
  {
    "objectID": "tutorials/data_collection.html#in-class-exercises",
    "href": "tutorials/data_collection.html#in-class-exercises",
    "title": "Tutorial 7: Obtaining Macroeconomic Data through FRED",
    "section": "In-Class Exercises",
    "text": "In-Class Exercises\nYou will discuss these exercises in class with your peers in small groups and with your tutor. These exercises build from the exercises you have prepared above, you will get the most value from the class if you have completed those above before coming to class.\n\nExercise 4: Seasonal Adjustment for Unemployment Data\nWhen you retrieve the unemployment data from FRED using the series tag, UNRATE, you obtain seasonally adjusted unemployment rate.\n\nIn this exercise, we are going to get a deeper understanding of seasonal adjustment by comparing seasonally adjusted and unadjusted unemployment rate.\n\n(a) Retrieve Data\n\nSearch using the FRED API (or the FRED’s website) to obtain tags for both seasonally adjusted and unadjusted unemployment rate.\nUse fredr to acquire both seasonally adjusted and unadjusted unemployment rate data in the period “2020-01-01 to 2024-12-31.”\n\n\n# Unemployment rate (seasonally adjusted)\nunrate_sa &lt;- YOUR_CODE(\n  YOUR_CODE = \"YOUR_CODE\",\n  observation_start = \"YOUR_CODE\",\n  observation_end = \"YOUR_CODE\"\n) %&gt;% select(YOUR_CODE) \n# keep date and unemployment rate (adjusted), rename it as \"unrate_sa\"\n\n# Unemployment rate (seasonally unadjusted)\nunrate_usa &lt;- YOUR_CODE(\n  YOUR_CODE = \"YOUR_CODE\",\n  observation_start = \"YOUR_CODE\",\n  observation_end = \"YOUR_CODE\"\n) %&gt;% select(YOUR_CODE) \n# keep date and unemployment rate (unadjusted), rename it as \"unrate_usa\"\n\n\n\n(b) Visualize Unemployment Rates\nPlot the two series of data in the same plot with time as the horizontal axis and unemployment rate (%) as the vertical axis. Discuss (1) how the two lines differ from each other and (2) which one is more useful?\n\nggplot() +\n  YOUR_CODE(data = unrate_sa, \n            aes(x = YOUR_CODE, y = YOUR_CODE, \n                color = \"Seasonally Adjusted (%)\")) +\n  YOUR_CODE(data = unrate_nsa, \n            aes(x = YOUR_CODE, y = YOUR_CODE, \n                color = \"Non Seasonally Adjusted (%)\")) +\n  labs(\n    title = \"Unemployment Rate (Adjusted vs. Unadjusted)\",\n    x = \"Year-Month\", y = \"Percent\", color = NULL\n  ) +\n  scale_x_date(date_labels = \"%Y-%m\", \n               date_breaks = \"3 months\") + # Set up date labels for x-axis \n  theme_minimal(base_size = 12) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + \n  # Avoid text labels from overlapping with each other\n  theme(legend.position = \"top\")\n\n\n\n(c) Seasonal Adjustments\nAlthough you are not required to master the recipe and the algorithms that the FRED uses to adjust seasonality, you may use the two series (adjusted and unadjusted) to reverse-engineer the seasonal adjustments made to the data.\n(c1) Merge seasonal adjusted and unadjusted unemployment rates and calculate the adjustment made to the unadjusted number.\n\n# Merge two series of data and \n# generate a new variable \"adjustment\" for the adjustment made \nadjustment &lt;- YOUR_CODE\n\n(c2) Visualize the pattern of seasonal adjustments\n\nggplot(YOUR_DATA, aes(x = YOUR_VARIABLE, y = YOUR_VARIABLE)) +\n  YOUR_CODE() +\n  # Add a dashed line = 0 \n  geom_hline(yintercept = 0, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Seasonal Adjustments Made to Unemployment Rates (%)\",\n    subtitle = \"UNRATE – UNRATENSA\",\n    x = \"Year-Month\", y = \"Seasonal Adjustment (%)\"\n  ) +\n  scale_x_date(date_labels = \"%Y-%m\", date_breaks = \"3 months\") +\n  theme_minimal(base_size = 12) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n(d) Discuss your Findings\n\nDo you observe any seasonality in the unemployment rate data? What would be the possible causes of the seasonality?\nWhy is seasonal adjustment important when working on unemployment data?\n\n\n\n\nExercise 5: Okun’s Law\nYou may have learned the Okun’s Law from ECON10003 (Introductory Macroeconomics). This is an important relation for us to understand the relationship between economic output and unemployment.\n\n\n\n\n\n\nNoteDefinition: Okun’s Law\n\n\n\nOkun’s Law is a relation observed by Arthur Okun that links changes in the unemployment rate to changes in a country’s real gross domestic product (GDP). It states that there is a negative correlation between changes in unemployment rate and changes in real GDP: a rise in the unemployment rate by a certain percentage generally corresponds to a fall in real GDP by a larger percentage. For example, a 1% increase in unemployment often correlates with a 2% to 3% decrease in real GDP.\n\n\nIn this exercise, we are going to visualize the Okun’s Law using the macroeconomic data obtained from FRED. Here is our roadmap:\n\n(a) Obtain Data\nSet up the API and use it to obtain real Gross Domestic Product (GDP) and seasonally adjusted unemployment rate data from 1990-01-01 to 2024-12-31.\n\n# Load packages\nlibrary(fredr) # Package for Using FRED API\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(lubridate)\n\n# Assign your API key to a variable \"api_key\"\napi_key &lt;- \"YOUR API KEY\" \n# Set API Key \nfredr_set_key(api_key)\n\n\n# Real GDP Data\ngdp &lt;- YOUR_CODE(\n  series_id = \"YOUR_ID\",\n  observation_start = as.Date(\"1989-01-01\"), # One-year ahead for lag operation\n  observation_end   = as.Date(\"2024-12-31\")\n) %&gt;%\n  YOUR_CODE(date, gdp = value) \n\nunrate &lt;- fredr(\n  series_id = \"UNRATE\",\n  observation_start = as.Date(\"1989-01-01\"), # One-year ahead for lag operation\n  observation_end   = as.Date(\"2024-12-31\")\n) %&gt;%\n  YOUR_CODE(date, unrate = value)\n\n\n\n(b) Frequency of Data\nTake a quick look at the data, are the two data share the same frequency? Can you propose some solutions to this situation?\n\n\n(c) Variable Calculation\n(c1) Calculate YoY growth (in %) of real GDP (Hint: be careful about the number of periods for your lag operation).\n\n# Calculate YoY GDP Growth\ngdp_growth &lt;- gdp %&gt;%\n  YOUR_CODE(date) %&gt;%\n  YOUR_CODE(gdp_growth = 100 * (gdp / lag(gdp, YOUR_NUMBER) - 1)) \n  # Calculate the GDP growth relative to the same quarter in the previous year\n\n(c2) Convert monthly unemployment rate data into quarterly by calculating quarterly average unemployment rate. Calculate the changes in unemployment rate relative to the previous quarter.\n\n# Convert to Quarter Unemployment Rates and Calculate changes\nunrate_qtr &lt;- unrate %&gt;%\n  # Use the first month (Jan, Apr, Jul, Oct) to represent each quarter\n  YOUR_CODE(quarter = floor_date(date, \"quarter\")) %&gt;% \n  group_by(YOUR_OBJECT) %&gt;%\n  summarize(\n    unrate = YOUR_CODE(unrate), \n    .groups = \"drop\") %&gt;%\n  rename(date = quarter) %&gt;% # for easier merge in following steps\n  YOUR_CODE(date) %&gt;%\n  YOUR_CODE(unrate_chg = unrate - lag(unrate))\n\n\n\n(d) Visualize the Okun’s Law\nMerge the changes in unemployment rates and real GDP in one single dataset, use ggplot() to create a scattered plot showing the relation between changes in unemployment rates (on the horizontal axis) and changes in real GDP (on the vertical axis).\n\n# Merge data\nokun &lt;- YOUR_CODE(YOUR_OBJECT, YOUR_OBJECT, by = \"YOUR_VARIABLE\") %&gt;%\n        filter(!is.na(unrate_chg) &!is.na(gdp_growth) )\n\n# Scatter Plot\nscatter_plt &lt;- ggplot(YOUR_OBJECT, aes(x = YOUR_OBJECT, y = YOUR_OBJECT)) +\n  geom_point(color = \"steelblue\", alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"darkred\") +\n  labs(\n    title = \"Okun’s Law Visualization\",\n    x = \"Change in Unemployment Rate (%)\",\n    y = \"Change in Real GDP (%)\"\n  ) +\n  theme_minimal(base_size = 12)\n\n\n\n(e) Quantify the Relation\nThe fitted line provides with us a rough idea of the relation between changes in unemployment and real GDP. However, it is difficult to spot the exact slope of the fitted line in the graph. As Okun’s Law suggests, a 1% increase in unemployment often correlates with a 2% to 3% decrease in real GDP. We would like to confirm this quantitative relation using the data from FRED. To achieve our goal, we need to do a simple linear regression to obtain the exact value of the slope. In R, we can use the built-in method lm to fit a linear regression model. Once the model has been fitted, coefficient estimates will be stored in our assigned object. We can use the function coef() to get a list of numbers representing the intercept and the slope.\n\n# Slope of fitted line\nline_fit &lt;- lm(gdp_growth ~ unrate_chg, data = okun)\ncoef(line_fit)\n\n# Equation (Just print it out in a string format)\nline_equation &lt;- paste0(\"ΔGDP = \",\n                          round(coef(line_fit)[1], 2), \" + \", \n                          round(coef(line_fit)[2], 2), \" × ΔUnemp\")\n\n# Print out the equation\nline_equation\n\n\n\n\n\n\n\nNoteWhat if our replication is inconsistent with the Okun’s Law?\n\n\n\n\nCheck if everything is correct in our program (always necessary and important).\nThe sample is different: Okun’s Law is based on data decades ago, whereas we are using recent decades. This may suggest that Okun’s Law no longer holds (?).\nIs there anything odd in the scattered plot?\n\n\n\n\n\n(f) Eliminate Outliers (Extreme Values)\nAs you can see from the scattered plot in (d), there are a few outliers to the right of the graph (almost 10% unemployment rate). These points are due to the outbreak of COVID-19 in early 2020, when many people became jobless in a night because of lockdowns.\nOkun’s law describes a general relation between changes in unemployment and GDP. It may not apply to such extreme cases as the COVID-19 outbreak. Additionally, such observations in the data will largely affect our inferences: it will affect the shape of the fitted line as well as the slope significantly. Hence, we need to get rid of these extreme cases (“outliers”) from our sample when we are interested in obtaining a generalizable conclusion.\nThere are many techniques dealing with outliers in data. We are going to adopt a simple and straightforward approach - trimming. By trimming the data, we drop observations with extremely large (small) values from the sample to eliminate their influence on our results and inferences. In common practice, we trim the sample at the 1^{st} and the 99^{th} percentiles, meaning that one observation is dropped if it has a value larger (smaller) than the 99^{th} (1^{st}) percentile of the data.\n\n# Remove the outliers\nokun_trim &lt;- okun |&gt;\n  filter(\n    between(gdp_growth, \n            quantile(gdp_growth, 0.01, na.rm = TRUE), \n            quantile(gdp_growth, 0.99, na.rm = TRUE)\n            ),\n    between(unrate_chg, \n            quantile(unrate_chg, 0.01, na.rm = TRUE), \n            quantile(unrate_chg, 0.99, na.rm = TRUE)\n            )\n  )\n\n\n\n(g) Okun’s Law without Outliers\nAfter removing outliers from the data, let’s do the plot again:\n\n# Plot again\nggplot(\"FILL IN THE BLANK HERE\") +\n  labs(\n    title = \"Okun’s Law Visualization without Outliers\",\n    x = \"Change in Unemployment Rate (%)\",\n    y = \"Change in Real GDP (%)\"\n  ) +\n  theme_minimal(base_size = 12)\n\nAgain, let’s take a look at the slope. Does it look consistent with the Okun’s Law?\n\n# Fitted Line after Trim\nline_fit &lt;- YOUR_CODE(gdp_growth ~ unrate_chg, data = okun_trim)\nYOUR_CODE(line_fit)\n\n# Equation\nline_equation &lt;- paste0(\"ΔGDP = \",\n                          round(coef(line_fit)[1], 2), \" + \", \n                          round(coef(line_fit)[2], 2), \" × ΔUnemp\")\n\nline_equation",
    "crumbs": [
      "Tutorials",
      "Tutorial 7: Obtaining Macroeconomic Data through FRED"
    ]
  }
]